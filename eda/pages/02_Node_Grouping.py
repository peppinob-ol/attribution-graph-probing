"""Page 2 - Node Grouping: Classify and name supernodes for interpretation"""
import sys
from pathlib import Path

# Add parent directory to path
parent_dir = Path(__file__).parent.parent.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

import streamlit as st
import pandas as pd
import json
import io
import os
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import node grouping functions
import importlib.util
script_path = parent_dir / "scripts" / "02_node_grouping.py"
spec = importlib.util.spec_from_file_location("node_grouping", script_path)
node_grouping = importlib.util.module_from_spec(spec)
spec.loader.exec_module(node_grouping)
prepare_dataset = node_grouping.prepare_dataset
classify_nodes = node_grouping.classify_nodes
name_nodes = node_grouping.name_nodes
DEFAULT_THRESHOLDS = node_grouping.DEFAULT_THRESHOLDS

st.set_page_config(page_title="Node Grouping", page_icon="üîó", layout="wide")

st.title("üîó Node Grouping & Classification")

st.info("""
**Automatically classify and name supernodes** to facilitate attribution graph interpretation.

This pipeline transforms SAE features into interpretable supernodes through 3 steps:
1. **Preparation**: Identify functional vs semantic tokens and find target tokens
2. **Classification**: Assign each feature to a class (Semantic, Say X, Relationship)
3. **Naming**: Generate descriptive names for each supernode
""")

# ===== SIDEBAR: CONFIGURATION =====

st.sidebar.header("‚öôÔ∏è Configuration")

# File upload
st.sidebar.subheader("üìÅ Input Files")

# Default paths
default_csv_path = parent_dir / "output" / "2025-10-21T07-40_export_ENRICHED.csv"
default_json_path = parent_dir / "output" / "activations_dump (2).json"
default_graph_path = parent_dir / "output" / "graph_data" / "clt-hp-the-capital-of-201020250035-20251020-003525.json"

# Auto-load default files if they exist and haven't been loaded yet
if 'default_files_loaded' not in st.session_state:
    st.session_state['default_files_loaded'] = False
    
    if default_csv_path.exists():
        st.session_state['default_csv'] = default_csv_path
        st.sidebar.info(f"‚úÖ CSV auto-loaded: `{default_csv_path.name}`")
    
    if default_json_path.exists():
        st.session_state['default_json'] = default_json_path
        st.sidebar.info(f"‚úÖ JSON auto-loaded: `{default_json_path.name}`")
    
    if default_graph_path.exists():
        st.session_state['default_graph'] = default_graph_path
        st.session_state['graph_json_uploaded'] = default_graph_path  # Save for upload too
        st.sidebar.info(f"‚úÖ Graph JSON auto-loaded: `{default_graph_path.name}`")
    
    st.session_state['default_files_loaded'] = True

uploaded_csv = st.sidebar.file_uploader(
    "CSV Export (required)",
    type=["csv"],
    help="CSV file generated by Probe Prompts (e.g. *_export.csv or *_export_ENRICHED.csv)"
)

uploaded_json = st.sidebar.file_uploader(
    "Activations JSON (optional)",
    type=["json"],
    help="JSON file with token-by-token activations (improves naming for Relationship)"
)

uploaded_graph = st.sidebar.file_uploader(
    "Graph JSON (optional)",
    type=["json"],
    help="Original Graph JSON file (for csv_ctx_idx fallback in Semantic naming)"
)

uploaded_nodes_json = st.sidebar.file_uploader(
    "Selected Nodes JSON (optional)",
    type=["json"],
    help="JSON file with node_ids selected from Graph Generation (for accurate subgraph upload)"
)

# Pipeline parameters
st.sidebar.subheader("üéõÔ∏è Pipeline Parameters")

window_size = st.sidebar.slider(
    "Target search window",
    min_value=3,
    max_value=15,
    value=7,
    help="Maximum number of tokens to explore to find semantic targets"
)

# Token blacklist
st.sidebar.markdown("**üö´ Token Blacklist**")
blacklist_input = st.sidebar.text_area(
    "Tokens to exclude (one per line)",
    value=st.session_state.get('blacklist_input', ''),
    height=100,
    help="Tokens that should not be used as labels. If the first token with max activation is in blacklist, the system falls back to the next one. Enter one token per line (case-insensitive).",
    key='blacklist_input'
)

# Parse blacklist (split by newline, strip, lowercase)
blacklist_tokens = set()
if blacklist_input.strip():
    for line in blacklist_input.strip().split('\n'):
        token = line.strip().lower()
        if token:
            blacklist_tokens.add(token)

if blacklist_tokens:
    st.sidebar.info(f"üö´ {len(blacklist_tokens)} tokens in blacklist")
else:
    st.sidebar.caption("No tokens in blacklist")

# Classification thresholds
st.sidebar.subheader("üìä Classification Thresholds")

# Threshold management (save/load)
st.sidebar.markdown("**üíæ Threshold Management**")

col_save, col_load = st.sidebar.columns(2)

with col_save:
    # Prepare current thresholds for export
    current_thresholds = {
        'dict_peak_consistency_min': st.session_state.get('dict_consistency', DEFAULT_THRESHOLDS['dict_peak_consistency_min']),
        'dict_n_distinct_peaks_max': st.session_state.get('dict_n_peaks', DEFAULT_THRESHOLDS['dict_n_distinct_peaks_max']),
        'sayx_func_vs_sem_min': st.session_state.get('sayx_func_min', DEFAULT_THRESHOLDS['sayx_func_vs_sem_min']),
        'sayx_conf_f_min': st.session_state.get('sayx_conf_f', DEFAULT_THRESHOLDS['sayx_conf_f_min']),
        'sayx_layer_min': st.session_state.get('sayx_layer', DEFAULT_THRESHOLDS['sayx_layer_min']),
        'rel_sparsity_max': st.session_state.get('rel_sparsity', DEFAULT_THRESHOLDS['rel_sparsity_max']),
        'sem_layer_max': st.session_state.get('sem_layer', DEFAULT_THRESHOLDS['sem_layer_max']),
        'sem_conf_s_min': st.session_state.get('sem_conf_s', DEFAULT_THRESHOLDS['sem_conf_s_min']),
        'sem_func_vs_sem_max': st.session_state.get('sem_func_vs_sem', DEFAULT_THRESHOLDS['sem_func_vs_sem_max']),
    }
    
    thresholds_json = json.dumps(current_thresholds, indent=2)
    st.download_button(
        label="üíæ Save",
        data=thresholds_json,
        file_name=f"thresholds_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json",
        help="Download current thresholds as JSON",
        use_container_width=True
    )

with col_load:
    uploaded_thresholds = st.file_uploader(
        "Load Thresholds",
        type=['json'],
        help="Load thresholds from JSON file",
        label_visibility="collapsed",
        key="upload_thresholds"
    )

# Load thresholds from file if provided
if uploaded_thresholds is not None:
    try:
        uploaded_thresholds.seek(0)  # Reset file pointer to beginning
        loaded_thresholds = json.load(uploaded_thresholds)
        
        # Validate that it contains all required keys
        required_keys = set(DEFAULT_THRESHOLDS.keys())
        loaded_keys = set(loaded_thresholds.keys())
        
        if required_keys == loaded_keys:
            # Update session state
            st.session_state['dict_consistency'] = loaded_thresholds['dict_peak_consistency_min']
            st.session_state['dict_n_peaks'] = loaded_thresholds['dict_n_distinct_peaks_max']
            st.session_state['sayx_func_min'] = loaded_thresholds['sayx_func_vs_sem_min']
            st.session_state['sayx_conf_f'] = loaded_thresholds['sayx_conf_f_min']
            st.session_state['sayx_layer'] = loaded_thresholds['sayx_layer_min']
            st.session_state['rel_sparsity'] = loaded_thresholds['rel_sparsity_max']
            st.session_state['sem_layer'] = loaded_thresholds['sem_layer_max']
            st.session_state['sem_conf_s'] = loaded_thresholds['sem_conf_s_min']
            st.session_state['sem_func_vs_sem'] = loaded_thresholds['sem_func_vs_sem_max']
            
            st.sidebar.success("‚úÖ Thresholds loaded!")
            # Remove file uploader to avoid continuous reloads
            st.session_state['upload_thresholds'] = None
            st.rerun()
        else:
            missing = required_keys - loaded_keys
            extra = loaded_keys - required_keys
            error_msg = []
            if missing:
                error_msg.append(f"Missing keys: {', '.join(missing)}")
            if extra:
                error_msg.append(f"Extra keys: {', '.join(extra)}")
            st.sidebar.error(f"‚ùå Invalid JSON file:\n" + "\n".join(error_msg))
    except json.JSONDecodeError as e:
        st.sidebar.error(f"‚ùå JSON parsing error: {e}")
    except Exception as e:
        st.sidebar.error(f"‚ùå Error loading thresholds: {e}")

# Reset thresholds
if st.sidebar.button("üîÑ Reset Default", help="Restore default thresholds", use_container_width=True):
    for key in ['dict_consistency', 'dict_n_peaks', 'sayx_func_min', 'sayx_conf_f', 
                'sayx_layer', 'rel_sparsity', 'sem_layer', 'sem_conf_s', 'sem_func_vs_sem']:
        if key in st.session_state:
            del st.session_state[key]
    st.sidebar.success("‚úÖ Thresholds restored!")
    st.rerun()

st.sidebar.markdown("---")

with st.sidebar.expander("Dictionary Semantic", expanded=False):
    dict_consistency = st.slider(
        "Peak Consistency (min)",
        min_value=0.5,
        max_value=1.0,
        value=st.session_state.get('dict_consistency', DEFAULT_THRESHOLDS['dict_peak_consistency_min']),
        step=0.05,
        help="How often the token should be peak when it appears in the prompt",
        key='dict_consistency'
    )
    dict_n_peaks = st.number_input(
        "N Distinct Peaks (max)",
        min_value=1,
        max_value=5,
        value=st.session_state.get('dict_n_peaks', DEFAULT_THRESHOLDS['dict_n_distinct_peaks_max']),
        help="Maximum number of distinct tokens as peak",
        key='dict_n_peaks'
    )

with st.sidebar.expander("Say X", expanded=False):
    sayx_func_min = st.slider(
        "Func vs Sem % (min)",
        min_value=0.0,
        max_value=100.0,
        value=st.session_state.get('sayx_func_min', DEFAULT_THRESHOLDS['sayx_func_vs_sem_min']),
        step=5.0,
        help="% difference between max activation on functional vs semantic",
        key='sayx_func_min'
    )
    sayx_conf_f = st.slider(
        "Confidence F (min)",
        min_value=0.5,
        max_value=1.0,
        value=st.session_state.get('sayx_conf_f', DEFAULT_THRESHOLDS['sayx_conf_f_min']),
        step=0.05,
        help="Fraction of peaks on functional tokens",
        key='sayx_conf_f'
    )
    sayx_layer = st.number_input(
        "Layer (min)",
        min_value=0,
        max_value=30,
        value=st.session_state.get('sayx_layer', DEFAULT_THRESHOLDS['sayx_layer_min']),
        help="Minimum layer for Say X (typically high layers)",
        key='sayx_layer'
    )

with st.sidebar.expander("Relationship", expanded=False):
    rel_sparsity = st.slider(
        "Sparsity (max)",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.get('rel_sparsity', DEFAULT_THRESHOLDS['rel_sparsity_max']),
        step=0.05,
        help="Maximum sparsity (low = diffuse activation)",
        key='rel_sparsity'
    )

with st.sidebar.expander("Semantic (Concept)", expanded=False):
    sem_layer = st.number_input(
        "Layer (max)",
        min_value=0,
        max_value=10,
        value=st.session_state.get('sem_layer', DEFAULT_THRESHOLDS['sem_layer_max']),
        help="Maximum layer for Dictionary fallback",
        key='sem_layer'
    )
    sem_conf_s = st.slider(
        "Confidence S (min)",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.get('sem_conf_s', DEFAULT_THRESHOLDS['sem_conf_s_min']),
        step=0.05,
        help="Fraction of peaks on semantic tokens",
        key='sem_conf_s'
    )
    sem_func_vs_sem = st.slider(
        "Func vs Sem % (max)",
        min_value=0.0,
        max_value=100.0,
        value=st.session_state.get('sem_func_vs_sem', DEFAULT_THRESHOLDS['sem_func_vs_sem_max']),
        step=5.0,
        help="Maximum % difference to consider Semantic",
        key='sem_func_vs_sem'
    )

# ===== MAIN: PIPELINE EXECUTION =====

# Use uploaded files or default
csv_to_use = uploaded_csv if uploaded_csv is not None else st.session_state.get('default_csv')
json_to_use = uploaded_json if uploaded_json is not None else st.session_state.get('default_json')
graph_to_use = uploaded_graph if uploaded_graph is not None else st.session_state.get('default_graph')
nodes_json_to_use = uploaded_nodes_json  # Only if manually uploaded (no default for now)

if csv_to_use is None:
    st.warning("‚¨ÜÔ∏è Load a CSV file to begin")
    st.markdown("""
    ### üìñ How It Works
    
    #### Step 1: Dataset Preparation
    - **Classify tokens**: Identify functional tokens (e.g. "is", "the", ",") vs semantic tokens (e.g. "Texas", "capital")
    - **Target tokens**: For functional tokens, find the first semantic token in the specified direction
    - **Source**: Use tokens from JSON if available, otherwise fallback tokenization
    
    #### Step 2: Node Classification
    
    Each feature is classified based on aggregate metrics:
    
    - **Semantic (Dictionary)**: Always activates on the same specific token
      - E.g.: Feature that only activates on "Texas"
      - Characteristics: high peak_consistency, n_distinct_peaks = 1
    
    - **Semantic (Concept)**: Activates on semantically similar tokens
      - E.g.: Feature that activates on "city", "capital", "state"
      - Characteristics: high conf_S, medium-low layer
    
    - **Say X**: Activates on functional tokens to predict the next token
      - E.g.: Feature that activates on "is" before "Austin"
      - Characteristics: high func_vs_sem, high conf_F, high layer
    
    - **Relationship**: Connects multiple semantic concepts
      - E.g.: Feature that activates on "city", "capital", "state" together
      - Characteristics: low sparsity (diffuse activation), high K
    
    #### Step 3: Supernode Naming
    
    Generate descriptive names for each supernode:
    
    - **Relationship**: `"(X) related"` where X is the first semantic token with max activation
    - **Semantic**: Name of token with max activation (e.g. "Texas", "city")
    - **Say X**: `"Say (X)"` where X is the target_token (e.g. "Say (Austin)")
    
    ### üéØ Key Parameters
    
    - **Peak Consistency**: How often a token is peak when it appears in the prompt
    - **Func vs Sem %**: % difference between max activation on functional vs semantic
    - **Confidence F/S**: Fraction of peaks on functional/semantic tokens
    - **Sparsity**: How concentrated (high) vs diffuse (low) the activation is
    - **Layer**: Model layer where the feature resides
    """)
    st.stop()

# Load CSV
try:
    if isinstance(csv_to_use, Path):
        # Default file (path)
        df = pd.read_csv(csv_to_use)
        csv_name = csv_to_use.name
    else:
        # Uploaded file
        df = pd.read_csv(csv_to_use)
        csv_name = csv_to_use.name if hasattr(csv_to_use, 'name') else 'uploaded file'
    
    st.success(f"‚úÖ CSV loaded: {csv_name} - {len(df)} rows, {df['feature_key'].nunique()} unique features")
except Exception as e:
    st.error(f"‚ùå CSV loading error: {e}")
    st.stop()

# Load JSON (optional)
tokens_json = None
if json_to_use:
    try:
        if isinstance(json_to_use, Path):
            # Default file (path)
            with open(json_to_use, 'r', encoding='utf-8') as f:
                tokens_json = json.load(f)
            json_name = json_to_use.name
        else:
            # Uploaded file
            json_to_use.seek(0)  # Reset file pointer to beginning
            tokens_json = json.load(json_to_use)
            json_name = json_to_use.name if hasattr(json_to_use, 'name') else 'uploaded file'
        
        n_prompts = len(tokens_json.get('results', []))
        st.success(f"‚úÖ JSON activations loaded: {json_name} - {n_prompts} prompts")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è JSON loading error: {e}")

# Load Selected Nodes JSON (optional, for subgraph upload)
selected_nodes_data = None
if nodes_json_to_use:
    try:
        nodes_json_to_use.seek(0)  # Reset file pointer to beginning
        selected_nodes_data = json.load(nodes_json_to_use)
        # Save in session state for upload
        st.session_state['selected_nodes_data'] = selected_nodes_data
        
        # Show info
        metadata = selected_nodes_data.get('metadata', {})
        n_nodes = metadata.get('n_nodes', len(selected_nodes_data.get('node_ids', [])))
        n_features = metadata.get('n_features', len(selected_nodes_data.get('features', [])))
        
        st.success(f"‚úÖ Selected Nodes JSON loaded: {n_features} features, {n_nodes} nodes")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Selected Nodes JSON loading error: {e}")

# ===== STEP 1: PREPARATION =====

st.header("üìã Step 1: Dataset Preparation")

with st.expander("‚ÑπÔ∏è What does this step do?", expanded=False):
    st.markdown("""
    **Classify each token** as:
    - **Functional**: Token with low semantic specificity (e.g. "is", "the", ",")
    - **Semantic**: Token with specific meaning (e.g. "Texas", "capital")
    
    **Find target tokens** for functional tokens:
    - Functional tokens "point" to nearby semantic tokens
    - E.g.: "is" ‚Üí "Austin" (forward), "," ‚Üí "Texas" (backward) + "USA" (forward)
    
    **Token source**:
    - Prefers tokens from JSON activations (more accurate)
    - Falls back to tokenization of prompt text
    """)

if st.button("‚ñ∂Ô∏è Run Step 1", key="run_step1"):
    with st.spinner("Preparing dataset..."):
        try:
            df_prepared = prepare_dataset(
                df,
                tokens_json=tokens_json,
                window=window_size,
                verbose=False
            )
            
            # Save in session state
            st.session_state['df_prepared'] = df_prepared
            
            # Statistics
            n_functional = (df_prepared['peak_token_type'] == 'functional').sum()
            n_semantic = (df_prepared['peak_token_type'] == 'semantic').sum()
            n_json = (df_prepared['tokens_source'] == 'json').sum()
            n_fallback = (df_prepared['tokens_source'] == 'fallback').sum()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Functional Tokens", f"{n_functional} ({n_functional/len(df_prepared)*100:.1f}%)")
            with col2:
                st.metric("Semantic Tokens", f"{n_semantic} ({n_semantic/len(df_prepared)*100:.1f}%)")
            with col3:
                st.metric("Tokens from JSON", f"{n_json}/{len(df_prepared)}")
            
            st.success("‚úÖ Step 1 completed!")
            
        except Exception as e:
            st.error(f"‚ùå Step 1 error: {e}")
            import traceback
            st.code(traceback.format_exc())

# Show Step 1 results
if 'df_prepared' in st.session_state:
    df_prepared = st.session_state['df_prepared']
    
    st.subheader("üìä Step 1 Results")
    
    # Complete table
    st.write(f"**Complete results** ({len(df_prepared)} rows):")
    display_cols = ['feature_key', 'prompt', 'peak_token', 'peak_token_type', 'target_tokens', 'tokens_source']
    st.dataframe(df_prepared[display_cols], use_container_width=True, height=400)
    
    # Download
    csv_step1 = df_prepared.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üíæ Download CSV Step 1",
        data=csv_step1,
        file_name=f"node_grouping_step1_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )

# ===== STEP 2: CLASSIFICATION =====

st.header("üè∑Ô∏è Step 2: Node Classification")

with st.expander("‚ÑπÔ∏è What does this step do?", expanded=False):
    st.markdown("""
    **Classify each feature** based on aggregate metrics:
    
    **Decision Tree**:
    1. **Dictionary Semantic**: peak_consistency ‚â• 0.8 AND n_distinct_peaks ‚â§ 1
    2. **Say X**: func_vs_sem ‚â• 50% AND conf_F ‚â• 0.90 AND layer ‚â• 7
    3. **Relationship**: sparsity < 0.45
    4. **Semantic (Concept)**: layer ‚â§ 3 OR conf_S ‚â• 0.50 OR func_vs_sem < 50%
    5. **Review**: Ambiguous cases requiring manual review
    
    **Computed Metrics**:
    - `peak_consistency_main`: How often the main token is peak when it appears
    - `n_distinct_peaks`: Number of distinct tokens as peak
    - `func_vs_sem_pct`: % difference between max activation on functional vs semantic
    - `conf_F / conf_S`: Fraction of peaks on functional/semantic tokens
    - `sparsity_median`: Median sparsity (active prompts only)
    - `K_sem_distinct`: Number of distinct semantic tokens
    """)

if 'df_prepared' not in st.session_state:
    st.warning("‚ö†Ô∏è Run Step 1 first")
else:
    # Prepare custom thresholds
    custom_thresholds = {
        'dict_peak_consistency_min': dict_consistency,
        'dict_n_distinct_peaks_max': dict_n_peaks,
        'sayx_func_vs_sem_min': sayx_func_min,
        'sayx_conf_f_min': sayx_conf_f,
        'sayx_layer_min': sayx_layer,
        'rel_sparsity_max': rel_sparsity,
        'sem_layer_max': sem_layer,
        'sem_conf_s_min': sem_conf_s,
        'sem_func_vs_sem_max': sem_func_vs_sem,
    }
    
    if st.button("‚ñ∂Ô∏è Run Step 2", key="run_step2"):
        with st.spinner("Classifying nodes..."):
            try:
                df_classified = classify_nodes(
                    st.session_state['df_prepared'],
                    thresholds=custom_thresholds,
                    verbose=False
                )
                
                # Save in session state
                st.session_state['df_classified'] = df_classified
                
                # Statistics
                classifications = df_classified.groupby('feature_key')['pred_label'].first()
                label_counts = classifications.value_counts()
                
                st.success("‚úÖ Step 2 completed!")
                
                # Display distribution
                st.subheader("üìä Class Distribution")
                
                cols = st.columns(len(label_counts))
                for i, (label, count) in enumerate(label_counts.items()):
                    with cols[i]:
                        pct = 100 * count / len(classifications)
                        st.metric(label, f"{count} ({pct:.1f}%)")
                
                # Review warnings
                n_review = df_classified['review'].sum()
                if n_review > 0:
                    st.warning(f"‚ö†Ô∏è {n_review} rows require manual review")
                    review_features = df_classified[df_classified['review']]['feature_key'].unique()
                    st.write(f"Feature keys: {', '.join(review_features[:5])}")
                
            except Exception as e:
                st.error(f"‚ùå Step 2 error: {e}")
                import traceback
                st.code(traceback.format_exc())

# Show Step 2 results
if 'df_classified' in st.session_state:
    df_classified = st.session_state['df_classified']
    
    st.subheader("üìä Step 2 Results")
    
    # Filter by class
    selected_classes = st.multiselect(
        "Filter by class",
        options=df_classified['pred_label'].unique(),
        default=df_classified['pred_label'].unique()
    )
    
    df_filtered = df_classified[df_classified['pred_label'].isin(selected_classes)]
    
    # Reorder columns
    priority_cols = [
        'feature_key', 'layer', 'prompt', 'supernode_class', 'pred_label', 
        'subtype', 'confidence', 'review', 'why_review', 'peak_token'
    ]
    
    # Remaining columns (exclude those already in priority)
    other_cols = [col for col in df_filtered.columns if col not in priority_cols]
    
    # Final order: priority + others
    ordered_cols = [col for col in priority_cols if col in df_filtered.columns] + other_cols
    df_display = df_filtered[ordered_cols]
    
    # Complete table with all columns
    st.write(f"**Complete results** ({len(df_filtered)} rows, {len(df_filtered.columns)} columns):")
    st.dataframe(
        df_display, 
        use_container_width=True, 
        height=400,
        column_config={
            "prompt": st.column_config.TextColumn(
                "prompt",
                width="medium",
                help="Prompt text"
            )
        }
    )
    
    # Feature search and explanation
    st.subheader("üîç Explain Feature Classification")
    
    col_search, col_filter = st.columns([3, 1])
    
    with col_search:
        feature_to_explain = st.text_input(
            "Search feature_key",
            placeholder="e.g. 22_11998",
            help="Enter the feature_key to see the classification explanation"
        )
    
    with col_filter:
        st.write("")  # Spacer for alignment
        filter_table = st.checkbox(
            "Filter table",
            value=True,
            help="Show only rows for the searched feature in the table above"
        )
    
    # Update table if feature searched and filter active
    if feature_to_explain and filter_table:
        df_filtered_search = df_display[df_display['feature_key'] == feature_to_explain]
        
        if len(df_filtered_search) > 0:
            st.info(f"üìå Table filtered for feature: **{feature_to_explain}** ({len(df_filtered_search)} rows)")
            st.dataframe(
                df_filtered_search, 
                use_container_width=True, 
                height=min(200, len(df_filtered_search) * 35 + 38),  # Dynamic height
                column_config={
                    "prompt": st.column_config.TextColumn(
                        "prompt",
                        width="medium",
                        help="Prompt text"
                    )
                }
            )
        else:
            st.warning(f"‚ö†Ô∏è No rows found for feature '{feature_to_explain}' in filtered results")
    
    if feature_to_explain:
        # Find the feature
        feature_data = df_classified[df_classified['feature_key'] == feature_to_explain]
        
        if len(feature_data) == 0:
            st.warning(f"‚ö†Ô∏è Feature '{feature_to_explain}' not found in dataset")
        else:
            # Take the first record (all have same classification for feature_key)
            record = feature_data.iloc[0]
            
            # Extract aggregate metrics (recalculate if necessary)
            feature_group = df_classified[df_classified['feature_key'] == feature_to_explain]
            feature_metrics_df = node_grouping.aggregate_feature_metrics(feature_group)
            
            if len(feature_metrics_df) > 0:
                metrics = feature_metrics_df.iloc[0]
                
                # Box with general info
                st.info(f"""
                **Feature**: `{feature_to_explain}`  
                **Classification**: **{record['pred_label']}**  
                **Subtype**: {record['subtype'] if pd.notna(record['subtype']) else 'N/A'}  
                **Confidence**: {record['confidence']:.2f}  
                **Review**: {'‚ö†Ô∏è Yes' if record['review'] else '‚úÖ No'}
                """)
                
                # Key metrics
                st.write("**üìä Aggregate Metrics**:")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Layer", int(metrics['layer']))
                with col2:
                    st.metric("Peak Consistency", f"{metrics['peak_consistency_main']:.2f}")
                with col3:
                    st.metric("N Distinct Peaks", int(metrics['n_distinct_peaks']))
                with col4:
                    st.metric("Func vs Sem %", f"{metrics['func_vs_sem_pct']:.1f}%")
                
                col5, col6, col7, col8 = st.columns(4)
                with col5:
                    st.metric("Conf F", f"{metrics['conf_F']:.2f}")
                with col6:
                    st.metric("Conf S", f"{metrics['conf_S']:.2f}")
                with col7:
                    st.metric("Sparsity", f"{metrics['sparsity_median']:.2f}")
                with col8:
                    st.metric("K Semantic", int(metrics['K_sem_distinct']))
                
                # Generate explanation
                st.write("**üí° Classification Explanation**:")
                
                pred_label = record['pred_label']
                layer = int(metrics['layer'])
                peak_cons = metrics['peak_consistency_main']
                n_peaks = int(metrics['n_distinct_peaks'])
                func_vs_sem = metrics['func_vs_sem_pct']
                conf_F = metrics['conf_F']
                conf_S = metrics['conf_S']
                sparsity = metrics['sparsity_median']
                
                # Generate explanation based on class
                if pred_label == "Semantic":
                    # Determine which rule was triggered
                    if peak_cons >= custom_thresholds['dict_peak_consistency_min'] and n_peaks <= custom_thresholds['dict_n_distinct_peaks_max']:
                        explanation = f"""
The feature **{feature_to_explain}** was classified as **Semantic (Dictionary)** because:

1. **Peak Consistency** = {peak_cons:.2f} (‚â• {custom_thresholds['dict_peak_consistency_min']:.2f} ‚úÖ)
   - The main token is peak in **{peak_cons*100:.0f}%** of cases when it appears in the prompt
   - Indicates a highly selective feature on a specific token

2. **N Distinct Peaks** = {n_peaks} (‚â§ {custom_thresholds['dict_n_distinct_peaks_max']} ‚úÖ)
   - The feature always activates on the same token
   - Typical behavior of "dictionary" features (e.g. always on "Texas")

**Rule applied**: Dictionary Semantic (highest priority)
                        """
                    elif layer <= custom_thresholds['sem_layer_max']:
                        explanation = f"""
The feature **{feature_to_explain}** was classified as **Semantic (Dictionary fallback)** because:

1. **Layer** = {layer} (‚â§ {custom_thresholds['sem_layer_max']} ‚úÖ)
   - Low layer typical of basic semantic features
   - Conservative fallback for low layers

2. **Confidence S** = {conf_S:.2f}
   - Fraction of peaks on semantic tokens: {conf_S*100:.0f}%

**Rule applied**: Semantic Concept (low layer fallback)
                        """
                    elif func_vs_sem < custom_thresholds['sem_func_vs_sem_max']:
                        explanation = f"""
The feature **{feature_to_explain}** was classified as **Semantic (Concept)** because:

1. **Func vs Sem %** = {func_vs_sem:.1f}% (< {custom_thresholds['sem_func_vs_sem_max']:.1f}% ‚úÖ)
   - The difference between max activation on functional vs semantic is small
   - Indicates the feature activates mainly on semantic tokens

2. **Confidence S** = {conf_S:.2f}
   - Fraction of peaks on semantic tokens: {conf_S*100:.0f}%

3. **Layer** = {layer}
   - Medium layer, typical of conceptual features

**Rule applied**: Semantic Concept
                        """
                    else:
                        explanation = f"""
The feature **{feature_to_explain}** was classified as **Semantic (Concept)** because:

1. **Confidence S** = {conf_S:.2f} (‚â• {custom_thresholds['sem_conf_s_min']:.2f} ‚úÖ)
   - Fraction of peaks on semantic tokens: {conf_S*100:.0f}%
   - Dominance of semantic tokens

2. **Layer** = {layer}

**Rule applied**: Semantic Concept
                        """
                
                elif pred_label == 'Say "X"':
                    explanation = f"""
The feature **{feature_to_explain}** was classified as **Say "X"** because:

1. **Func vs Sem %** = {func_vs_sem:.1f}% (‚â• {custom_thresholds['sayx_func_vs_sem_min']:.1f}% ‚úÖ)
   - Max activation on functional tokens is **{func_vs_sem:.1f}%** higher than on semantic
   - Indicates strong preference for functional tokens (e.g. "is", ",")

2. **Confidence F** = {conf_F:.2f} (‚â• {custom_thresholds['sayx_conf_f_min']:.2f} ‚úÖ)
   - Fraction of peaks on functional tokens: {conf_F*100:.0f}%
   - Almost all peaks are on functional tokens

3. **Layer** = {layer} (‚â• {custom_thresholds['sayx_layer_min']} ‚úÖ)
   - High layer typical of predictive features
   - Say X features are typically in final layers

**Rule applied**: Say "X" (predicts next token)
                    """
                
                elif pred_label == "Relationship":
                    explanation = f"""
The feature **{feature_to_explain}** was classified as **Relationship** because:

1. **Sparsity** = {sparsity:.2f} (< {custom_thresholds['rel_sparsity_max']:.2f} ‚úÖ)
   - Low sparsity indicates **diffuse** activation in the prompt
   - The feature activates on multiple tokens, not concentrated on one

2. **K Semantic** = {int(metrics['K_sem_distinct'])}
   - Number of distinct semantic tokens it activates on
   - Indicates connection between multiple concepts

3. **Layer** = {layer}
   - Medium-low layer typical of relational features

**Rule applied**: Relationship (connects multiple concepts)
                    """
                
                else:
                    explanation = f"""
The feature **{feature_to_explain}** requires **manual review**.

**Reason**: {record['why_review']}

**Metrics**:
- Layer: {layer}
- Peak Consistency: {peak_cons:.2f}
- Func vs Sem %: {func_vs_sem:.1f}%
- Confidence F/S: {conf_F:.2f} / {conf_S:.2f}
- Sparsity: {sparsity:.2f}
                    """
                
                st.markdown(explanation)
                
                # Show applied decision tree
                with st.expander("üå≥ Complete Decision Tree", expanded=False):
                    st.markdown(f"""
**Evaluation order**:

1. ‚úÖ **Dictionary Semantic**: peak_consistency ‚â• {custom_thresholds['dict_peak_consistency_min']:.2f} AND n_distinct_peaks ‚â§ {custom_thresholds['dict_n_distinct_peaks_max']}
   - Result: {'‚úÖ MATCH' if pred_label == 'Semantic' and peak_cons >= custom_thresholds['dict_peak_consistency_min'] and n_peaks <= custom_thresholds['dict_n_distinct_peaks_max'] else '‚ùå No match'}

2. ‚úÖ **Say "X"**: func_vs_sem ‚â• {custom_thresholds['sayx_func_vs_sem_min']:.1f}% AND conf_F ‚â• {custom_thresholds['sayx_conf_f_min']:.2f} AND layer ‚â• {custom_thresholds['sayx_layer_min']}
   - Result: {'‚úÖ MATCH' if pred_label == 'Say "X"' else '‚ùå No match'}

3. ‚úÖ **Relationship**: sparsity < {custom_thresholds['rel_sparsity_max']:.2f}
   - Result: {'‚úÖ MATCH' if pred_label == 'Relationship' else '‚ùå No match'}

4. ‚úÖ **Semantic (Concept)**: layer ‚â§ {custom_thresholds['sem_layer_max']} OR conf_S ‚â• {custom_thresholds['sem_conf_s_min']:.2f} OR func_vs_sem < {custom_thresholds['sem_func_vs_sem_max']:.1f}%
   - Result: {'‚úÖ MATCH' if pred_label == 'Semantic' else '‚ùå No match'}

5. ‚ö†Ô∏è **Review**: Ambiguous cases

**Final classification**: **{pred_label}**
                    """)
            else:
                st.error("‚ùå Unable to calculate aggregate metrics for this feature")
    
    # Download
    csv_step2 = df_classified.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üíæ Download CSV Step 2",
        data=csv_step2,
        file_name=f"node_grouping_step2_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )

# ===== STEP 3: NAMING =====

st.header("üè∑Ô∏è Step 3: Supernode Naming")

with st.expander("‚ÑπÔ∏è What does this step do?", expanded=False):
    st.markdown("""
    **Generate descriptive names** for each supernode:
    
    **Naming Rules**:
    - **Relationship**: `"(X) related"` where X is the first semantic token with max activation from the original prompt
      - Requires JSON activations for accuracy
      - Fallback: uses peak_token from record with max activation
    
    - **Semantic**: Name of the token with max activation
      - E.g.: "Texas", "city", "capital"
      - Preserves capitalization if present in at least one occurrence
      - Edge cases: "punctuation", "Semantic (unknown)"
    
    - **Say X**: `"Say (X)"` where X is the target_token from record with max activation
      - E.g.: "Say (Austin)", "Say (capital)"
      - Tie-break: shorter distance, then backward > forward
      - Fallback: "Say (?)" if no target found
    
    **Token Blacklist** (NEW):
    - If a token is in the blacklist, it is automatically skipped
    - The system falls back to the token with the second (or next) highest activation
    - Useful for excluding generic or uninformative tokens (e.g. "the", "a", "is")
    - Configurable in the "Token Blacklist" sidebar
    
    **Normalization**:
    - Strip whitespace
    - Remove trailing punctuation (e.g. "entity:" ‚Üí "entity")
    - Preserve capitalization if present (e.g. "Texas" not "texas")
    """)

if 'df_classified' not in st.session_state:
    st.warning("‚ö†Ô∏è Run Step 2 first")
else:
    if st.button("‚ñ∂Ô∏è Run Step 3", key="run_step3"):
        with st.spinner("Naming supernodes..."):
            try:
                # Save temporary JSON if available
                json_path = None
                if tokens_json:
                    json_path = Path("temp_activations.json")
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(tokens_json, f)
                
                # Determine path to Graph JSON
                graph_path = None
                if graph_to_use:
                    if isinstance(graph_to_use, Path):
                        graph_path = str(graph_to_use)
                    else:
                        # If uploaded file, save temporarily
                        graph_path = Path("temp_graph.json")
                        graph_to_use.seek(0)  # Reset file pointer to beginning
                        graph_json_content = json.loads(graph_to_use.read().decode('utf-8'))
                        with open(graph_path, 'w', encoding='utf-8') as f:
                            json.dump(graph_json_content, f)
                    
                    # Save graph_to_use in session state for Neuronpedia upload
                    st.session_state['graph_json_uploaded'] = graph_to_use
                
                df_named = name_nodes(
                    st.session_state['df_classified'],
                    activations_json_path=str(json_path) if json_path else None,
                    graph_json_path=graph_path,
                    blacklist_tokens=blacklist_tokens if blacklist_tokens else None,
                    verbose=False
                )
                
                # Remove temporary files
                if json_path and json_path.exists():
                    json_path.unlink()
                if graph_path and Path(graph_path).name == "temp_graph.json" and Path(graph_path).exists():
                    Path(graph_path).unlink()
                
                # Save in session state
                st.session_state['df_named'] = df_named
                
                # Statistics
                n_features = df_named['feature_key'].nunique()
                n_unique_names = df_named.groupby('feature_key')['supernode_name'].first().nunique()
                
                st.success("‚úÖ Step 3 completed!")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Total Features", n_features)
                with col2:
                    st.metric("Unique Names", n_unique_names)
                
                # Examples per class (compact, no duplicates)
                st.subheader("üìù Naming Examples by Class")
                
                for label in ['Relationship', 'Semantic', 'Say "X"']:
                    # Get unique names (no duplicates)
                    examples = df_named[df_named['pred_label'] == label].groupby('feature_key')['supernode_name'].first().unique()
                    if len(examples) > 0:
                        # Limit to max 5 examples
                        examples_str = ', '.join([f'"{ex}"' for ex in examples[:5]])
                        st.write(f"**{label}**: {examples_str}")
                
            except Exception as e:
                st.error(f"‚ùå Step 3 error: {e}")
                import traceback
                st.code(traceback.format_exc())

# Show Step 3 results
if 'df_named' in st.session_state:
    df_named = st.session_state['df_named']
    
    st.subheader("üìä Step 3 Results (Final)")
    
    # Filter by class
    selected_classes_final = st.multiselect(
        "Filter by class (final)",
        options=df_named['pred_label'].unique(),
        default=df_named['pred_label'].unique(),
        key="filter_final"
    )
    
    df_filtered_final = df_named[df_named['pred_label'].isin(selected_classes_final)]
    
    # Reorder columns
    priority_cols = [
        'feature_key', 'layer', 'prompt', 'supernode_label', 'supernode_name', 'pred_label', 
        'subtype', 'peak_token','activation_max', 'target_tokens'
    ]
    
    # Remaining columns (exclude those already in priority)
    other_cols = [col for col in df_filtered_final.columns if col not in priority_cols]
    
    # Final order: priority + others
    ordered_cols = [col for col in priority_cols if col in df_filtered_final.columns] + other_cols
    df_display_final = df_filtered_final[ordered_cols]
    
    # Complete table with all columns
    st.write(f"**Complete results** ({len(df_filtered_final)} rows, {len(df_filtered_final.columns)} columns):")
    st.dataframe(
        df_display_final, 
        use_container_width=True, 
        height=400,
        column_config={
            "prompt": st.column_config.TextColumn(
                "prompt",
                width="medium",
                help="Prompt text"
            )
        }
    )
    
    # Group by supernode_name
    st.subheader("üîç Analysis by Supernode Name")
    
    # Calculate node_influence per feature (take 1 value per feature, not all rows)
    if 'node_influence' in df_named.columns:
        # Take node_influence for each feature_key (use first value, all equal for same feature)
        feature_influence = df_named.groupby('feature_key')['node_influence'].first().reset_index()
        
        # Add supernode_name for each feature
        feature_to_name = df_named.groupby('feature_key')['supernode_name'].first().reset_index()
        feature_influence = feature_influence.merge(feature_to_name, on='feature_key')
        
        # Sum node_influence per supernode_name
        name_influence = feature_influence.groupby('supernode_name')['node_influence'].sum().reset_index()
        name_influence.columns = ['supernode_name', 'total_influence']
    else:
        name_influence = None
    
    # Base aggregations
    name_groups = df_named.groupby('supernode_name').agg({
        'feature_key': 'nunique',
        'pred_label': lambda x: x.mode()[0] if len(x) > 0 else '',
        'layer': lambda x: f"{x.min()}-{x.max()}" if x.min() != x.max() else str(x.min())
    }).reset_index()
    name_groups.columns = ['Supernode Name', 'N Features', 'Class', 'Layer Range']
    
    # Add total_influence if available
    if name_influence is not None:
        name_groups = name_groups.merge(
            name_influence.rename(columns={'supernode_name': 'Supernode Name', 'total_influence': 'Total Influence'}),
            on='Supernode Name',
            how='left'
        )
        # Sort by Total Influence (descending)
        name_groups = name_groups.sort_values('Total Influence', ascending=False)
    else:
        name_groups = name_groups.sort_values('N Features', ascending=False)
    
    st.dataframe(name_groups, use_container_width=True)
    
    # Final download
    st.subheader("üíæ Download Results")
    
    col1, col2 = st.columns(2)
    
    with col1:
        csv_final = df_named.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• Download Complete CSV",
            data=csv_final,
            file_name=f"node_grouping_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    
    with col2:
        # Export summary JSON
        summary = {
            'timestamp': datetime.now().isoformat(),
            'n_features': int(df_named['feature_key'].nunique()),
            'n_unique_names': int(df_named.groupby('feature_key')['supernode_name'].first().nunique()),
            'class_distribution': df_named.groupby('feature_key')['pred_label'].first().value_counts().to_dict(),
            'thresholds_used': custom_thresholds,
            'top_supernodes': name_groups.head(10).to_dict('records')
        }
        
        json_summary = json.dumps(summary, indent=2).encode('utf-8')
        st.download_button(
            label="üì• Download Summary JSON",
            data=json_summary,
            file_name=f"node_grouping_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
    
    # Upload to Neuronpedia
    st.divider()
    st.subheader("üåê Upload to Neuronpedia")
    
    st.info("Upload the subgraph with supernodes to Neuronpedia for interactive visualization.")
    
    # API Key input (load from .env if available)
    default_api_key = os.getenv("NEURONPEDIA_API_KEY", "")
    api_key = st.text_input(
        "Neuronpedia API Key",
        value=default_api_key,
        type="password",
        help="Enter your Neuronpedia API key (required for upload). Can be auto-loaded from .env"
    )
    
    # Display name
    display_name = st.text_input(
        "Display Name",
        value=f"Node Grouping - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        help="Display name for the subgraph on Neuronpedia"
    )
    
    # Overwrite ID (optional)
    overwrite_id = st.text_input(
        "Overwrite ID (optional)",
        value="",
        help="If provided, overwrites an existing subgraph instead of creating a new one"
    )
    
    # Verify we have Graph JSON
    graph_json_available = st.session_state.get('graph_json_uploaded') is not None
    
    if not graph_json_available:
        st.warning("‚ö†Ô∏è Graph JSON not loaded. Load the Graph JSON in Step 3 to enable upload.")
    
    # Upload button
    if st.button("üöÄ Upload to Neuronpedia", disabled=not (api_key and graph_json_available)):
        if not api_key:
            st.error("‚ùå Enter your API Key!")
        elif not graph_json_available:
            st.error("‚ùå Load the Graph JSON before proceeding!")
        else:
            try:
                # Save Graph JSON temporarily
                graph_to_use = st.session_state.get('graph_json_uploaded')
                
                if isinstance(graph_to_use, Path):
                    graph_path = str(graph_to_use)
                else:
                    # If uploaded file, save temporarily
                    graph_path = "temp_graph_upload.json"
                    graph_to_use.seek(0)  # Reset file pointer to beginning
                    graph_json_content = json.loads(graph_to_use.read().decode('utf-8'))
                    with open(graph_path, 'w', encoding='utf-8') as f:
                        json.dump(graph_json_content, f)
                
                # Import upload function
                import sys
                import importlib.util
                spec = importlib.util.spec_from_file_location("node_grouping", "scripts/02_node_grouping.py")
                node_grouping = importlib.util.module_from_spec(spec)
                sys.modules["node_grouping"] = node_grouping
                spec.loader.exec_module(node_grouping)
                upload_subgraph_to_neuronpedia = node_grouping.upload_subgraph_to_neuronpedia
                
                # Upload
                with st.spinner("Uploading to Neuronpedia..."):
                    # Retrieve selected_nodes_data from session state if available
                    selected_nodes_data = st.session_state.get('selected_nodes_data')
                    
                    result = upload_subgraph_to_neuronpedia(
                        df_grouped=df_named,
                        graph_json_path=graph_path,
                        api_key=api_key,
                        display_name=display_name if display_name else None,
                        overwrite_id=overwrite_id if overwrite_id else None,
                        selected_nodes_data=selected_nodes_data,
                        verbose=False
                    )
                
                # Remove temporary file
                if Path(graph_path).name == "temp_graph_upload.json" and Path(graph_path).exists():
                    Path(graph_path).unlink()
                
                st.success("‚úÖ Upload completed!")
                st.json(result)
                
                # Link to subgraph (if available in response)
                if 'url' in result:
                    st.markdown(f"üîó [View on Neuronpedia]({result['url']})")
                
            except Exception as e:
                st.error(f"‚ùå Upload error: {e}")
                import traceback
                st.code(traceback.format_exc())
                
                # Display debug payload if it exists
                debug_payload_path = Path("output") / "debug_neuronpedia_payload.json"
                if debug_payload_path.exists():
                    st.info("Debug: payload saved to output/debug_neuronpedia_payload.json")
                    with open(debug_payload_path, 'r', encoding='utf-8') as f:
                        payload_data = json.load(f)
                    
                    with st.expander("View sent payload"):
                        st.json(payload_data)

# ===== FOOTER =====

st.divider()

st.markdown("""
### üìö References

- **Script**: `scripts/02_node_grouping.py`
- **Documentation**: `output/STEP3_READY_FOR_REVIEW.md`
- **Tests**: `tests/test_node_naming.py`

### üí° Tips

- **Relationship**: Always provide the JSON activations for accurate naming
- **Thresholds**: Start with default values, then refine based on results
- **Review**: Manually check features with `review=True`
- **Iteration**: You can re-run Steps 2 and 3 with different thresholds without redoing Step 1
""")

