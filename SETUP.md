# Environment Setup - Circuit Tracer + Probe Rover

**Pipeline v2.0.0-clean**

---

## Quick Installation

### 1. Create virtual environment and install dependencies

**Windows PowerShell:**
```powershell
# Run automatic setup script
.\setup_venv.ps1
```

**Manual installation:**
```bash
# Create venv
python -m venv .venv

# Activate venv (Windows)
.\.venv\Scripts\Activate.ps1

# Activate venv (Linux/Mac)
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

Create a `.env` file in the project root:

```env
NEURONPEDIA_API_KEY='your-neuronpedia-key-here'
OPENAI_API_KEY='your-openai-key-here'
```

**Where to get keys:**
- Neuronpedia: https://www.neuronpedia.org/api-key
- OpenAI: https://platform.openai.com/api-keys

### 3. Verify installation

```bash
# Test basic imports
python -c "import streamlit, pandas, numpy; print('OK')"

# Test torch (optional, for graph visualization)
python -c "import torch; print(f'Torch: {torch.__version__}')"
```

---

## Main Dependencies

- **streamlit**: Interactive UI
- **pandas**: Data manipulation
- **plotly**: Interactive visualizations
- **requests**: API calls
- **openai**: Concept generation
- **python-dotenv**: Environment variable management
- **torch**: (Optional) For advanced graph visualization

See `requirements.txt` for the complete list.

---

## Launch Application

### Method 1: Automatic script (Windows)

```powershell
.\start_streamlit.ps1
```

This script:
1. Automatically activates the venv
2. Starts Streamlit
3. Opens browser at `http://localhost:8501`

### Method 2: Manual

```bash
# Activate venv
source .venv/bin/activate  # Linux/Mac
# or
.\.venv\Scripts\Activate.ps1  # Windows

# Start Streamlit
streamlit run eda/app.py
```

### Method 3: Directly from eda folder

```bash
cd eda
streamlit run app.py
```

---

## Project Structure

```
circuit_tracer-prompt_rover/
├── eda/                              # Streamlit application
│   ├── app.py                        # Main app (homepage)
│   ├── pages/
│   │   ├── 00_Graph_Generation.py    # Stage 1: Graph generation
│   │   ├── 01_Probe_Prompts.py       # Stage 2: Probe prompting
│   │   └── 02_Node_Grouping.py       # Stage 3: Node grouping
│   ├── components/                   # UI components
│   ├── config/                       # Default configuration
│   └── README.md                     # Complete documentation
│
├── scripts/                          # Standalone Python scripts
│   ├── 00_neuronpedia_graph_generation.py
│   ├── 01_probe_prompts.py
│   ├── 02_node_grouping.py
│   └── causal_utils.py
│
├── tests/                            # Test suite
│   ├── test_node_naming.py
│   ├── test_probe_prompts_api.py
│   └── ...
│
├── output/                           # Files generated by pipeline
│   ├── graph_data/                   # Attribution graphs JSON
│   ├── checkpoints/                  # Probe prompts checkpoints
│   └── *.csv                         # Exports and results
│
├── docs/                             # Documentation
│   ├── Anthropic_circuit_tracing.md.txt
│   └── archive_old_pipeline/         # Old pipeline (archived)
│
├── .env                              # API keys (do not commit!)
├── requirements.txt                  # Python dependencies
├── setup_venv.ps1                    # Automatic setup script
├── start_streamlit.ps1               # Streamlit launch script
├── SETUP.md                          # This file
└── readme.md                         # Main README
```

---

## Pipeline Workflow

The pipeline consists of 3 sequential stages:

### Stage 1: Graph Generation
- Generate attribution graph on Neuronpedia
- Extract static metrics (node_influence, frac_external)
- Select relevant features

### Stage 2: Probe Prompts
- Generate semantically related concepts
- Get activations via API for each feature
- Calculate activation pattern metrics

### Stage 3: Node Grouping
- Classify features into supernodes (Semantic/Say X/Relationship)
- Assign automatic names
- Upload to Neuronpedia for visualization

For complete details, see `eda/README.md` (600+ lines).

---

## Troubleshooting

### PowerShell Execution Policy Error

If you get an error running `setup_venv.ps1` or `start_streamlit.ps1`:

```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### API Key Error

**Error:** `API key not found` or `Invalid API key`

**Solution:**
1. Verify that `.env` exists in the root
2. Check format: `NEURONPEDIA_API_KEY='sk-np-...'`
3. Restart Streamlit after modifying `.env`

### Torch Installation Issues

If you have problems with torch:

```bash
# CPU only (lighter, sufficient for pipeline)
pip install torch --index-url https://download.pytorch.org/whl/cpu

# With CUDA (NVIDIA GPU, optional)
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

**Note:** Torch is optional. The pipeline works without it.

### Import Errors

Make sure you're in the project root directory and venv is active:

```bash
# Verify directory
pwd  # Linux/Mac
Get-Location  # Windows

# Verify venv is active (you should see (.venv) in prompt)
echo $VIRTUAL_ENV  # Linux/Mac
Write-Host $env:VIRTUAL_ENV  # Windows
```

### Unicode Errors (Windows)

If you see encoding errors:

```powershell
$env:PYTHONIOENCODING='utf-8'
streamlit run eda/app.py
```

### Streamlit Not Found

If `streamlit` is not found:

```bash
# Verify installation
pip list | grep streamlit

# Reinstall if needed
pip install --upgrade streamlit
```

---

## Advanced Configuration

### Custom Port

To use a different port than 8501:

```bash
streamlit run eda/app.py --server.port 8080
```

### Disable Auto-Open Browser

```bash
streamlit run eda/app.py --server.headless true
```

### Streamlit Configuration File

Create `.streamlit/config.toml` for persistent configurations:

```toml
[server]
port = 8501
headless = false

[theme]
primaryColor = "#FF6B6B"
```

---

## Next Steps

1. ✅ **Install environment** (this document)
2. 📖 **Read complete documentation**: `eda/README.md`
3. 🚀 **Launch Streamlit**: `streamlit run eda/app.py`
4. 🌐 **Stage 1**: Generate an attribution graph
5. 🔍 **Stage 2**: Analyze with probe prompts
6. 🔗 **Stage 3**: Classify and name supernodes

---

## Useful Links

- **Complete Documentation**: `eda/README.md`
- **Main README**: `readme.md`
- **Circuit Tracer** (Anthropic): https://github.com/safety-research/circuit-tracer
- **Neuronpedia**: https://www.neuronpedia.org
- **Paper**: https://transformer-circuits.pub/2025/attribution-graphs/

---

## Notes

- **Python Version**: Requires Python 3.8+
- **OS**: Windows, Linux, Mac (tested on all three)
- **GPU**: Not required (optional for advanced visualizations)
- **RAM**: Minimum 4GB recommended
- **Disk Space**: ~500MB for dependencies, ~1-2GB for output graphs

---

**Version:** 2.0.0-clean  
**Last Updated:** October 2025
