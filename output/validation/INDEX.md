# Validation Directory Index

**Last Updated**: 2025-10-27

---

## ğŸ¯ Quick Start

### For Paper Writing â†’ Start Here:
ğŸ“„ **`CROSS_PROMPT_ROBUSTNESS_MASTER.md`** â­

This master file explains both robustness methods and provides ready-to-use text for your paper.

---

## ğŸ“‚ Main Analyses

### â­ Cross-Prompt Robustness (Dual-Level Validation)

**Overview**: `CROSS_PROMPT_ROBUSTNESS_MASTER.md`

Two complementary methods testing robustness at different levels:

#### 1. **feature_level_robustness/** - Computational Stability
**Question**: Do individual features maintain stable activation patterns?

**Method**: Activation overlap analysis

**Key Results**:
- 100% feature survival (25/25)
- Perfect activation overlap (1.000)
- 96% peak token consistency

**Start here**: `feature_level_robustness/CROSS_PROMPT_EVIDENCE.md`

---

#### 2. **supernode_level_robustness/** - Semantic Disentanglement  
**Question**: Do conceptual groupings transfer appropriately?

**Method**: Supernode transfer analysis

**Key Results**:
- 100% universal concept transfer (7/7)
- 100% entity separation (8/8)
- 94% activation stability
- 96% grouping consistency

**Start here**: `supernode_level_robustness/PAPER_READY_SUMMARY_CORRECTED.md` â­

---

### ğŸ“Š **baseline comparison/** - Method Validation
**Purpose**: Compares concept-aligned grouping vs geometric baselines

**Key Results**:
- +132% peak token consistency vs cosine similarity
- +486% activation similarity vs cosine similarity

**Start here**: `baseline comparison/CLAIM_EVIDENCE.md`

---

## ğŸ“„ Root-Level Documents

### Primary References
- **`CROSS_PROMPT_ROBUSTNESS_MASTER.md`** â­ - Overview of both methods
- `PUBLICATION_READY_CLAIMS.md` - All claims for paper
- `CORRECTED_CLAIMS_HONEST_ASSESSMENT.md` - Honest assessment

### Technical Documentation  
- `README_CORRECTED.md` - Corrected README after reviews
- `STATISTICAL_REVIEW_RESPONSE.md` - Response to statistical concerns

---

## ğŸ¯ Navigation by Task

### Writing Abstract
â†’ See "Combined Results for Paper" in `CROSS_PROMPT_ROBUSTNESS_MASTER.md`

### Writing Results Section 5.5
â†’ Use paragraphs 1-3 from `CROSS_PROMPT_ROBUSTNESS_MASTER.md`  
â†’ Plus Table 3 from `supernode_level_robustness/PAPER_READY_SUMMARY_CORRECTED.md`

### Writing Discussion
â†’ Use "Interpretation" from `CROSS_PROMPT_ROBUSTNESS_MASTER.md`

### Creating Figures
â†’ `supernode_level_robustness/cross_prompt_robustness_*.png` (comprehensive)  
â†’ Or combine: `feature_level_robustness/robustness_summary.png` + supernode figure

### Verifying Claims
â†’ `supernode_level_robustness/VERIFIED_CLAIMS.md` (all numbers verified)  
â†’ `feature_level_robustness/cross_prompt_robustness_dallas_oakland.json` (raw data)

---

## ğŸ“Š Summary Statistics (Quick Reference)

### Feature-Level Robustness
| Metric | Value |
|--------|-------|
| Survival Rate | 100% (25/25) |
| Activation Overlap | 1.000 (perfect) |
| Peak Token Consistency | 96% |
| Layer Distribution | p=1.000 (stable) |

### Supernode-Level Robustness
| Metric | Value |
|--------|-------|
| Universal Transfer | 100% (7/7) |
| Entity Separation | 100% (8/8) |
| Activation Stability | 94% |
| Grouping Consistency | 96% (corrected) |

---

## ğŸ”¬ Analysis Scripts

Located in `scripts/experiments/`:

### Cross-Prompt Robustness
- `cross_prompt_robustness.py` - Feature-level analysis
- `analyze_cross_prompt_robustness.py` - Supernode-level analysis
- `run_cross_prompt_analysis.py` - Quick runner
- `verify_claims.py` - Claim verification

### Baseline Comparison
- `compare_grouping_methods.py` - Method comparison
- `visualize_comparison.py` - Visualization

### Documentation
- `README_CROSS_PROMPT_ROBUSTNESS.md` - Full methodology

---

## ğŸ”„ Reproduction

### Feature-Level Analysis
```bash
python scripts/experiments/cross_prompt_robustness.py \
    --prompt1_csv "output/examples/Dallas/node_grouping_final_*.csv" \
    --prompt2_csv "output/examples/capital oakland/node_grouping_final_*.csv" \
    --prompt1_name "Dallas" --prompt2_name "Oakland"
```

### Supernode-Level Analysis
```bash
cd scripts/experiments
python run_cross_prompt_analysis.py
```

### Verify All Claims
```bash
cd scripts/experiments
python verify_claims.py
```

---

## ğŸ“‚ Complete Directory Structure

```
output/validation/
â”œâ”€â”€ INDEX.md (this file)
â”œâ”€â”€ CROSS_PROMPT_ROBUSTNESS_MASTER.md â­ START HERE
â”‚
â”œâ”€â”€ feature_level_robustness/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ CROSS_PROMPT_EVIDENCE.md
â”‚   â”œâ”€â”€ CROSS_PROMPT_ROBUSTNESS_REPORT.md
â”‚   â”œâ”€â”€ MASTER_VALIDATION_SUMMARY.md
â”‚   â”œâ”€â”€ cross_prompt_robustness_dallas_oakland.json
â”‚   â”œâ”€â”€ robustness_summary.png
â”‚   â”œâ”€â”€ robustness_feature_analysis.png
â”‚   â””â”€â”€ entity_mapping_diagram.png
â”‚
â”œâ”€â”€ supernode_level_robustness/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ PAPER_READY_SUMMARY_CORRECTED.md â­
â”‚   â”œâ”€â”€ VERIFIED_CLAIMS.md
â”‚   â”œâ”€â”€ DOUBLE_CHECK_RESULTS.md
â”‚   â”œâ”€â”€ supernode_transfer_*.csv
â”‚   â”œâ”€â”€ activation_similarity_*.csv
â”‚   â”œâ”€â”€ cross_prompt_report_*.md
â”‚   â”œâ”€â”€ cross_prompt_robustness_*.png
â”‚   â””â”€â”€ figure_supernode_transfer_matrix.png
â”‚
â”œâ”€â”€ baseline comparison/
â”‚   â”œâ”€â”€ CLAIM_EVIDENCE.md
â”‚   â”œâ”€â”€ coherence_comparison.png
â”‚   â”œâ”€â”€ improvement_chart.png
â”‚   â””â”€â”€ summary_table.png
â”‚
â”œâ”€â”€ PUBLICATION_READY_CLAIMS.md
â”œâ”€â”€ CORRECTED_CLAIMS_HONEST_ASSESSMENT.md
â”œâ”€â”€ README_CORRECTED.md
â””â”€â”€ STATISTICAL_REVIEW_RESPONSE.md
```

---

## âš ï¸ Important Notes

### Which Method for Which Claim?

**Feature-Level** (activation overlap):
- âœ… "Features are computationally stable"
- âœ… "Activation patterns consistent across probes"
- âœ… "100% survival rate"

**Supernode-Level** (concept transfer):
- âœ… "Concepts are semantically disentangled"
- âœ… "Universal vs entity discrimination"
- âœ… "96% grouping consistency"

**Both Together**:
- âœ… "Robust, generalizable concept discovery"
- âœ… "Evidence against overfitting"
- âœ… "Dual-level validation"

### Don't Mix Methods!
When citing statistics, be clear which method you're referencing:
- "Feature-level survival rate: 100%"
- "Supernode-level transfer rate: 100%"

These are different metrics from different analyses!

---

## ğŸ¯ Bottom Line

**You have TWO complementary validations**:
1. Features are computationally robust â†’ Feature-level analysis
2. Concepts are semantically organized â†’ Supernode-level analysis

**Use both** in your paper for strongest evidence.

**Start with**: `CROSS_PROMPT_ROBUSTNESS_MASTER.md` which combines both.

---

## ğŸ“ Need Help?

- **Methodology questions** â†’ See respective README.md files
- **Claims verification** â†’ See `supernode_level_robustness/VERIFIED_CLAIMS.md`
- **Paper text** â†’ See `CROSS_PROMPT_ROBUSTNESS_MASTER.md`
- **Raw data** â†’ CSV files in respective directories
- **Scripts** â†’ See `scripts/experiments/`

---

**Status**: âœ… All analyses complete, verified, publication-ready




