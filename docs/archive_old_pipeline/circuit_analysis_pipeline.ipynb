{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Circuit Analysis Pipeline: Anthropological Feature Analysis\n",
        "\n",
        "**Analisi antropologica completa delle feature di LLM attraverso Circuit Tracing**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Overview\n",
        "\n",
        "Questo notebook implementa l'intera pipeline di analisi delle feature estratte da un attribution graph generato con Circuit Tracer.\n",
        "\n",
        "### Risultati Finali\n",
        "- âœ… **52.3% logit influence coverage** (post influence-first refactoring)\n",
        "- âœ… **23 supernodi** (15 semantici + 8 computazionali)\n",
        "- âœ… **483 feature** coperte nei supernodi\n",
        "- âœ… **2.4% BOS leakage** (controllato)\n",
        "\n",
        "### Metodologia\n",
        "- **Influence-First Filtering**: Ammissione basata su causalitÃ  (`logit_influence`)\n",
        "- **Dual View**: Separazione \"Situational Core\" vs \"Generalizable Scaffold\"\n",
        "- **Supernodi Semantici**: Clustering narrativo con crescita controllata\n",
        "- **Validazione Empirica**: Coverage logit influence + correlazioni metriche\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”„ Workflow Completo\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[ğŸ¯ COLAB: Circuit Tracer] --> B[example_graph.pt]\n",
        "    A --> C[graph_feature_static_metrics.csv]\n",
        "    A --> D[acts_compared.csv]\n",
        "    \n",
        "    B --> E[ğŸ“¥ Download file localmente]\n",
        "    C --> E\n",
        "    D --> E\n",
        "    \n",
        "    E --> F[Step 1: Anthropological Basic]\n",
        "    F --> G[feature_personalities_corrected.json]\n",
        "    F --> H[feature_typology.json]\n",
        "    \n",
        "    G --> I[Step 2: Compute Thresholds]\n",
        "    I --> J[robust_thresholds.json]\n",
        "    \n",
        "    G --> K[Step 3: Cicciotti Supernodes]\n",
        "    J --> K\n",
        "    K --> L[cicciotti_supernodes.json]\n",
        "    \n",
        "    L --> M[Step 4: Final Clustering]\n",
        "    J --> M\n",
        "    M --> N[final_anthropological_optimized.json]\n",
        "    \n",
        "    N --> O[Step 5: Verify Logit Influence]\n",
        "    G --> O\n",
        "    O --> P[logit_influence_validation.json]\n",
        "    \n",
        "    N --> Q[Step 6: Visualizzazioni]\n",
        "    H --> Q\n",
        "    Q --> R[feature_space_3d.png]\n",
        "    Q --> S[neuronpedia_url.txt]\n",
        "    \n",
        "    style A fill:#e1f5ff\n",
        "    style F fill:#fff3e0\n",
        "    style I fill:#fff3e0\n",
        "    style K fill:#fff3e0\n",
        "    style M fill:#fff3e0\n",
        "    style O fill:#e8f5e9\n",
        "    style Q fill:#f3e5f5\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Fase COLAB (Prerequisito)\n",
        "\n",
        "La fase Colab utilizza il modello Gemma-2-2B con Circuit Tracer per generare i file di input.\n",
        "\n",
        "### Output da Colab (da scaricare in `output/`):\n",
        "1. âœ… `example_graph.pt` - Grafo attribution (~167MB)\n",
        "2. âœ… `graph_feature_static_metrics.csv` - Metriche statiche (logit_influence, frac_external)\n",
        "3. âœ… `acts_compared.csv` - Attivazioni su concetti semantici (Dallas, Texas, Capital, etc.)\n",
        "\n",
        "**ğŸ’¡ Questi file devono esistere prima di eseguire questa pipeline.**\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Setup e Verifiche\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importazioni\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Setup\n",
        "OUTPUT_DIR = Path('output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Verifica prerequisiti\n",
        "required_files = [\n",
        "    'output/example_graph.pt',\n",
        "    'output/graph_feature_static_metrics (1).csv',\n",
        "    'output/acts_compared.csv'\n",
        "]\n",
        "\n",
        "print(\"ğŸ“Š Verifica prerequisiti:\\n\")\n",
        "missing = []\n",
        "for f in required_files:\n",
        "    if Path(f).exists():\n",
        "        size_mb = Path(f).stat().st_size / (1024*1024)\n",
        "        print(f\"âœ… {f} ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"âŒ {f} - MANCANTE\")\n",
        "        missing.append(f)\n",
        "\n",
        "if missing:\n",
        "    print(\"\\nâš ï¸ File mancanti! Scaricali da Colab prima di continuare.\")\n",
        "else:\n",
        "    print(\"\\nâœ… Tutti i prerequisiti presenti. Puoi procedere!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“Š Step 1: Anthropological Basic\n",
        "\n",
        "Calcola metriche antropologiche per ogni feature:\n",
        "- `mean_consistency`: generalizzabilitÃ  cross-prompt\n",
        "- `max_affinity`: specializzazione semantica\n",
        "- `conditional_consistency`: consistency quando attiva\n",
        "- `activation_threshold`: soglia adattiva (p75 + Otsu)\n",
        "\n",
        "**Output**: `feature_personalities_corrected.json`, `feature_typology.json`, `quality_scores.json`, `metric_correlations.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ­ STEP 1: ANTHROPOLOGICAL BASIC ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Esegui script\n",
        "exec(open('scripts/02_anthropological_basic.py', encoding='utf-8').read())\n",
        "\n",
        "# Mostra risultati\n",
        "with open('output/feature_personalities_corrected.json', 'r') as f:\n",
        "    personalities = json.load(f)\n",
        "with open('output/feature_typology.json', 'r') as f:\n",
        "    typology = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Risultati:\")\n",
        "print(f\"   Feature totali: {len(personalities)}\")\n",
        "print(f\"\\n   Typology distribution:\")\n",
        "for t, features in typology.items():\n",
        "    print(f\"      {t}: {len(features)} feature\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¯ Step 2: Compute Robust Thresholds\n",
        "\n",
        "Calcola soglie robuste per influence-first filtering:\n",
        "- **Ï„_inf**: max(p90, cutoff_80% cumulata)\n",
        "- **Ï„_aff**: 0.60 (configurabile)\n",
        "- **Ï„_inf_very_high**: p95 (BOS filter)\n",
        "\n",
        "**Criterio**: `admitted = (logit_influence >= Ï„_inf) OR (max_affinity >= Ï„_aff)`\n",
        "\n",
        "**Output**: `robust_thresholds.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ¯ STEP 2: COMPUTE ROBUST THRESHOLDS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "exec(open('scripts/03_compute_thresholds.py', encoding='utf-8').read())\n",
        "\n",
        "# Mostra thresholds\n",
        "with open('output/robust_thresholds.json', 'r') as f:\n",
        "    thresholds = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Thresholds:\")\n",
        "print(f\"   Ï„_inf: {thresholds['tau_inf']:.6f}\")\n",
        "print(f\"   Situational core: {thresholds['situational_core']['n_features']} features ({thresholds['situational_core']['influence_coverage']*100:.1f}% coverage)\")\n",
        "print(f\"   Generalizable scaffold: {thresholds['generalizable_scaffold']['n_features']} features\")\n",
        "print(f\"   BOS leakage: {thresholds['bos_leakage_pct']:.1f}% {'âœ…' if thresholds['bos_leakage_ok'] else 'âŒ'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸŒ± Step 3: Cicciotti Supernodes (Semantic)\n",
        "\n",
        "Costruisce supernodi semantici tramite:\n",
        "1. **Seed selection influence-first**: ordina per (logit_influence, max_affinity)\n",
        "2. **Narrative-guided growth**: compatibilitÃ  = 0.4Â·cosine + 0.3Â·jaccard + 0.2Â·consistency + 0.1Â·layer\n",
        "3. **Coherence tracking**: stop quando coherence < soglia\n",
        "\n",
        "**Output**: `cicciotti_supernodes.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸŒ± STEP 3: CICCIOTTI SUPERNODES (SEMANTIC)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "exec(open('scripts/04_cicciotti_supernodes.py', encoding='utf-8').read())\n",
        "\n",
        "# Mostra supernodi\n",
        "with open('output/cicciotti_supernodes.json', 'r') as f:\n",
        "    cicciotti = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Supernodi Semantici: {len(cicciotti)}\\n\")\n",
        "for name, data in list(cicciotti.items())[:3]:\n",
        "    print(f\"   {name}: {data['narrative_theme']}\")\n",
        "    print(f\"      {data['n_members']} members, influence={data['seed_logit_influence']:.4f}, coherence={data['final_coherence']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”§ Step 4: Final Optimized Clustering\n",
        "\n",
        "Clusterizza feature residue (non nei supernodi semantici) che superano i threshold:\n",
        "- Identifica quality residuals con Ï„_inf/Ï„_aff\n",
        "- Clustering per dominant_token e layer_range\n",
        "- Merge con supernodi semantici\n",
        "\n",
        "**Output**: `final_anthropological_optimized.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ”§ STEP 4: FINAL OPTIMIZED CLUSTERING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "exec(open('scripts/05_final_optimized_clustering.py', encoding='utf-8').read())\n",
        "\n",
        "# Mostra risultati\n",
        "with open('output/final_anthropological_optimized.json', 'r') as f:\n",
        "    final = json.load(f)\n",
        "\n",
        "n_semantic = len(final.get('semantic_supernodes', {}))\n",
        "n_computational = len(final.get('computational_supernodes', {}))\n",
        "total_features = sum(sn['n_members'] for sn in final.get('semantic_supernodes', {}).values())\n",
        "total_features += sum(sn['n_members'] for sn in final.get('computational_supernodes', {}).values())\n",
        "\n",
        "print(f\"\\nğŸ“Š Supernodi Finali:\")\n",
        "print(f\"   Semantic: {n_semantic}\")\n",
        "print(f\"   Computational: {n_computational}\")\n",
        "print(f\"   TOTALE: {n_semantic + n_computational}\")\n",
        "print(f\"   Feature coperte: {total_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Step 5: Verify Logit Influence\n",
        "\n",
        "Valida copertura logit influence dei supernodi:\n",
        "- Calcola % influence totale coperta\n",
        "- Breakdown per feature type\n",
        "- Rating (EXCELLENT/GOOD/MODERATE/WEAK)\n",
        "\n",
        "**Output**: `logit_influence_validation.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"âœ… STEP 5: VERIFY LOGIT INFLUENCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "exec(open('scripts/06_verify_logit_influence.py', encoding='utf-8').read())\n",
        "\n",
        "# Mostra validazione\n",
        "with open('output/logit_influence_validation.json', 'r') as f:\n",
        "    validation = json.load(f)\n",
        "\n",
        "print(f\"\\nğŸ“Š Validazione:\")\n",
        "print(f\"   Coverage: {validation['coverage_percentage']:.1f}%\")\n",
        "print(f\"   Rating: {validation['rating']}\")\n",
        "print(f\"\\n   Breakdown:\")\n",
        "for t, data in validation['type_breakdown'].items():\n",
        "    print(f\"      {t}: {data['count']} features ({data['pct']:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“ˆ Step 6: Visualizzazioni e Export\n",
        "\n",
        "Genera visualizzazioni e export:\n",
        "- Plot 3D spazio typology (mean_consistency Ã— max_affinity Ã— logit_influence)\n",
        "- Export supernodi su Neuronpedia (URL interattivo)\n",
        "\n",
        "**Output**: `feature_space_3d.png`, `neuronpedia_url_improved.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“ˆ STEP 6: VISUALIZZAZIONI E EXPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Visualizzazione 3D\n",
        "try:\n",
        "    exec(open('scripts/visualization/visualize_feature_space_3d.py', encoding='utf-8').read())\n",
        "    print(\"\\nâœ… Visualizzazione 3D: output/feature_space_3d.png\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Errore viz 3D: {e}\")\n",
        "\n",
        "# Export Neuronpedia\n",
        "try:\n",
        "    exec(open('scripts/visualization/neuronpedia_export.py', encoding='utf-8').read())\n",
        "    print(\"âœ… Export Neuronpedia: output/neuronpedia_url_improved.txt\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Errore export: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ‰ Riepilogo Finale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ğŸ‰ PIPELINE COMPLETATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Carica risultati finali\n",
        "with open('output/robust_thresholds.json', 'r') as f:\n",
        "    th = json.load(f)\n",
        "with open('output/final_anthropological_optimized.json', 'r') as f:\n",
        "    final = json.load(f)\n",
        "with open('output/logit_influence_validation.json', 'r') as f:\n",
        "    val = json.load(f)\n",
        "\n",
        "n_sem = len(final.get('semantic_supernodes', {}))\n",
        "n_comp = len(final.get('computational_supernodes', {}))\n",
        "total_feat = sum(s['n_members'] for s in final.get('semantic_supernodes', {}).values())\n",
        "total_feat += sum(s['n_members'] for s in final.get('computational_supernodes', {}).values())\n",
        "\n",
        "print(f\"\\nâœ… Supernodi: {n_sem + n_comp} ({n_sem} semantic + {n_comp} computational)\")\n",
        "print(f\"âœ… Feature coperte: {total_feat}\")\n",
        "print(f\"âœ… Coverage logit influence: {val['coverage_percentage']:.1f}% ({val['rating']})\")\n",
        "print(f\"âœ… BOS leakage: {th['bos_leakage_pct']:.1f}%\")\n",
        "print(f\"\\nğŸ“ Output directory: {OUTPUT_DIR.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š Riferimenti e Documentazione\n",
        "\n",
        "### Metriche Chiave\n",
        "\n",
        "| Metrica | Range | Significato |\n",
        "|---------|-------|-------------|\n",
        "| `mean_consistency` | 0-1 | GeneralizzabilitÃ  cross-prompt |\n",
        "| `max_affinity` | 0-1 | Specializzazione semantica |\n",
        "| `logit_influence` | 0-âˆ | Impatto causale sull'output |\n",
        "\n",
        "### Typology\n",
        "\n",
        "- **Generalist**: Alta consistency + Alta affinity + Bassa influence\n",
        "- **Specialist**: Bassa consistency + Alta affinity + Alta influence\n",
        "- **Computational**: Alta consistency + Bassa affinity\n",
        "- **Hybrid**: Combinazioni miste\n",
        "\n",
        "### Dual View (Influence-First)\n",
        "\n",
        "1. **Situational Core**: `logit_influence >= Ï„_inf` - Feature causalmente determinanti\n",
        "2. **Generalizable Scaffold**: `max_affinity >= Ï„_aff OR mean_consistency >= Ï„_cons` - Feature stabili\n",
        "\n",
        "### Links\n",
        "\n",
        "- **Circuit Tracer**: https://github.com/safety-research/circuit-tracer\n",
        "- **Paper**: https://transformer-circuits.pub/2025/attribution-graphs/\n",
        "- **Neuronpedia**: https://www.neuronpedia.org\n",
        "\n",
        "---\n",
        "\n",
        "**Version**: 2.0 (Influence-First)  \n",
        "**Model**: Gemma-2-2B  \n",
        "**Last Updated**: 2025-10-09\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
