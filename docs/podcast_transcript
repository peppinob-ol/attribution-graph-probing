Hey there. Um, I'm Jack Lindseay. I'm a researcher at Enthropic working on model interpretability. Um, a few months ago, our team put out a couple of of research papers, uh, introducing a new method that we're using to kind of decompose the intermediate steps that are going on in a model's head as it's processing inputs and using them to decide what it's going to say. Um and then in the months since uh a few groups have uh re-implemented this method which is based on this thing we call attribution graphs uh and that includes some open source implementations and there's this really nice um interactive interface hosted on neuronedia.org that we're going to be using uh in this video to explore attribution graphs. Um so yeah, we're uh we're just going to dive into uh some examples of like um trying to interpret models through the lens of these graphs. Um what that's like. Uh there's kind of an art and a science to it. Um and I think uh everyone in uh in on this video has has spent a lot of time thinking about interpretability, but we have kind of uh varying degrees of experience with with attribution graphs specifically. So hopefully that will uh you know produce some good discussions. Um yeah. So why don't we um uh go ahead and introduce ourselves. I'm joined by an all-star crew of of interpretability researchers. Um maybe maybe Neil, do you want to start off uh just introduce yourself? Neil, I run the Deep Mind mechanistic interpretability team and I have never spent more than five minutes looking at a specific attribution. So I'm excited. I am in the presence of masters. Uh, Tom. Hey, I'm Tom. I'm not one of those masters. I'm the chief scientist at Goodfire. We're an interpretability research startup. And I'm uh Emanuel. Uh I work on interpretability at Enthropic as well. And I've spent uh hours of my life uh looking at attribution graphs so long that I've now convinced myself that they're very useful. Clear bias and conflict of interest. That's right. Okay, I'm gonna share my screen, I think. And um yeah. So, so here here is an attribution graph on on neuronedia. Maybe we should just start by like saying what some of these things are. Um I don't know, Emanuel, do you want to give the the like one minute version of what all the components on this page are? Yeah. Yeah. Yeah, I guess they're like really without breaking it. Oh, maybe you can zoom in. Yeah. Oh, yeah. Uh, let's see. Is this is this better? That's great. Yeah, that's great. Um, I guess I guess the main thing to to know is that you know the B that there's two axes at the bottom there's a prompt and so these are all of the all of the words in the prompt which Yeah, I think picked kind of a long one. Maybe I'll go to a to go to a shorter one so it's it's clear but yeah, perfect. Yeah. Um so you know at the bottom you have uh here human you know name one paper written by Andre Karpathy uh and then answer and so over each of these tokens are little circles and little little um squares as well as diamonds. So there are basically three types of things going on. The squares at the bottom are represent the embeddings of the given token. Um the squares at the top represent the prediction of the model and then everything in the middle is either a feature uh or a uh reconstruction error. And so features here uh in this example this is on haik coup. So they were uh extracting extracted using uh CLT a cross layer transcoder uh which is a specific way of replacing the MLP layers in the model. And then we also have Gemma 2B. Uh the features there um are per layer transcoders. The difference is maybe not super important for the purpose of looking at these graphs although um we might we might see more repeated concepts in the pearl or transcoder versions. We'll we'll see. Um and yeah, and then I guess Jack is hovering over the feature visualizations like each feature. Um, maybe if I could just interject in my role as the person who has no idea what's going on. Yes. How about I try repeating that back and you can correct all the things I'm confused about. Perfect. All right. So, we have some sentence here like fact the capital of the state containing Dallas is and we want to understand why does the model say whatever it says next. And at the top in this kind of somewhat difficult to read thing, we've got different outputs like Austin, I think Oklahoma, San, and I guess maybe the model thinks it's San Francisco or something. Yeah. And the point of this graph, as I understand it, is to kind of try to convert all of the complex, mysterious BS happening inside the model into something that we can look at and get an imperfect but hopefully useful sense of what's happening inside. We're doing well so far. That's exactly right. Yeah. And to say all of the all of the nodes that I was discussing are are attempts at intermediary explanations of what's going on, right? And then inside the transformer, we've got say like 26 layers, I think, and they're being there's like a separate activation for each layer on each of the tokens, the things at the bottom. But a layer is not really particularly understandable or useful. So instead we have this technique transcoders which try to replace a layer with a bunch of things that we actually can interpret that have these convenient labels like I know on the right you've got mentions of Texas and the surrounding area. And um I guess later we'll probably get more into all of the different kinds of these nodes. But the thing that's been done to make this graph is to take the model's processing to figure out what to say next. And then we try to throw away as much of the graph as possible apart from the really important bits so we can then look at them and how they connect up and try to get something useful. Is that the right spirit? That's exactly the right spirit. Yeah. And maybe maybe the the next thing to get into, I guess, is is what exactly each of these these these nodes means. Um so yeah we've uh they're they're features they're transcoder features. Um but then we need a we need a way of understanding like what they what each of these these concepts that is represented in inside a model layer means and uh I guess our kind of tool for doing that is this this sort of interface down here. So when I'm clicked on one of these nodes, I'm getting this like visualization of information about a feature. Uh and that's telling me uh like this is kind of like uh the classic uh feature visualization strategy. We're we're looking at data set examples in a big data set that caused this feature to be active. And in this one, we see that like uh and and sorry, the other the other thing we get um sorry, I'm I'm having a bit of trouble scrolling around this. All right, there we go. Um, so yeah, we get two pieces of information about a feature. One is the data set examples that caused it to activate. And we can kind of see in this one that like all of the data set examples are mentioning something related to Texas. Um, and then also we get information about um what like tokens that it causes the model to say. And often this is really helpful to us because it turn the model. This is like the logic lens and it's tell telling us it often it's the case that like the word that the feature is voting for the model to say is very closely related to kind of its meaning. So this one seems like a Texas feature and yeah just to spell out from a more zoomed out perspective what exactly we're doing here. You can kind of think of a feature as just a number in the model where just on every token of the input like every word the number has some value. It's zero almost all the time because there's one which is like what thing geographic place names related to Texas. Very few things are geographic place names related to Texas. But the way transcoders work, we don't tell it what concept to learn. Instead, it just tells us, "Oh, here's a magic number that probably means something useful, maybe." And we have to figure out what it means. And so, the standard thing to do is we just run it across a ton of data and then we just look at the particular words with big values. And that's the list we have here. and then we stare at it and hope there's a pattern or we get an LLM to stare at it because we're lazy and tell the LLM to come up with a pattern for us which is where the geographic place names near Texas came from. Yep. Yeah. All these labels are uh these ones are all LLM generated by looking at these these data set examples and these logic effects. Um, are there any other basic kind of what the heck is going on here in this on the screen questions to go over or should we start getting into a a graph? I mean, I maybe just one thing to say is that you'll see this this last type of node that we didn't discuss which is this ERR MLP is so the fourth node on the input feature list. Uh, these are errors. And so sometimes uh or uh transcoders don't capture everything that's going on in a given layer. Uh and so this this node here just represents basically you know the the residual like essentially explain like essentially it's what we do not know. And so sometimes when we try to explain a given behavior for the model will end up you know uh at one of these error nodes and then this sort of like is a sign that actually we're we're not quite sure what's what's going on here. Yeah. probably talk a bit about the edges in this graph as well, right? We talked a lot about the nodes. Um, so maybe it's time to talk about how they get connected. Like why are there all these lines from this purple circle to all these other circles, diamonds or whatever. Yeah. Um, I I can take a a really quick stab, please jump in, which is just right. So all of these nodes, there's there's a bunch of them in the model. The model is thinking about a bunch of things at all times. And the question that we're trying to answer is like which one of these are important. And the way that we answer this is that we try to connect each of these nodes. And the way that we connect them is that we compute linear attributions back from one node to the prior ones. Uh essentially measuring uh it's it's our attempt at measuring the like effect of you know nudging one of these nodes on all of the downstream nodes or or upstream nodes. And we do this for every single node uh that's active here. Uh and then we have all of these edges uh that that come from just these little linear effects. And then the graph that you're actually seeing has many more edges that we've hidden uh because most of those are not important. So these these are like the most important effects on a feature from from a feature on on other features. So I make a I take two nodes and I like draw an edge between them if changing one feature is going to seriously change another feature. And so I have this big graph and this tells me how each feature relates to each other feature and then in turn how this feature relates to another feature and then it all ends up connecting to one or more of the output output logs. Yeah. And then this is gives me this huge graph which is too big and then I I say like well I'm going to get rid of all of the edges which are below X. Right? This is a graph pruning step. Do do these get darker if they're important? Uh they Yeah, it's a So you shouldn't pay too much attention to the colors over here. I don't think these are meaningful. Um the the edges are being shown in a couple different ways. So right now I've clicked on a feature. And I'm seeing here a list of its strongest input features like the strongest incoming edges. Um and like this is like a geographic place names near Texas feature. Its strongest input is from another Texas feature which is often something we see um kind of two almost redundant seeming features activating each other. Um it also gets input from uh like the straight from the embedding of the Dallas token which makes sense and then a bunch of other you know Texas related stuff and then some error nodes that we can't that are h happen to be active over the is token. Um, so yeah, this is this is like probably the most useful way of viewing of kind of tracing the edges of a graph is to like look at this pane because it's telling you what what are the strongest inputs. Um, then something we'll be able to do is um we'll be able to like uh produce I'm I'm flipped to a different graph now. So these are all different features, but we'll be able to like construct interactively through this interface a kind of subcircuit that's of interest to us. uh where we like highlight the features we think are important. Uh and then in this interface the um the the darkness of the edges indicates the strength. This is manual step that we've done on top of the graph. We've gone like oh I think all these features like they are separate features but we think that they should be kind of grouped together. Exactly. Yeah. So let's Yeah, we can we can maybe go through that exercise uh together because Yeah, I was hoping to do that. It's kind of interesting. Yeah. So, just from a kind of high level like what is the point of all this? The way I'm currently thinking about it is this is kind of a tool that sometimes will be wildly inaccurate or missing the important things that sometimes will let me generate the correct hypothesis about what's going on. And if I really want to make sure I'm right, I can go test it via methods like this or totally different things or kind of not making the claim that it's definitely always totally legit, just that it's useful. Is that how you think about it? I think that's exactly right. basically like our workflow or like you know when I look at one of these things is I'm I'm confronted with uh with this blank graph uh and you know I know the prompt was the model was given this long prompt it produced this output I spend you know an hour or two clicking around in this graph um identifying features of interest uh trying to make sense of the connections between them to sort of tell a like story about like what like the model thought about A and then as a result it thought about B and then as a result it decided to say C. Uh and then we kind of have to go and test that story uh somehow using like intervention experiments which hopefully we'll get to um to to demonstrating. Uh but sometimes you just can't make sense of the graph at all. Uh like you know you you want to know why the model said X and then it turns out the strongest input was from one of these error nodes and then so you know you can't give an answer. um or you it's not error nodes but just the inputs are hard to make sense of. There's a lot of like weird features that are hard to interpret and so you don't know what they mean. So there's lots of ways things can go wrong. Um but sometimes things go right and then you have this kind of like hypothesis about uh you know the general kind of steps the model used to uh to give it answer and you can go and go and try to test that with with a bunch of other methods. Oh what fraction of the time do things go right? That is a great question. I think so. One one thing we've noticed is that well it it depends uh how you definitely kind of get out what you put in with the transcoder training. So like we spend a lot of time and resources into training these cross layer transcoders for claude 3.5 haik coup. So I think we have kind of like a higher hit rate on haiku than the the Gemma 2B transcoders do um just because they're like bigger. Um and I think we've also found maybe transcoders are insufficient quality. Our experience with the transcoders we were training like you go round and round and round each time you try a little bit you try a little harder the transcoder gets a little bit better and you can see it reflected in the graph but like after doing that but the thing the loop here is very long right because you have to train the transcoder and then you look at the graph and you're like don't think this transcoder is good enough. Let's do another one. Yeah, I think so. So, I think for like where we are with these kind of haiku graphs, it's it's a bit hard to put a number on it, but I would um because there's some kinds of prompts that are just like like if the prompt is too long, we just it it's hard to make sense of because the the graph is like, you know, it goes goes way up the screen. Um, but for anything that's kind of like of this length or or lower, um, I'd say we've got kind of a kind of a 25% shot with our current methods of like getting a decent sense of what's going on inside. And I think like there are some improvements we can make that hopefully will get that number up in the future, but that's probably where we are right now. Yeah, one frame that that we've been thinking of uh is sort of like as a you know you you go to the doctor and there's like a battery of tests that you just run baseline and and in many of the cases the tests won't immediately tell you what's going on but in some of the cases they will just give you a smoking gun. Uh and so having that as as a sort like first line of investigation is kind of kind of helpful. I really want to get into clicking on the graph in a minute. Um, but I want to ask one more thing and um, how good is Claude at doing the clicking? Like if we were to drop off the call and Claude were to take over, more importantly, Gemini are doing the click. We also have merchandise. You You can buy Claude at claude.ai. You can buy anyway. I yeah I think um that is something we're very interested in in making happen uh to make our lives easier. Um I would say I would say Claude or other language models um are like pretty good at this this feature labeling step. You know this or they're not probably not quite as good as humans, but they can do a decent job at labeling what features mean. Um, but I think this like kind of take this whole graph and like make sense of it is this sort of like agentic task that probably requires some some effort to like invest in the scaffolding for. Um, so I think that's like a pretty exciting direction, but we're probably not quite there yet. I guess it's like very visual, which is great for us. Yeah. The weak spot for language models. Exactly. Yeah. Cool. Okay. Thanks. Okay, I'll stop I'll stop interrupting and we can start clicking. Yeah, I figure I figure we could just start with this this example to like just show how kind of how big and and complicated these things can be. Uh maybe that's not a good idea, but um this is this is a a prompt from our paper. Um it like presents a medical kind of like a mini case study. Uh a 32-year-old female at 30 weeks gestation presents with severe right upper quadrant pain, mild headache, and nausea. Uh and then it, you know, goes goes on to describe some other symptoms. And then it says, uh, if we can only ask about one other symptom, we we should ask ask whether she's experiencing dot dot dot. Um, and then, uh, so these are the tokens of this prompt like laid out on the x-axis kind of, you know, a little bit messily. Uh, and then these are the top two out um like next token predictions that Claude 3.5 haiku gives. Uh visual uh leads to it says like visual deficits or visual disturbances. Um throat leads to uh protein ura or protein in the urine which is like a medical condition that you can have. Um and both of these are like symptoms of uh a condition called preeacclampsia. um that is like a complication of pregnancy that these symptoms are sort of suggestive of. Um so what we were curious is like whether you can kind of see in Claude 3.5 Hiku's head that it's thinking about the preeacclampsia diagnosis and then like uses that to to like um come up with these follow-up questions about symptoms. Um, so just to spell out why this is an interesting example. Um, we have this model. It's, you know, it's not like a frontier model anymore, but it seems decent at having some medical knowledge. It asks about symptoms in a reasonable way, but maybe it's just memorized things or it's just making a lucky guess and maybe it actually has some understanding of what's going on. This is the kind of question people really care about if they want to use models in these highstakes settings and this is like a tentative sort of concept that we can learn something and that there is something interesting happening here. Yeah. And let's let's see what Yeah. I mean there's a lot of possibilities of what what the model could be doing here. It could be doing it could have memorized this specific case study. Um, it could have like raw kind of uh token level correlate like maybe just every time it sees uh like the word uh you know maybe every time it sees I don't know elevated liver enzymes it like it thinks about protein ura um or maybe it's doing this more kind of like multi-step like reasoning in its head about what the possible diagnosis um so just trying to get yeah there's some sort of latent computation that's being done it's like it's already thinking about preeacclampsia is like yeah I heard of when it's saying anything in the tokens or even you might not like you might not spell it out in the tokens is saying like ah right is actually already thinking the like the sort of latent factor and you deliberately given it this really difficult task where you're not letting it say what condition could it be maybe it's preeacclampsia if it's preeacclampsia what are the symptoms I should ask about you're just just making it do all of it in its Right. Yeah. We've kind of constructed this prompt to like force it to do a lot in its head. Um yeah, so we're starting at the output, right? Let's pick one wishes to circuit trace. Do you generally start at the output? Exactly. We generally start at the output. So we're we're we're saying like the model had a 235% chance of saying pro the token pro. Why why did it do do that? And then we go looking at the top strongest inputs. And boom, right off the bat, we've got uh this one, which uh if you look down at the the like data set examples, seems pretty clearly to be like predicting, you know, it like comes right before uh the model's about to say protein area. Um so it seems like pretty specific to this condition. Um so that's these um because these are haiku, they don't have labels associated with them. That's right. Yeah. Sorry, we didn't in Neuronipedia, we don't have the LLM generated labels in for the haiku graphs. Um, but what we can do, uh, so what we we can commandclick on this feature. It's going to appear down here. Uh, and then I believe I can label it. Yeah. And then I can say protein ura if I can type. And now it's labeled. Um, this is like say throat. Yeah, that's actually it's actually like a say, right? That's I should have said protein. Yeah, because it kind of activates right before you would want to say that word. Yeah, but in the context of that of that disease it seems like or related medical context, right? Seems to be specifically about reduce. We could get into this for ages. So, actually maybe we should just move. Yeah, it's it often for reduce sometimes it's other things. Um but yeah there's a lot of yeah so what we'll see yeah if if I click back on the on the output to the output node and then look at the other inputs there's a lot of other protein proteinura related features so this one is you know activating kind of in context that are talking about excess protein in the urine uh this one is kind of similar to the first one uh and you know actually okay look that looks like that's all we're getting right Now everything else has been pruned out. But yeah, there's like a few different a few different features that are kind of like getting at different protein ura in different contexts or different aspects of it. And so then the thing that we often do um you know uh to make our lives easier is we group these all together and we'll just call this like protein area. uh recognizing that we are losing a lot of information about the specifics of these features mean we're making like we've got an abstraction sort of somewhat leaky abstraction that we've stacked on top of another somewhat leaky abstraction but hopefully not too much is being leaked that's the hope yeah yes and there's something kind of interesting where these are on the token simp I think yeah so on this exactly if we can only ask about one other simp uh and then if you look If you look at these things, it looks like they're kind of boosting the token prop and things like that. But I guess right now that's useless because the next token's obviously symptom, but the model's thinking about them anyway. Yeah, this is something that you see a lot where the model is kind of feels like it's thinking about saying a word like before it actually has to say the word. Um, and it's an interesting question why why they've chosen to work like that. But yeah, we do see this somewhat often. One pattern that is I've not seen enough for it to be like definitive, but I guess it's coming up here is that maybe you choose like very e tokens where the next token is very easy to do this like simp. Well, of course it's going to be simple, right? Like this this this computation is more or less free. Um, so is is this also your experience that like when you see when if the model is like I'm gonna I'm gonna stash some computation somewhere, it usually chooses something easy. Yeah, I think you're you're already an expert, Tom, because that's definitely what we see. I I think we see Yeah. like you know double token words but also also periods and sort of like white space where it's pretty obvious you know how you're going to your next sentence are are places where we often see this sort of like premputation as in you see it on periods in general or only when it's pretty obvious what the next word's going to be I don't know that I won't yeah I I I think yeah I think both of these are I think these are kind of two different things there's like periods and special delimiter characters often have like special features on them and then also like these kinds of tokens where like the next word is like brain numbingly obvious. Uh that also seems to be kind of a hot spot for like maybe like planning ahead type information. Uh but yeah, these are these are pretty like vibesy statements I think not. So I have a older paper on like how represent sentiment where we found that they often use punctuation characters for summarization. I've also found they really like using special tokens like assistant for summarization where what I think is probably going on is when models in like a few sentences want to look back it's nice to have a single clear token to look at to get all the information and so they want to look at like the period or whatever. I think it's actually unusually hard to figure out what comes next after a period because often in the flow of a sentence there's kind of an overdetermined what comes next but after a full stop it's like all over the place. There's actually an interesting thing I think you discussed in your paper where if you're if you if you're trying to jailbreak a model and you make it say, "Ah, I'm going to help you with an evil plan." Often after the first full stop, it will revert back to being a good aligned model. Right? Because that's the first like high entropy. Oh, maybe I should let my default programming take over. Yeah, this that's exactly right. Yeah. Um and then maybe Yeah, I'm wondering spell out the simpum thing. Um, for people who have not said g transformers, a weird thing about them is that they are forced to spend the same amount of computation on every token whether easy or hard. And so they have all of this spare thinking power on simp where obviously is going to come next. And so we were hypothesizing that it's using that for like things that will be useful later on because it's kind of wasted otherwise. Yeah. But I think that's speculative. Yeah. Can we carry on tracing? I'm excited to Yeah. get to the next step of this. Uh so let's click on you know one of these protein area features. Often you'll find that things like this where like protein area feature one just its strongest input is from another protein area feature. So you kind of just like click on that and you know skip that step because that's that's it's not interesting what's going on. Um and then okay we like we can we can keep going back and I think um so like this this is an interesting feature uh as is this um like like let's dig into what these are representing and if you kind of what you just saw say again I have no idea what made you say yeah great great let's so I was here right I was looking at this I was looking at this input which was just it's strongest. So first first of all first thing that mention is that this top strongest input is an error node. So we're missing some stuff here for sure. Uh well let's just like you know bravely venture forth and see what we can find despite all the errors. Um our next strongest input is is this feature. It's like activating on seemingly like a bunch of different kinds of uh symptoms. uh cerebrovascular hemorrhage um returns list is it list delimiters like max activating is list delimiters in medical context well maternal a lot you're halfway there yeah so there's there's a lot of maternal and then if you kind of like well if you're if you're a doctor which I'm I'm not uh but maybe you look at this and and you realize that these are all actually common symptoms of preeacclampsia uh And then you because you start to notice that in a lot of these data set examples it's coming shortly after mentioning preeacclampsia. Um maybe yeah you know you're this is cut off but this just said preeacclampsia is known to cause immediate maternal death. Um this talking about preeacclampsia affects such and such a percent of all pregnancies. This one also says the same thing. Um, and once you kind of like, yeah, there's another preeclampsia at the beginning being cut off. So once you kind of catch on to the pattern here that all these contexts have something to do with preeacclampsia, then you know once you're kind of a a believer, we can like click on Oh, I forget how do you remember Emanuel how to click on the moream the examples scroll up uh in your view and then Oh, I thought that you could pop. So we probably don't have these for the haiku. Oh, right. Yeah, you'd have to have model access and I guess Neuropedia does not. Yeah, but okay. If Yeah, you can kind of like start to believe that, you know, all of these contexts are are in are mentioning. What I was going to ask is like now you've got this hypothesis which you sort of is a little bit in jeopardy, right? How do you how do you really put it at risk? Yeah. So you one thing you do is you have access to these full data set examples and then you notice that in all the ones that don't where you can't see the word preeclampsia here you see it kind of earlier on in the context and many of them. Um the other thing you do is you look up all these symptoms on on Google and you discover that there are like kind of canonical symptoms of this particular condition. Um and uh like what we'll find is so like here is an kind of the other thing we do is like the our interpretations of features kind of like they sort of help each other out. So we'll notice like if we go from this feature and try to click on its top input we get to we get to this one and then if we look at this one what are we finding? We're finding more mention of preeclampsia. we're finding more descriptions of the same kind of cluster of symptoms. Um, and often in the context of like having discussed preeacclampsia recently in the context. Um, and so like it seems like the in when these features are active, it's like when here's the only true cure for preeclampsia is it feels like it's you know in context where this this the concept of this condition is is especially relevant whether it's because it's been explicitly mentioned or because of this like cluster of symptoms having me been mentioned that's evocative of it. Uh but but to your point to your question, this is like where this kind of interpretation process is like as much an art as a science is that you kind of you know it's a sort of it is sort of a squishy claim to like just like stare at this screen and then do the thing I'm about to do uh which is I'm going to group these two features together and label them preeacclampsia. There's like a leap of faith. Yeah, my instinct would have been to kind of just start typing text into the model and do things like remove the word preeacclampsia and see if the thing stops activating or change them for some other similar symptoms and things like that. Is that something you'd do that? Yeah. So uh the second thing is something uh we've tried at least and that that's exact exactly the right instinct is like give a give another case study with like a bunch of similar words but that doesn't suggest preeclampsia and then see if the preeclampsia feature isn't active and that that gives more more credence to this hypothesis. Um, ablating single tokens from the examples is a bit tricky just because like often there's so much context that even if you ablate like the preeacclampsia token, it kind of isn't enough to make the model stop thinking about it. Gotcha. Yeah. What I was imagining was I just start writing out a kind of short snippet about preeacclampsia symptoms and then I see what I can do to make it stop lighting up. Yeah. So there, so yeah, I I'm glad I hovered on this feature because Yeah. So I was on I think what was I on? I was on like this feature and then this was it top input. It was another one of these like maybe it's about preeacclampsia, but I'm not quite sure. That second strongest input is is uh this one which is like oh it literally just activates on the word preeacclampsia or preeacclamptic. Um it also activates on some other kind of you know other some other contexts that are relevant. Um, and so this is a good example of like the edges between features are helping us out in interpreting what they mean. Um, sorry, is this on the same? This is not on the same. This is an input feature. Yeah. To sorry, not input as in like input embedding. This is a feature which is like connected to the symptoms of preeacclampsia feature. Right. Exactly. Yeah. So maybe like symptoms like Yeah. There was like there I was wondering we got the symptoms thing. Is there anything that like is a dumber version of it, right? And I care a lot about this because you know we're trying to like find new scientific knowledge in give a quick good fire plug, right? We're trying to find new scientific knowledge in scientific foundation models and there we don't have the advantage of there just being the word preeacclampsia. Um, so we need to like, so here I'm like, great, this is an example of like it's gone from the the spec like the specific case to consequences of it. And seeing that structure makes me feel really infused. And one kind of crazy thing here is that this feature is in layer um, wait, how do I see the layer was there? We don't have probably cannot. Well, you can look at the min output features and the max input features layer and Right. Yeah. Yeah. We Sorry. We don't have the layers here, right? But we do have Don't give the game away. Yeah, we do have the we what we do have is the fact that it like is fairly early in the kind of computational graph. Um so like the model kind of got to thinking about uh preeacclampsia like pretty quickly. Um in fact if we look at uh this features inputs um which in principle oh wait why is this oh you have to click save on your on your label pre-clampsia oh wait there we go yeah yeah okay now the hovering is working again um yeah so I'm on pre the preeacclampsia feature I look at its input and now like what we'll see is that it's getting its input from like really uh like early features um that are like responsive to these kind of over that are responsive to the symptoms being described in the like token inputs. So this one is like a feature that's all about words relating to pregnancy trimesters. Uh is it specifically the third third trimester? I'm not sure. I think it's the seventh month onwards I think. Oh no, sometimes fifth month. Yeah, it does seem to be at least like latter sort of like latter stages of pregnancy. Um it's a little fuzzy. Um then if I go back I'm going back to the preeacclampsia feature. I'm looking at uh this input uh which let's see this seems to be another oh this this actually. Yeah. Yeah. So, here's an example of a feature that like it's a little bit it's a little bit haphazard. What what it's representing, it's like clearly referring to kind of like issues, complications that can arise during pregnancy. It it it fires for pre-clampsia. It fires for um toxmia. I'm not maybe pronouncing that incorrectly. Um and I guess kind of just those two. So yeah, sometimes we have these like wonky features that seem to care about like, you know, a grab back a couple things. Here's lighting up on quadrants and and yeah, it's on the quadrant token. Uh, unfortunately, it looks like we don't have I think we pruned out its inputs, so we can't see the inputs to this one. Um, I'm guessing it's got it's getting its inputs from like back at these um back at these uh token positions. Oh, so you can actually end up with like a dead end in the graph. Yeah, that's just because we did a lot of prune. We did maybe more more pruning than we should have. Um uh ah here's a good one. Here's back up. I'm back at the pre-clampsia feature. I'm looking at some more of its inputs. This one is clearly coming from this. This is an input that's like about uh blood pressure specifically. It seems to be hypertension. Uh, it's like activating on the token position that gives the blood pressure. Um, so I feel like I've got a little bit lost in the weeds. So, can you zoom out a bit and remind me what what exactly are we doing here? Yeah, we have these black circles. You're clicking on random things and saying things that sound intelligent. What's going on? Yeah. So maybe I'll skip to the like imagine we did this for a while uh and kept clicking on things and kept grouping together the related ones. I'm just going to find the the link to the sort of like worked example here. Um so go check out Lindsay at al if they want the worked example of that one. Uh that is true. Uh but you can also check out this link uh where so you know so at some point like you know one of us uh spent a while looking at this graph and then we did a lot of this this sort of clicking and tracing and puzzling process that we've just been doing and then like arrived at this picture. Um and this probably took you know like two hours for a kind of experienced circuit tracer to to get to. And it's like we've got this picture where like okay there's a bunch of features that kind of like represent like the sort of like low-level inputs relating to the symptoms that are described in the in the input. Um there's like high blood pressure. There's like liver related features um because it talks about having elevated liver enzymes. There's headache features. There's right upper quadrant pain features or just like right upper quadrant related features. There's a lot of features related to pregnancy. And then these all sort of like or not quite all of them but many of them sort of like funnel in to like provide input to this cluster of features that are related to to preeacclampsia. And these groups are very as like Tom said this is a leaky abstraction and that you know this cluster of preeacclampsia features is like got all these you know this preeclampsia feature is kind of different than the ones we were just looking at. Um so there's like a whole bunch of different types of preeclampsia features. Um, but they're all kind of like teaming up to, you know, rep, it seems like they're all teaming up to represent this like candidate diagnosis and then that kind of like so there's this like fanin structure and then there's this fan out structure where the preeacclampsia features activate um a variety of specific like other symptoms related to the condition. And then for whatever reason the model these ones kind of went out visual deficits and protein ura went out and those are like the two answers that the model gives. Um and this is like so like big picture is like we got we now have this kind of like highle view on what's going on in this prompt. It's like bunch of like token level features representing symptoms. They kind of like uh all join forces to to like upweight the features representing the concept of preeclampsia. And then there's this like fan out where then the model starts like thinking about all of the other possible symptoms associated with preeclampsia and then it like chooses to say one one or two of them. But there's a lot of like unanswered questions here like why the heck why does the model pick these two and not one of these ones? It's like uh a little hard to say. Um how does the model know not to say one of the like symptoms that was already in the input is another interesting question. there's probably some like inhibitory thing going on where it like um these are like inhibiting alternative responses. So this is something that this is kind of a weakness of these attribution graphs is um they're telling you why the model said what it said but they're not telling you why the model didn't say what it didn't say um or at least not the way we're presenting them here. So, just to spell that out a bit, um there's actually like millions of features that could be in this graph, almost all of which are off and we've like printed some extra ones. Um, and this makes sense because you want to focus on the interesting bits. But if we want to ask like why didn't the model ask about the hemorrhage that is asking why is the output hemorrhage feature off and well node or whatever and some things are going to be positively going in like the preeacclampsia feature some are going to be negative and it doesn't fire we just don't track it in the graph at all and maybe that was actually quite interesting and maybe I don't know we can do some kind of just manual intervention where we just shove it in the graph. I don't know if that's something you played with, right? Yeah. And there are cases where like um we have uh like there there was this one example from our from our paper um where there's like a if you ask a question about a person who like doesn't who's not famous. Um there's these like features uh that seem to like respond to like unknown names that activate. And then there's this question of like, well, why aren't these features active when you ask about someone who is famous? Uh, and then like what we did is we kind of went looking in a in a different prompt. When you ask about a famous person like Michael Jordan, we went looking for like inhibitory negative connections onto those unknown name features that we had identified from the first prompt. U, but we kind of only knew to do that because we had found them in that other prompt. So often like if you have kind of a hypothesis of like I think this feature is like inhibited in this prompt uh but but it's like normally active but something in this prompt is like turning it off then you can go and like check for that explicitly. But if you kind of don't know what you're looking for in advance it can be hard to find those cases. So a lot of the scientific work is happening outside of the context of the attribution graph like you have to know to come like you you can't discover the unknown names features easily by clicking around in in sort of randomly selected attribution graphs. You have to take that context over too. We we did find them actually but with an attribution graph you just need a pair of Oh yes sorry. Yeah I I phrased it badly. Yes. Right. Yeah. Yeah. You need to like take the take hypothesis from some other context. It's not like a sort of machine for scientific discovery. You you have to someone has to like drive the machine or an LL. Yeah. Someone someone I think that's exactly Yeah. So the the analogy we're like we kind of use to death uh is this idea of a microscope like um it's like making one of these attribution graphs is like you know you're looking through a microscope at like a particular prompt and you spend some time analyzing it but that's usually not the end of the story. us like to to learn something about the model, you typically have to make several of these. Um, you know, you got to do, you know, often at least two, but often many more, and kind of get a sense of like, do the graphs for this family of prompts all look similar? Like, are is there common structure for prompts of form X? Uh, where are the differences, etc. And there was a sort of funny funny moment last year where it seemed like, oh, interpretability is when you train the sparse autoenccoder. And I think you no one anyone actually does it like professionally like as like a as a as a main activity. It's like no interpretability is what happens after you train the sparse auto encoder. One of my many vendettas is that if we used a different word for a piece of the sparse autoenccoder people will be less confused. I'm looking forward to the my many vendettas blog post by deal. My many vendettas one of seven. one day. So, I'm noticing that we have uh just a few minutes left in our like aotted time slot here. Uh I do wonder if it's actually worth having a part two of this video where we like do I don't know one more example and and play with steering. Mhm. Um, if people are interested, yeah, if you get one of us, if one of us drove, then like one of us drove from a kind of blank slate, that might be quite good for getting people who are not experienced tracers of circuits into the circuit tracing game because as it is like you sort of knew the answer and I will I will not know the answer. Yeah, I think that would be great. So maybe I don't know, Emanuel, are you down? Yeah, let's let's let's do a second session. One of you can drive. I can advise or help. Awesome. And yeah, I guess as a concluding note before I turn off the recording, uh people can just go and play with this themselves on neuronedia.org. It is glorious. And we will have links in the description. You can just trace things. Exactly. And yeah, thanks much. This is super interest. Awesome.
pretty good. Oh, okay. Well, that's too bad. We're missing out. But we are we are live. We are live. Okay. Uh um yeah. Uh I guess welcome back everyone. Uh we are back for a second session of circuit tracing. And we've all changed our outfits and are are ready to dive in to some more attribution graphs. Um, you guys changed your outfits. Got an exciting new t-shirt. Oh, wait. Hang on. Let me stop sharing so I can see your t-shirt, Neil. It's the alignment faking meme. Oh, nice. Oh, yes. I discovered you can just make your own t-shirts. Neil, where can I where can I buy this merchandise? interp.shop. Are you kidding? No. [Music] All proceeds go to suffering shrimp. They are sold for zero profit as a service to the interpretability community. Wow. Oh, there's so many options. Goodness me. Batch top K every day. Um, if you judge one of them, assume my scholars designed this. Okay. Awesome. Nice. Did you know you just make your own merch store in like five minutes for free? It's wild. Incredible. I had no idea the internet gave me this much agency. This is This is wonderful. Um, okay. So now we are in fact a real content creator operation. Exactly. No, I don't make money from them. We've had our real content creator if we're making money from our merch. Well, I guess the interpretability community is making money from them. We're back in the room. Virtual. Someone's got to share their screen. Okay, I'm going to share my screen. Um, credit to Samox actually making the glorious meme. I'm going to share Neuronedia's circuit tracer, so you should be able to see. Can you see it? Yep. Amazing. Great. So, let us get started. Oh, this is not a walkthrough. Okay. Well, I guess we'll do the walk through ourselves. Um, so let's do it with do you prefer Claude Haiku because you guys are more familiar with the features. So I think one thing we could do if if you think it'll be fun is to maybe like make a new graph that we think we have a reasonable chance of explaining maybe one of the things that I was um thinking about slash playing with was the like uh two hop factual like you know fact the capital of the state containing say an American city is that capital. I think Gemma 2 can do a bunch of these and we could we could just look at it and that way it's unlabeled and so you're getting the true experience of of going through the marvelous. So if anyone wants to do this at home they can go to neurompedia.org and click on this circuit tracer. That's right. And then you will land on this page and then I guess we press new graph. Mhm. Okay. That's a sign of a good UI. User friendly. That's right. What's the capital of the state containing? Charlotte is a good one if you want or any state. Yes. Okay. Estimated generation time 29 seconds. All right. I'm going to start my stopwatch. It actually often exceeds expectations. I'm going to jinx it, but I I bet it'll be less than 29 seconds. Let's go. It down below. Is this counter accurate? Better under promise and over. That was like less than 10 seconds. We didn't even have a chance to cut away to our sponsors. What are you talking about? We already did the interp.shop section. Okay, there's quite a lot of things here, right? Let me see if I can just hide the little popover that GBC gives us. Okay, so I'm driving. I actually do not know what the capital of the state containing Charlotte is. I'm guessing it's not Charlotte. I'm guessing that after looking at this, it probably is Raleigh. Yes, believe that's right. Yeah, that's the highest probability answer. So what I'm seeing here is I'm looking at all the output logits. And so if I remember from last time, the thing to do is click on one of these and work start working backwards. Y many lines. Okay. And so now I'm seeing lots of quite long distance lines. Um I could so I could start looking here uh at the input features. And so these input features are that's interesting um are things which are one hop in the graph away. Yeah. The rally logic. So this top one is the embedding of Charlotte which is interesting. Um oh can I go back? Yeah going to go I would just click on it again if you want to go back. Um, and if you want to if you want to stash a feature for later but don't want to select it, you can commandclick. Oh, nice. Okay. Yeah. Generally, the workflow I find useful is starting from the end like you're doing and then command clicking on all the features that seem even remotely interesting and then slowly sort of building up this interesting mental map of what's going on. Okay. So, there you go. Now, it appears on the bottom left. I see. just spell out to viewers what it means for embedding Charlotte to be there. So that's the previous token. So what's going on is there are some attention heads that have looked at that token. We're lazy. So we assume the attention patterns are god-given and constant and can never be understood as I assume will be the state of the field forever. And so this means that something about the Charlotte token is just boosting the rally token. Presumably this is because they're in the same state and so the model has just got some kind of association between them when mediated by a particular attention head. Yeah. All cities in that state. I don't know. Probably boosts random other stuff because Charlotte's also a name. And one one note here is like I guess yeah often it's a good idea to you know think before you start clicking around kind of what what might you expect to see in this graph. And so here we're saying the capital of the state containing Charlotte is Charlotte is a city in in North Carolina. The capital of North Carolina is Raleigh. Um so we might expect the model to kind of go through that you know two-step process in its head kind of the way that I just did. Uh and in fact that's like what we saw you know in our paper uh was you could kind of see this intermediate step where it thinks about the state. Um but another thing we saw in our paper was that in addition to uh you know kind of doing explicitly representing that intermediate step the model also seems to just do this kind of like blind association like as Neil said where in this case it's just like Charlotte and Raleigh I guess are kind of similar enough vibes that the model just sort of you know associates them with one another presumably the fact Oh, sorry. Neil, yes. So, maybe to push back slightly, I think we don't actually know because this is clearly an attention based thing. There is a head that chose to look at the Charlotte's token and that maps to the relay token. And one of the limitations of this method is it could be a head that always looks at the previous token, in which case it's probably a blind association. Or it could be a head which because it says capital earlier on has decided to look at places and look at the capital subspace or something. Yeah, that is Raleigh and like I would actually consider that a pretty sophisticated thing. We just don't really know. Yeah, if a lot of if a lot of what's going on here is mediated by a series of attention heads, then it's sort of happening under the radar for us. Yeah, that's yeah, just to say that another way, I guess an an attention head can kind these edges in the graph that are mediated by attention have the potential to kind of extract facets of the in input feature like specific facets that are headd dependent. And something that we don't show in attribution graphs at least as of now. Although this is like something we definitely want to you know to to improve on is like we're not showing you you know which heads are carrying this edge and like what kinds of things those heads do. Uh, and so as a result, like we don't know whether this is the model doing something dumb, which is kind of blind association between Charlotte and Raleigh because they're both in North Carolina, or doing something smarter, which is, as Neil said, kind of, you know, extracting the like state capital subspace part of the Charlotte vector, right? Because these edges, if I remember correctly, are like a sum over all of the all of the paths, right? Charlotte and Raleigh. And so we c we could not we could just decompose that sum into its individual terms and show those instead. And now I guess the graph would look kind of more spaghetti like but we would get more information out of it. Yeah. What one way I often find useful to think about this kind of thing is there are two kinds of computation in a transformer. the attention heads figuring out where to look or kind of what information to move around and then the MLPS doing all the processing and thinking and as I understand it attribution graphs are kind of only for MLPS and for a given task it could be that all the work is done by attention something like induction or it could be that basically all the work is being done by MLPS and the attention is really boring like looking at the previous token or the noun or something or it could be kind in the middle and one of the things we're looking out for is just where do things fall. Yeah. The the only thing I would uh I would correct you on there is uh attri the attribution graph you see on your screen right now is just for MLPS. Uh in in principle there is no reason why the kind of core ideas behind attribution graphs couldn't be extended to to kind of you know show you information about about attentional computation. And so this is sort of like obvious next step for this this whole thing. I look forward to the forthcoming anthropic paper. Attention is part of what you need. Yeah. Like this line here from Charlotte to Raleigh would in fact be like a bunch of separate lines, right? Okay. Let's do some more clicking. Yes. Um so I'm going to the embedding to Charlotte is as we said like a little bit mysterious. Um, if we were going outside of this interface, we could do some more, but within the confines of this interface, the best thing to do is click on the word cities. Okay, hang on a minute. Let me just move the GBC thing away. Right. So now if I look on the cities feature here which is this little this little pinky purple magenta circle um this has one edge directly to rally and it also has a bunch of other edges which are like also activating other features like locations. This is do we know where these names come from? Yeah. So, so there's a couple couple comments to make here. One is, um, if you lally look at the cities feature down the the the viz for it down in the bottom right, um, it seems possible to me that this is something more specific. So, so these labels are coming from an LLM auto interpretability thing. Um, just looking at the top uh, logits there, it seems plausible that this is a capital cities feature. Um, I'm not confident enough in my knowledge of world capitals to say that confidently, but it seems Yeah. Washington, London, Moscow, Berlin, Damascus. This one also responds to the literal word capital. Yeah. So, I would probably reabel like if you go click on edit label, you can you can uh save us some confusion and Yeah. And it also Yeah. And I would even more specifically suggest that this is like say a capital city um given that it's the top logits are names of capital cities right and it's quite near the end so you'd expect this is interesting where like here um it's activating on between and then it's not actually a capital but it would have been it would have been quite reasonable to say between Berlin. So here it's like oh maybe maybe the thing for me to do here is to say Berlin and oh well actually the text didn't didn't go that way. Okay, so we're going to call this one capital cities. And I can see now now when I look at the graph, I can see that it's propagated like this feature also sort of forward propagations to a bunch of other a bunch of other features. But these presumably are not actually affecting the rally the rally logic otherwise they would have been higher up in this list. Yeah. Or maybe they are, but they're affecting it. They're affecting it less. Yeah. Yeah. Yeah. Is is there an easy way to see um is it possible that the um thing we're just looking at the like capital cities one is affecting rally a bunch via intermediate nodes or do we just know that the main thing is the direct connection where it just goes straight to the end. So we we know that the direct connection is is quite well correlated with the sort of like aggregate influence um in general. There's yeah there's like um a plot that's in that in that uh paper we can we can attach it in the description but but sometimes it's hot it is mostly like the effect through indirect notes that matters. Um I actually think we don't show it in this in this interface but the like actual library can compute it. it just is not displayed. That would be like cities activated and then it bounced around through places and locations to rally. Now in practice in this in this example it doesn't happen but right okay let's do some more clicking. So we've got our friend say a capital here. Yes. Um we also have North Carolina locations to see if we agree with North Carolina locations. I'm afraid I'm going to be the least. Oh, North Car I'm not very very American America wise, but even I can say North Carolina speaking. I feel like that's reasonable. Um, yeah, it's interesting that we have this prompt about American geography with three Europeans on the call. Only Jack can save us. Yeah, Samson's a county in North Carolina according to my rapid Googling skills. Got to represent my employer here. Oh, coach CB McGrath. There we go. Ah, team McGrath. Sick of see. Yeah, I've been Yeah, I've been got by the top activations. Gemma Tubi is scheming at me. Okay. Who knew it was so smart? Shut the whole thing down. Right. So, what where should I go from here? I'm a little So now I'm starting to get a little bit lost um in the sense of like I can I can imagine I just keep clicking and I have built up this sort of mosaic in my head. But what what would you recommend I do now to actually start making sense of things? You to start like building up some sort of some sort of hypothesis in my mind. One meta goal that I that I find useful is to sort of have a path that doesn't have to be all-encompassing that goes from all the way from the loads which you started out to back to the embeddings. So sort of like I guess what we'd expect here is that there's some path that goes from you know Charlotte and from you know capital and perhaps from state. uh those tokens should probably all matter in in some in one way or another and there's going to be some intermediate computation which maybe includes North Carolina um or probably should include North Carolina and and then so like that should give us maybe like an initial hypothesis or something like what are the paths going from embeddings to loits right that's interesting yes because I would imagine that like capital of the state somehow must get propagated forward onto the Charlotte token Right. And like but or well yeah so some maybe a different way to think about it would be like you have a you found a North Carolina related feature. Yeah. Where did that come from? It looks like it's got an input from the Charlotte embedding. Um but maybe it's got some other inputs that you know why why Charlotte has to do with many other things besides the state of North Carolina. So why did it activate the North Carolina feature? Maybe because of that you know maybe because of that state feature that comes next in the list. This is interesting. Yeah. So it's also gotten embedding from the capital. So now we're making progress in the goal of tying things back to logits. So on our surprising North Carolina locations that's getting that's getting pushed up by both Charlotte and Capital, right? Like why would capital push it up if it's truly just the state of North Carolina? It should be independent. We I think we saw this in in Haiku as well and that state was surprisingly like less of a big deal than capital in the prompt. I guess if I just gave you the sentence, the capital Charlotte is, right? I probably told you there were some missing words, you'd probably get you would have a much easier time than the capital state is. Yeah. If I if I recall, we we ran that exact experiment on hyon. It does, you know, it predicts the same thing if you emit that word. Interesting. I'm sure JMA might as well. Yeah. I guess to maybe throw in uh my more boring hypothesis. Um I have the hypothesis that actually the model is not doing complex multihop factual recall. Instead, it's associated Charlotte with North Carolina and states and capitals are just so important that the kind of structure of the space can take you from a capital to a state easily. Like you just add an is capital direction and you go from North Carolina direction to the rally direction rather than it having some database or MLP based processing that needs to happen. That's I have the same I I think the same as you, but I I guess I'm not sure that that's so just different from factual recall. I guess I think of it as like capitals are pretty are pretty related to states. Uh like whether what's the kind of thing that has a a capital a state is the kind of thing that has a capital. So the model probably, you know, interprets them, you know, embeds them in a somewhat related way. at least you know you're talking about a similar topic if you see the word state and you see versus if you see the word capital and so the same kind of vector direction there's some vector direction that like when combined with Charlotte like gives you North Carolina and maybe state is like the ideal direction but capital is kind of close enough okay so maybe to defend my patentry for why this matters um so models know a ton of facts And this seems to be often stored in the early MLP layers as some kind of database where you look stuff up which is a very different mechanism from you add the is capital direction and you go from the North Carolina thing to the rally thing and in particular database lookups should appear in the transcoders while adding directions doesn't necessarily because it's linear you don't need uh you don't need MLPS to do the processing but maybe it happens anyway and so to me the question is are there two database lookups in different layers or is it fuzzier and weirder right and this does rule out the hypothesis of Charlotte got like got disambiguated into Charlotte the city which then did something or other with an early MLP to to recall facts Like it might be you see you you disamiguate Charlotte to be the city then you recall a bunch of facts from the MLP early on and then you like this is sort of enriched the residual stream and you selectively pull those facts. As far as I can tell this graph is not compatible with that hypothesis because this edge goes directly from the embedding. Yeah the hypothesis a bit. I think it's kind of interesting. So Charlotte is weird because it's a city and it's also common female name. the model should act very differently depending on which of the two this means. So you might expect that Charlotte is actually an incredibly boring embedding and early layers figure out from context whether it should have this the kind of city embedding or the like common female name embedding. If that was true then the Charlotte embedding should have nothing to do with North Carolina. It would need to go via early layers. This it's worth noting the same attention thing that we were talking about earlier could here. There might be an attention head that cares about kind of cities and states and places that's extracting the like city state place aspects of the Charlotte embedding. Yeah. Yes. like the Charlotte embedding just because there's a different version of this where the Charlotte embedding just carries the information for both Charlotte the name and Charlotte the place and so if you were to apply the same embedding so if you were to apply this same attention like the attention is just taking the is capital of right um that's not quite right but like yeah it's like is like yeah it has to do with states I guess Yeah. Or maybe there's like two hypotheses here. So the first is that it simultaneously knows everything about Charlotte the city and Charlotte the name and this is all present and it just needs to kind of filter and we've got the capital extraction or the state extraction head that only looks at the part of the residual stream that has the city information. Or maybe the tension head itself is where the fact is stored. Like the shan embedding is really boring and there is a head that stores loads of associations between boring directions and specific things if they're in the right context. That seems less that seems less likely to me, but I couldn't actually defend that intuition. Um I less likely, but it's a hypothesis that is worth considering. Yeah, I think one thing we can see from the graph too, uh, just on your screen right now is if you look at the fifth feature in the left panel, the one that says references to legal proceedings and North Carol, that is an MLP feature, you know, or this is like representative ser like MLP computation at layer 6 over the Charlotte token, which is also consistent. I mean, who knows there might be attention can be involved before that, right? But that's also consistent with the MLP doing something like the enrichments that we were discussing, right? So this this is interesting. Hang on a second. So let's go back to this North Carolina locations. Oh, no, that's wrong. The feature I'd love is the ability to sort by layer for like the input or output column. So you could then see what's the earliest thing. Maybe even with a slider so you can kind of filter out the tiny ones that don't matter. Okay. So, hang on. I've got North Carolina locations and you're saying here, yeah, right. I see. I mean, it would be really nice while we're while we're engaging in wish lists. I I love I love intervention experiments. It'd be very nice to be able to like up ablate this feature and see how the computation reorganizes itself. Do I have good news for you? Hey, um you'll notice Okay. So, so you won't see how the computation reorganizes itself, but you'll notice in that bottom left panel, uh Neuronipedia has implemented a steer uh button that green. Incredible. This is a bottom. would do a live demo of right now. Okay. If any neuronedia guys are watching, uh, I do think we might have more luck with this once we've like kind of found all of the like features that we think. Yeah. Okay, cool. Um, I will sit on my metaphorical hands with the steer the steer feature. I think I think that feature that Emanuel's pointing out, the references to legal proceedings is um, plausibly. So yeah, I guess one one meta note is like often there's always like multiple many things going on in the graph. And so if the when you have a question like is the model doing a sort of like proper lookup of like state, Charlotte goes to North Carolina or is it doing some like jumbled messy thing where it's just kind of like Charlotte happens to be correlated with North Carolina. Um the answer is often it's doing both of these things. Yeah, the answer is yes. Oh, so in so this feature actually seems plausibly like it's doing something closer to the lookup kind of computation and that this is a transcoder feature on the Charlotte token. Its top inputs are Charlotte and then like locations that have to do with states and cities. But maybe it's worth actually hovering on that feature on the input side. Sorry, was that one on the side? Second one on the second topic. So like this. So you're on the the the feature you're on is is relating to North Carolina. Um and it's top inputs. I How do I How do I make this? So I don't want to select the feature. I just want to keep I want it to behave as though it still had focus. Oh, I think you just want to hover your mouse over the like Yes. But now I want to come down and scroll scroll the top activations. Oh, select it to you might have to select it. Okay, never mind. Cool. So, this is although I think I think you clicked but maybe maybe not on the future we wanted. Hold on. I'm back here. You're back there. You were looking at outputs, but I think we want to be looking at the or I was suggesting this one which I claim actually is also a North Carolina feature because I'm cheating and looking at it on my screen too. Does look like North Carolina. So then So then we want to know where that North Carolina feature came from. Yes. So, I put the I put this one in the graph as well. Yeah. And we could label it just, you know, North North Carolina or something. Hold on. This guy. Yeah. I'm going to edit. This guy also North Carolina. The also is an integral part of the movie. Um, you can tell because it happens earlier in the Okay, so got our North Carolina feature. Now, this is this this is good stuff. This is I think this is exactly what you were you were looking for, Neil. Or maybe maybe I shouldn't say that, but but uh bold claims. top. This is like state of North Carolina feature and its top inputs are aside from an error node which we should always respect the error nodes Charlotte and state and Charlotte is by far the majority of the input here. It's very very dominant in the list. Yeah, that is good point. Although I do wonder if we like you know integrate all of the smaller contributions. Yeah. Yeah, I mean these are starting to look pretty noisy. Yeah, yeah. Yeah, you lived in a man. I'm just I'm looking at this. I'm having so many feature request ideas. Like I'd love if we could group the input features by the token they're from or like search all the auto interp labels for the word North Carolina. [Music] Okay, let's keep let's keep us some momentum going. So, I've got my I've got my also North Carolina feature here, which has definitely been activated a bunch by Charlotte. Now I I think a quick thing you could do now just uh on the output side there are north a few North Carolina features that aren't in our graph yet that we should command click if only because then when we steer we probably will want to steer on all of them. Yes. Yeah. Does do you want to consider this to be a North Carolina question? potentially more specific. South Carolina SC these many places close to nice. What is up with some of these labels being really short and some being extremely complicated? If only we could interpret why the LLM sometimes gives short labels. Oh, this is just the word capital. Never mind. Um, it's probably an example of Okay, so hang on. I'm I'm I'm making sure that I'm adding these North Carolina locations. These are And now Should I group them? Would you recommend grouping them? Yeah, you you can do that by holding G and then clicking on, you know, the the features you want to group in turn and then releasing G. Hello, this one is also and you can add it the same way. Oh yeah. And then you can also call that thing whatever. Yeah. North Carolina. North Carolina. Look at that. This is a a bit of a digression, but just that that capital in a financial senses feature I think is a good example of what we were talking about just a bit earlier of like that models will sometimes kind of just have all of the interpretations of the token uh before they know which one is right. Um so you know the word capital can mean all sorts of things. Oh, and look on the Yeah, there's also this capital and financial context. There's also major cities, though I feel like it's weirder than that because it's not just that the thing lit up but wasn't used. It's useful in graph which to me suggests that actually it's not perfectly monossemantic that it actually slightly means geographical capital as well and I guess we're using single layer transcoders so maybe I don't know does that create more of an incentive for this kind of things playing double duty because the output of the layer contains a bit for the geographic capital concept that it's useful to shut in. Yeah, it seems maybe I guess one hypothesis is just that the model like the you know the direction the underlying directions in the model for capital in financial sense and capital in a geographic sense are somewhat correlated. And so any any you know outgoing edges from one of them also leak a little bit into the other like the model kind of and maybe this is actually useful for the like you could imagine like so maybe maybe the model sometimes misinterprets in which sense a token is being used. Uh and then so it's helpful to have like a little some of these kind of you know other edges that are kind of leaked in from the other meaning just in case you kind of got the interpretation wrong. Uh but I'm I'm just like even relatively late there's no way there's no way at this token position that the sense of capital can be disambiguated. Um but it looks that if we looked at the output features we would think that this was we' think this was like a capitals purely in the sense of countries because those are the only those are the only things which are downstream to push up. Yeah. I mean, I would also say that if I just saw the capital at the start of a sentence, I'd put much higher probability that it's geographic than financial. Just like syntactically because financial capital often have different sentence structures, I think, right? Yeah, I agreed. I mean, not necessarily like the capital markets of France blah blah blah would be a legit sentence maybe, but Yeah, maybe I don't know. I even know if what I just said is true. Let's bring there are prompts you can construct uh for sent like garden path sentences which can serve like uh or sentences which could be interpreted in one of two ways one of them which is a garden path sentence and the model has features for both and I think on Gemma 2 we have a steering experiment uh in like little demos where you can you can push on one of the interpretation versus the other and it'll continue you know one way or the other so fun this is interesting so just clicking around a bit. This feature is labeled code and SQL, but that doesn't seem right. In fact, is also just not writing code, right? It's also related to Charlotte. Oh, look, there's some Oh, that's interesting. It's Yeah. What one looks like? Yeah. Ah, this is maybe an interesting time to start talking about activation densities or do is that not something we is that is there never an interesting time to start activation densities? It is always the right time, Tom. Go for it. Oh, look at CLT. Gemma TV is is really scheming a bit here. Um, particularly terrible because these are single layer transcoders we trained for Gemoscope. Well, just as well we caught it while it was scheming ineptly. They should not be standing for CLTs. Um, this is interesting because like this one at the at the top range of its activation distribution, you feature can take a take an activation range from zero to infinity. And here we can see a histogram of that range of activations. And it seems like at the absolute top of the activation range, it really is about Charlotte um and like Charlotte, North Carolina and various North Car Carolina related things. And then it becomes like also some other things like Charlotte song. Oh yeah, that's fair. Um but then it becomes kind of strangely polymantic here, right? Now there's some some code of some sort, something about a scientific instrument. Um, so it's yeah the but the actual activation of this feature which I think is what this green this green vertical bar means is way inside the sort of poly semantic region. Yeah. So, I mean, I would just the I think the TLDDR here is this feature isn't so good. Yeah. Um, which happens sometimes. I mean, I'll also note if you scroll back down to the intervals, it says that the bottom interval contains like 99.98% of all the times this thing activates. So, so maybe this is just this is just a bit of a blooper. Yeah. I mean, wait, that's a lot of identical text. Oh, I guess probably just a bug in the UI. [Music] Okay, so it looks like we've got a bit of a blooper on that feature. Let's move swiftly on. Um, I didn't just a reminder for viewers, Neil trained this transcoder, not me. No, no, no, no. Arthur Connie trained this trans. Ah, right. Okay. Well, I guess he'll be hearing about this at Perf. No, no, no. I'm proud of him. He was like, "Transcoders, there's a weird new technique that no one really cares about. I'll just randomly train someone to shove them in our sees release because why not?" Okay, we love Arthur. So, workforce sighted man, especially you've now given up once. I'm afraid that we have we have also lost momentum again. Um, where would you guys experienced circuit traces? Where would you be going from here? So maybe so just to just to call out one thing. Uh if you you you've done a cool thing already like you've got we start we've got capital we've got Charlotte we've we've got capital goes to say a capital Charlotte and capital are going to North Carolina. I think if you commandc on on state you would see that it's also going maybe a little bit into North Carolina. Command click. Yeah. What would I Oh this is actually something that's interesting to talk about. What should I make of these these dead ends? Oh, no, never mind. I just can't read dead ends. So, these are not actually dead ends. They just don't they don't they have no like they they just go onwards to an output. Yeah. Okay. We'll talk about the green edges. Those are just the strong the strongest edges. Oh, look. We can see the same business with multiple senses like quantum state. Oo fun. That's cool. Um word state and less strongly the word agency. It would be interesting government agencies. I know this is not the function of circuit tracing but it' be very interesting to see the the way in which these senses get filtered out. I guess this is about inhibition isn't it? Which is something that we we can't really look at here. But it'd be very interesting to be able to look at that in like that sort of inhibitory process where it's like oh no no no actually no quantum states are required right I mean it wouldn't even surprise me if it it gets implicitly filtered out because later on once it knows what it needs it searches for the info and it looks at the relevant subspaces on the state token and the quantum info is always there it just never gets access right Yeah. And later layers tends to write with just higher norm as well. And so it just gets overridden by default as well. Oh no, one of my one of my Yeah, we're good. I think for the question of what should we even do here? I think one one fun thing we could do uh although we are free to just not do that is to think of okay so so we've identified that there's probably a North Carolina super node which I feel like we've done a good job of of making and there's probably some state stuff going on maybe some capital stuff going on in terms of like feature a bundle of features which matter and then if we had those then we could just try steering on each of those and seeing if the expected thing happens like if we steer down on the capital does the model say another city that's not the capital. Steer on the state. Forget about the state. Wait a minute. So if I take So if I want to be in the say a capital thing. Mhm. So now now is it time? Do I get to play with the steering tool? I think you've earned your right. I what I would say is like a meta meta principle is um like usually you kind of trace through the graph and you find this sort of like representative path where there's one feature of each of these classes and they're like connecting in a way that makes sense and then I think we often do a kind of a phase two where then you go back and you're like you you know you click on one of those state this the state embedding or one of the state features and you just and you notice that there's like six other state features it and you like command click on all those group them together and ju just so to make sure you're kind of like gobbling up, you know, as much juice as as you can. And then like once you've sort of grabbed all of the all of the nodes that seem to represent a similar thing, then you're kind of in a position to steer with them and be capturing like more of the variance that's associated with that concept. So I think I think you've kind of achieved that for North Carolina right now. Like you've got you found like a bunch of North Carolina features. So I think we could like steer but I do not have all I probably have not c captured all of the capital features. So there's this guy. So the way that my my thought process here was I'm going to look for all the other features which are capital related. And the way that I'm going to do that is by looking at one step like forward projections from the word capital to the final token. And I guess I'm just going to commandclick these to kind of put them into this working space here. So, I mean, looking at this, it feels like there ought to be a bunch of automated ways we could make these super nodes like sort by cosign sim with the decoder vectors or something. And one of the things I found interesting in your paper was you said, "No, you actually just need human labor. We tried automating it. It didn't really work. Is that correct? I I don't think I don't think we said that as much as uh that exactly what you're saying is right and we haven't had a chance to try yet. Um we we haven't it's not as much that we've had negative results as much as as we've we've not had a chance yet to we've not dedicated the time to to try it. But it certainly seems like decoder sim or auto interparity to group nodes would be probably pretty reasonable here. Yeah. Actation correlation. Yeah. I do think there is like yeah it I think it's a bit trickier than it seems like it it might be and that like um there's kind of a sense in which the way you want to group the nodes can be contextual. Um so like for instance uh like if you have like I remember in the capital the state containing Dallas prompt there was like um there was a feature that fired on like names of cities in Texas. There was another feature that fired on like the word Texas and then another feature that fired on like discussion of like legal you know like juristp prudence in Texas like the Texas Supreme Court rulings. Um and in another context you these might be totally different in their kind of implications. Um but in this context all that mattered about them was that they all had to do with Texas. Um, so like the sort of which things should be grouped together can be a bit uh can can be sort of contextual and dependent on the role you think they're playing in the circuit. That feels like it should be solvable. Like for example, you could do decode a cosine sim but restrict to the subspace of encoder vectors of downstream features. I think even more direct thing would be like similarity of the graph edges. Um yeah, that might just be way simpler. But yeah, something in this category feels promising. Yeah. Uh cool. All right. I totally miss misunderstood that part of your paper. So in the background I have been trying to get enough capital city related things and I I've I've run into a bit of a stumbling block which is this is definitely capital this seems pretty capital city like. So I'm happy to I'm pretty happy to move this guy into Hold on. I'm pressing G. There we go. Um, and this one, this guy's major cities. And I guess it's hard for me to disambiguate to what extent it is like definitely major cities versus capital cities. Yeah. Like there's Philadelphia, but you know, I guess I would assume that that is the state capital of Philadelphia. the Chicago land which land real or is that a theme park? Yeah, I think this isn't capitals because I think Philadelphia is not the capital of Pennsylvania. Okay. And Seville is not the capital of Spain. So, I'm just going to get rid of this guy. And now I've I've exhausted most of the like sensible looking forward projections of this. Well, this feature is uh a little deranged. So now I've got capital but surprisingly I don't have any any projections from say a capital any like really strong projections from say capital to say a capital. Yeah because I think you're probably going to have to trace back a little bit more from say a capital. Um so if I wanted to figure out how has the model gone from like okay capitals are present in my residual stream they're just here. How do I is there any way in which I could say you know the idea of capitals is present so I should say a capital I think you're likely to find that if you keep clicking a bit more working backwards from say a capital okay so now I'm at say a capital and I know that my cap my capital super node features never mind okay let's just go into this say a capital we are here wait we're Yeah. Yeah. And I would just look at that list, the the input features list. Um. Yep. So that looks like basically just another say capital feature. Yes. Yeah. Right. This is another like Oh, the text went a different way. Yeah. Yep. And so you probably want to command click on that one too. So I'm going to just a minute. Okay, good. So I'm just going to keep But I still have no strong projections from capital to say a capital. Let's carry on. Here's another say a capital feature. Um, this is responding. So here I think this is not a say capital feature because it is just responding. Yeah to the word capital or it's more abstract. It's kind of like discussions of capitals. This is also not this is like capital hill. Okay. So now I'm going to keep backtracking to to this capital feature. Now you're starting to get somewhere because you notice it has inputs from the embedding of capital and then also this feature the the word capital capital and state right so government buildings the word capital. So, it seems like this is the transition at this point. It's like, yeah. Now, and you'll notice that one, if you go back to that one, um, that was on the capital token. So, I think that really is the transition because that's where the information is being moved across token positions. [Music] And this one is I see And this is the word capital in multiple senses because the model hasn't had a chance to disambiguate this. And is this already in a super node? It's hard for me to see. No, I think it's not. It would show a little bracket in the label if it was. Oh, great. Have you ever done attribution graphs to logic differences? Like I'm looking at this and I'm thinking what I'm actually interested in is how does it know to say rally rather than shot? North Carolina might just be boosting saying all North Carolina related cities. I care about how it knows that it's about the capital and looking at that info. Yeah, that's a thing you can do. Uh we've like we've done that sometimes. Uh yeah, it's it just kind of depends what you're looking for because there's always this question of sort of, you know, why did the model say X? What you're really asking is why did the model say X and not Y, but you might be interested in different values of Y. Yeah. Yeah. This is maybe most relevant when we've done anything with looking at mathemat, you know, why did you say this number versus all the other numbers and that we filter out all of the I'm just saying a number of features. So here if I look like hang on if I go back to say a capital right I've got a couple of say capital features am I right in thinking there's an unusually high amount of error node in these like or is this because now I'm looking here and I'm seeing that most of the top things most of the top inputs are nodes. Well, I think by Oh, never mind. Yeah. Um, I guess what I'm wondering is like I I I don't yet feel like I understand how the transition from capital to say a capital has like actually mechanistically happened. Yeah, I think I mean something's happening with so some attention head decided to attend to the word capital probably more than one uh and bring it over to the final token position and this isn't telling you why why they attended. I wonder if there's a generic move from move from thinking about X to saying X head or heads seems seems possible. Yeah. Um anyway, okay. So, there's some Yes. So, we now we sort of we've narrowed down where the mystery is. um well at least what I what I consider one of the most mysterious parts which is how we got from okay we're thinking about capitals now we're going to say a capital um I guess my intuition before even looking at this graph would have been on the word is the model knows that it's going to be saying something like probably a noun easy to figure that out and then it's trying to figure out what kind of categories of noun it wants to say And it seems like a pretty natural way to implement that is to have some attention head whose job is to like extract categories. And it seems plausible that you could just have a linear thing that maps capital to say capital, country to say country, etc. Like that seems like a pretty natural way for models to represent things because they very often will go from kind of describing a category earlier on to saying an instance of that category later on. Yeah. Yeah. And my guess would have been there's like an attention head that where like it kind of the there's like a query that's representing I better come up with something to say right now cuz the word is just appeared and then there's like and then it goes looking for a key that's like uh you know categories of things to say and then the sentence capital is one of the more salient. Is it plausible that so I was wondering how do we go from say a capital to okay this particular capital is it actually maybe just the case that there is no further explanation needed than Raleigh is the intersection of capitals and North Carolina locations I think that's what's happening I've got two two subspaces they intersect in one place that the soft the soft max lets you do it um because you you just you You upweight North Carolina can upweight all the logets associated with North Carolina stuff. Capital say a capital associates all the logets associated with capitals and then you know the softmax means that only the intersection of those two will will get chosen. I apologize for the cruise ship in the background if anyone can hear the horn. I hear something actually have an old paper called additive mechanisms behind facial recall that found this exact thing for like kind of the neat stories of factual recall like there's a database actually more complicated and one of the other mechanisms is this additive thing where have you published this uh yeah it's called additive mechanisms by factual recall led by guy called Bill Chukai who's since joined my team at Deep Mind. That sounds interesting. Oh, good reading. Something interesting we noticed in in Haiku is that Haiku seems to use both the additive mechanism and a more kind of lookup table recall mechanism and that like Haiku has a Texas fe Texas Texas features and say a capital features and they both they go directly to the logit but then they also go to a intermediate say Austin feature which is the capital of Texas. So it's kind of like doing the additive thing and then also doing the kind of and function thing just in case. Yeah, that's interesting. I guess this is probably a general a general general observation like you're saying earlier if you're like does it do x or y answer is probably yes. Well well I think the interesting thing is that we found these features in haiku but we've yet to find a single equivalent feature in Gemma 2 2b. So this might be something that emerges with scale. uh initially you just do the sort of like additive mechanism and then eventually you have sort of like enough room to to do the refinement part that Haiku is doing. I wonder what the the advantage of doing the the non-additive thing is well it lets you do subsequent computations on top of that. Oh of course yes yes because you only get one softax at the end so you better make count. Yes. Just skimming this old paper. Uh, one of the particularly cursed things we found is this thing we called mixed attention heads where you get some that look at the subject like Michael Jordan and just boost things related to that. Some that look at the relationship like sport and boost sports. And then there are some heads that look at both and are doing both of these things at once. So that if you look at the output of the head, basketball's the highest. You're like, "Ah, has it done something intelligent?" And it's just both of those heads kind of stuck together in a messy interfer. Why are models like this? They're just really parallel. It's like the really hard constraint on a transformer until we invented reasoning models is like the number of serial steps. But like they got tens of thousands of dimensions, however many tokens, they can they can do everything in parallel. Yeah. I mean, and I think we see this in these graphs as well, right? As as Jack was mentioning earlier, that they're always doing seven different things at once. Um I I do want since we're, you know, close, not close to time, we have some time, but I I do want to make sure that Tom gets gets the reward of pressing the steer. Okay. There are two features that I wanted to try now that I've noticed them in the interface. Okay. Maybe maybe one question before we start doing this. Um I'm curious what you guys do to minimize confirmation bias in these investigations and like how easy you think it is to become attached to a hypothesis and think it supports that and actually it's way more complicated. I mean, I think interventions are are a big one, right? Uh, and in particular particular sort of like coming up with the hypothesis without doing an intervention at all, you know, and then validating validating it with an intervention helps rule out incorrect hypotheses here. Yeah. But it seems like you could have there's actually three different things all going on simultaneously and you figure out one, then you validate it with steering and you're like, "Oh my god, I'm so smart. this is all that's going on. Yeah, I think that's right. I think I I think that oftentimes uh we we claim that we found one thing that the mall is doing, but it's also doing a bunch of other things. I mean, even in this example, right, like one of the things you can try doing by steering is steering down on the, you know, North Carolina node and then maybe you get the model to like flip its prediction. However, we've all seen together that there's also like a direct, you know, Charlotte to rally path. So clearly when we're doing that intervention, we're not explaining everything. Um, yeah, I think that is a true that is a true consequence of the model doing seven things at once. Makes sense. Um, hey Tom, can you add the rally and Charlotte outputs to this graph as well? I certainly can. Rally and Charlotte. It would be nice. I think you added rather than Charlotte. Almost there. It would be nice if it was possible to zoom or scroll or otherwise. Uh I've already requested scrolling from Johnny. Okay. Um right. So now I'm going to participate in some staring. Let's go. North Carolina. Let's do it. Okay. So you can click some buttons. I This is a fun Yeah. Yeah. That's probably the first thing you want to do is is Yeah. You can try steer all on North Carolina if you want and maybe just before you do or at the same time or something we could just say briefly what what this does or something which is which is that you know we've identified a bunch of features we've grouped them uh we can choose one of the features or or the group of features here all of the North Carolina features and then all of those features represent like you know um a a direction in which like that layers transcoder and or if you want to think about it MLP rights and we can instead just change that by additively, you know, writing more in that direction, less in that direction. And we can do that at any context in the prompt. And, you know, if if we set it to zero or if we set it to like negative one and sort of it suppresses the concepts. This is, you know, well, it just helps us see what the causal effect of of a future is. Question. So, with the attribution graph, I'm not going to stop me, Neil. I'm staring. you you can steer while I'm asking this question. Don't worry about that. Um so you made all these assumptions about the attribution graph like attention patterns are constant etc. Um when we steer are we still making these assumptions or are we going back to the original model and recmputing everything including attention patterns? Yeah. So I guess there's there's two parts to the question. For attention specifically on the right, you'd see that we freeze attention. Although you can you can stop that from happening. But by default, because we make our hypotheses in in the fake world where attention is frozen and given to us, then we sort of like I mean it'd be nice if our intervention work in the real world, but the first validation is like, do they work in that fake world where we we still keep attention frozen. So that's that's why that's the default, right? like it too disheartening to realize our hypothesis was false immediately. We need to like wait a bit first. You need to first convince yourself that your hypothesis is real in the fake world you've invented. But like will it ever be the case that it is real in the fake world I invented false in the real world and I care about the fact that it's real in the fake world? I think I think so actually that that it tells you that like you know this feature was influencing this feature via the like OV of an attention head but for some reason ablating it and then like so if you steer in the fake world and confirm that then you've confirmed that but then if you if you allow attention to to readjust and that no longer happens it means that the this feature was also influencing the attention pattern somehow and like ablating it mucked with the attention pattern so that now that like attention head is no longer paying attention to where it was before and so that OV pathway isn't there anymore. Um but the like original thing you found is still kind of a true thing. Uh okay. So I think the thing that I would be sold on is that we should simultaneously steer with frozen attention and unfrozen attention and display both. I'd buy that one. That that seems good. That that seems like solidly useful info. I think the thing I was skeptical of is would I ever would it ever be useful to me to just not even have the unfrozen attention to info and not know if it actually generalized or not. But I totally buy that doing both is better than just doing one. That seems super reasonable. Now we can search. I think Neil, it's time for you to get into the philosophy of causation. I think uh I have a very exciting book for you. Well, the philosophy of causation just means yolo. You change the prompt and see what happens. No, there's there's a there's a lot of like some um there's there's some like what does it mean for something to actually be a cause? I don't know, man. I want to just prompt them. I thought you'd be excited. So, some comments on what what's going on in the screen here. Um, so first of all, you inhibited a bunch of North Carolina features and you noticed that when you inhibited them sufficiently strongly, it no longer says Raleigh. It says, go back and do that. I was. So here I press steer all and then the default is to ablate them all by you set them to minus one. Yeah. Then I press engage. Why is also North Carolina not unsteered? Um yeah I think the easiest way is if you scroll up a little bit um you can just steer them here. Uh and that will that will yeah that dial will move them all in concert. I think I think the the interface maybe is uh the little bit oh no not mean negatively ablate it means don't stare. Yeah. Um, but I think the default is just get I think the interface is maybe the the beta like the interface is in beta and so maybe doesn't quite do what you expect it to do. Um, but yes, so you're saying we get rid of the North Carolina locations and then Well, this is weird actually because Oh, so yeah. So this is a good an interesting example where you you stopped it from saying Raleigh right then. Yeah. But then it goes on to say another thing in North Carolina. Yeah. But kind of the the attribution graph is sort of only is only telling you about you know that first token but then you know probably what's going on here is that it so you you prevented it from saying Raleigh uh and it's kind of ad limming by saying the city of just to like hedge its bets and then now attention has another chance to go and copy Charlotte I guess. Um well and and to prevent that you can also uh steer negatively on the new tokens which is the right most which then I don't know why it's gray up here that it's amazing. Um, just in case any viewers are as confused as I initially was, um, is is blue. So, we can look at the logit afterwards, but it's a fixed word. Like, it's not being generated, right? So, here, this hasn't really done quite what we hoped. The generated text is the blue tokens apart from the first blue token. We did the general one general thing to note is that ablations generally don't are too don't have much effect and this is like reflecting kind of a real sort of limitation which is that typically the North Carolina features that you happen to have clicked on are only explaining sort of a fraction of all of the North Carolina juice in the model. And so when you are supposedly ablating those features, you're actually only, you know, getting rid of whatever half half a third, who knows of the North Carolina juice. And so typically we find in order to like see the kinds of effects you would expect, you need to kind of overshoot ablations and like negatively steer with the features. Um it's also possible to compute the intervention um on the on the residual stream such that the feature would have read out as z as like zero. Um and that we've had some luck doing that. That seems to be like a bit more of an effective way to get ablations going. Cool. Interesting. I mean yeah so one curiosity there suppose two hypotheses. North Carolina is the only thing going on, but we've only found uh half of the North Carolina juice. Or North Carolina and something else are the two things going on and we found all of the North Carolina juice. In both worlds, probably steering with minus two will break everything, right? Yeah. Yeah. How you can tell the difference? I don't think we're good at telling the difference between those. Fair enough. Yeah. Oh, yeah. Okay. Yeah. So here, yeah. So you here you inhibited North Carolina enough that it's not saying I kept I kept getting rid of North Carolina. I think now would be a good time to try. I kept getting rid of say a capital and eventually it got to so when it was higher up I'd say minus one which should be just like straightforward ablation. I mean I will well I will it does the same trick. It's like oh no I'm going to just do space strong which it seems to love doing. Um, and then if I take it down a bit more, it still does the same business. Oh, no. Now, and then it goes to like is fancy editing. So, this you can see model is kind of bust. Yeah. Like, have I'm surprised this has broken it that much. I mean, models are good at spelling, right? You get so many gnats from being good at spelling. It's deeply ingrained and we've broken its ability to spell. Fascinating. Yeah, I think that even that minus one steering where it said like strong Raleigh. So that it on the surface that looks like the steering didn't do anything, but it actually the steering did do something and that it it it got it to not say Raleigh. Then it had another token where you where it and you're since you're like all this steering is happening, you're steering on the is token primarily which it uh but which is the final token of the prompt. But as soon as it's generated another token, it now has a whole new token with which to copy over North Carolina. So probably like uh yeah, here you go. There you it is. I wonder if we could try a more gentle intervention on just uh North Carolina ignoring say a capital. Yes. Uh just because I was I think it's on in parallel. I was getting some good results. If you look at just the top and you steer negatively on Charlotte and is at like let's say five. Oh, okay. Can we try something crazy and just reload your page? I I feel like you got in a weird state where these buttons are doing weird things and I'm not sure why, but we are loading some nodes. Nothing Nothing that turning something off and on, you know, can't fix. Let's begin steering. Yeah. And then here, steer all and then Yeah. Here is it. Does that Can we steer negatively by dragging that? Oh, look at that. Amazing. Come on then. Minus 0.5 on Charlotte. Yeah. And on is because we've seen that it's moved over to is as well. And do you want also on new tokens? I think we can just do this for now. Yeah, let's go. I have different nose, but I was looking at on my own. It was Yeah. Okay. So, it looks strong Indianapolis. All right. I've got a much more important question. Emanuel, did I just notice that you've got a stcastic parrots t-shirt on? That is right. That is right. It's amazing. Where can I buy one? Oh, um, listen, I don't have my own store. Um, I can I will find where I bought it. We'll help you set it up in like five minutes. Yeah. Yeah. Can I just get uh this on your store and then we'll Perfect. Claude run the store. Even Claude Even Claude can make money off of this. Said with love to Claude. I'm sure Gem and I can make more. It uh it might be fun in like the last few minutes we have to try the the thing you were you were getting at before which was like trying to patch in like a Texas feature or another Yes. Yes. Yes. Oh yeah. So we we have this working intervention. I think now we can try to add a feature uh on the top right. Yes. So I'm going to say textless. That feels like a powerful intervention to me. And this is somewhat finicky because it depends on sort of like the layer at which the feature lives and all that stuff. But we can try to see if we had like Yeah. So I'm going to choose looking at this I'm inclined to choose this one because it is literally a promote the tech promote the token Texas feature. Seems great. It's not like these look like a respond to Texas feature with no immediate downstream effect. That seems it's also in kind of the right layer I think. So yeah, I think layers layer to the one steal this guy and I want to you can click on Charlotte I think here which is probably and then yeah yes so here probably we might want to add it to is as well just uh yeah this is still strong Indianapolis it's in layer 20 so I think you'd want it on the final token native APIs Whoa. What are the What are the log props over is? I wonder what's what's going on here. I can tell it's a Google model. No. Or close to saying Austin. So close. It's just Google. Maybe if you crank it down a little bit. I'm guessing it'll I think you just brain damage it too. It's a common mistake. People often mistake Austin for Google APIs. It's the city of Austin. Yes. Nice. Wow. And wait, that's actually really interesting. You didn't steer on the subsequent tokens, right? Yeah. Yet it still said Austin. It's like it's planning. Can you try doing that again, but not steering on Charlotte? Yeah, you could just do the little trash can at the bottom if you'd like. Yeah. Such a beautiful UI. Activate. Oh, no. We're back. Oh, good. All right. Can you try doing it on Charlotte but not on is I think we're still going to be in strong Indianapolis. Well, you might have to do like twice as hard. Oh, no. Boston. I take it back. Just we didn't bet on that. And it's in layer 20. It's just some heads. Oh, we've frozen attention as well. So, there some late. You could also try not freezing attention if you wanted and and seeing if How does that even work with steering? I guess the opens recmputed. Where is Lanzing? Lanszing is in Michigan. No. Yeah, it's the capital of Michigan. I know it's like Okay, so I guess this is the average. This is the average of North half of North Carolina back in. Okay, never mind. Cancel that. It's still lensing. I bet if you crank it up real hard, it might it might uh come on plus that poor attention head is very confused. a good Texas intervention attention. That is bizarre. I wonder wonder if one of the capital features has some some landling to it or something. I'm not sure. My my guess is what's happened here is um so you've unfrozen attention. So the attention patterns are re adapting in you know in response to the to the new activations you've induced after your steering and you are like massively brain damaging the model on the Charlotte token because you've just done a whole bunch of weird stuff to it and it's probably just fried that token and so no one wants to attend to that token anymore and now now the model is just so from the model's perspective now your prompt is just the capital of the state containing is I can't see Charlotte anymore and then it's just guessing something. So it doesn't matter what you what you put on Charlotte or what you take off of Charlotte. It's just like two. So I'm just going to I'm not going to do this. I'm only going to intervene in on the on the new to on is and the new tokens. I've got a more galaxy brain hypothesis. Even if you haven't brain damaged it, if we've stopped it at thinking about North Carolina on the Charlotte token in like early layers, the heads will never decide to look at it because they don't think it has state information. And then in layer 20, you introduce Texas, but it's too late. That also seems possible. Yeah, yours is more boring, so it's probably true, though. Okay, I'm afraid that I have to go on the hour. So I have we have we succeeded. I think we've solved interpretability. Web means concluded attribution graphs aren't very useful and we should do something else. Oh, Neil, you had to get it in before the end of the You had to. My team put out a blog post on blackbox interpretability this week. I gotta stand it. blackbox interpretability. Yeah, we we had a model stopped you turning it off. It looked really scary. Then we read the train of thought. We realized it was confused and we causally verified it by prompting it in creative ways. It was glorious. Sorry, I thought I thought for some reason you meant like mechanistic interpretability but blackbox and I was like I'm not sure that sentence works but I understand you now. I mean I've got a great paper called full anchors if you want blackbox mechanistic interpretability. That's the chain of thought one paper. Yeah. You just take the intermediate states of computation which in this case are the literal words in the chain of thought and you study their causal dependencies and interactions and you do ablations and you never need to look inside the model if you don't want to. Yeah, super cool. I I liked it a lot. Maybe just to give a like quick recap of you know what we've learned from this whole this exercise. I I would say that this experience you just had of tracing through this graph, lumping together nodes, trying some steering was like reasonably reflective of of what the typical experience is like with a few exceptions. So So yeah, there's like you found a bunch of fe you were able to kind of trace like a decent scaffold of a story of how this stuff came to be. There's a lot of outstanding mysteries having to do with attention heads because we're not showing you any information about what which attention heads were attending where or why they were attending there. Um there were some weird features, some poly semantic features that we kind of just ignored. Um there's uh there were multiple mechanisms going on at once like direct paths and also more like lookupy paths sometimes. Sometimes those were kind of fuzzy. Uh then we tried steering. The steering like was like a little jank, but like kind of does the thing you'd expect once you fiddle with it enough. And there's this kind of guessing game of exactly how hard you have to steer with the features to get the right effect without brain damaging the model. But like usually if you kind of like figure it out, you know, if you like get yourself in the right regime, you can sort of like see the the features you expect having the effects you'd expect, but it's like kind of finicky. Um, so I'd say all this is like pretty typical except just that took like probably way longer than it would if you like kind of know what you're doing coming in. Like like we probably could have like gotten to this state in, you know, 10 10 or 15 minutes. Um, if you're if you're like pretty experienced with this. Um, and also maybe like I think so one thing we've observed is that these graphs are typically a bit better on like or like the our big big cross layer transcoder on Haiku. I think things just tend to look a little cleaner because we we put a lot of, you know, resources into training the CLT and the model is smarter. And so I think that like medical graph we looked at in the first session was like generally sort of crisper than this one because Gemma 2B is like not that smart a model. No, not not Yeah, transcoders are like, you know, smaller than uh than the other transcoders. them. So, I think there's like there's plenty of room to go up from here, but this was like a decently representative kind of uh circuit tracing experience. Great. Yeah, this is really fun. Thanks, guys. Yeah. Any closing reflections, Tom, before you head out? Um, any closing reflections? I think we should do whenever we have new tools, we should probably do something like this. like it's it's quite fun to get a a kind of this is how you use it. Um and a pretty unvarnished look at it. It was good. Um other than that, no, this is great. Thanks everyone. Yeah, I think uh cynicism aside, um I now feel more excited about attribution graphs. I think the thing that has kind of clicked a bit more is oh I think you can just get a lot of info quite fast if you put in a ton of upfront effort. I'm still not convinced it's worth the ton of upfront effort. What if someone else were putting in the upfront effort? Uh but like hey we made these transcoders Tom. Um, but I know tools that can just let you learn a lot about a new problem fast do seem pretty great. So, kudos. On that note, little stuff there. And yeah, thanks everyone. Uh, this was a lot of fun. Yeah. Great. Thanks guys.