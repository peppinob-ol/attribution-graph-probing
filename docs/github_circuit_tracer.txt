Directory Structure:

└── ./
    ├── circuit_tracer
    │   ├── attribution
    │   │   ├── __init__.py
    │   │   ├── attribute.py
    │   │   └── context.py
    │   ├── frontend
    │   │   ├── assets
    │   │   │   └── README.md
    │   │   ├── __init__.py
    │   │   ├── feature_models.py
    │   │   ├── graph_models.py
    │   │   ├── local_server.py
    │   │   └── utils.py
    │   ├── transcoder
    │   │   ├── __init__.py
    │   │   ├── activation_functions.py
    │   │   ├── cross_layer_transcoder.py
    │   │   └── single_layer_transcoder.py
    │   ├── utils
    │   │   ├── __init__.py
    │   │   ├── create_graph_files.py
    │   │   ├── disk_offload.py
    │   │   └── hf_utils.py
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── graph.py
    │   └── replacement_model.py
    ├── demos
    │   ├── graph_visualization.py
    │   └── utils.py
    ├── tests
    │   ├── test_attribution_clt.py
    │   ├── test_attributions_gemma.py
    │   ├── test_attributions_llama.py
    │   ├── test_cross_layer_transcoder.py
    │   ├── test_graph.py
    │   ├── test_hf_utils.py
    │   └── test_single_layer_transcoder.py
    ├── CONTRIBUTING.md
    └── README.md



---
File: /circuit_tracer/attribution/__init__.py
---




---
File: /circuit_tracer/attribution/attribute.py
---

"""
Build an **attribution graph** that captures the *direct*, *linear* effects
between features and next-token logits for a *prompt-specific*
**local replacement model**.

High-level algorithm (matches the 2025 ``Attribution Graphs`` paper):
https://transformer-circuits.pub/2025/attribution-graphs/methods.html

1. **Local replacement model** - we configure gradients to flow only through
   linear components of the network, effectively bypassing attention mechanisms,
   MLP non-linearities, and layer normalization scales.
2. **Forward pass** - record residual-stream activations and mark every active
   feature.
3. **Backward passes** - for each source node (feature or logit), inject a
   *custom* gradient that selects its encoder/decoder direction.  Because the
   model is linear in the residual stream under our freezes, this contraction
   equals the *direct effect* A_{s->t}.
4. **Assemble graph** - store edge weights in a dense matrix and package a
   ``Graph`` object.  Downstream utilities can *prune* the graph to the subset
   needed for interpretation.
"""

import logging
import time
from typing import List, Literal, Optional, Tuple, Union

import torch
from tqdm import tqdm

from circuit_tracer.graph import Graph
from circuit_tracer.replacement_model import ReplacementModel
from circuit_tracer.utils import get_default_device
from circuit_tracer.utils.disk_offload import offload_modules


@torch.no_grad()
def compute_salient_logits(
    logits: torch.Tensor,
    unembed_proj: torch.Tensor,
    *,
    max_n_logits: int = 10,
    desired_logit_prob: float = 0.95,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Pick the smallest logit set whose cumulative prob >= *desired_logit_prob*.

    Args:
        logits: ``(d_vocab,)`` vector (single position).
        unembed_proj: ``(d_model, d_vocab)`` unembedding matrix.
        max_n_logits: Hard cap *k*.
        desired_logit_prob: Cumulative probability threshold *p*.

    Returns:
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            * logit_indices - ``(k,)`` vocabulary ids.
            * logit_probs   - ``(k,)`` softmax probabilities.
            * demeaned_vecs - ``(k, d_model)`` unembedding columns, demeaned.
    """

    probs = torch.softmax(logits, dim=-1)
    top_p, top_idx = torch.topk(probs, max_n_logits)
    cutoff = int(torch.searchsorted(torch.cumsum(top_p, 0), desired_logit_prob)) + 1
    top_p, top_idx = top_p[:cutoff], top_idx[:cutoff]

    cols = unembed_proj[:, top_idx]
    demeaned = cols - unembed_proj.mean(dim=-1, keepdim=True)
    return top_idx, top_p, demeaned.T


def compute_partial_influences(edge_matrix, logit_p, row_to_node_index, max_iter=128, device=None):
    """Compute partial influences using power iteration method."""
    device = device or get_default_device()

    normalized_matrix = torch.empty_like(edge_matrix, device=device).copy_(edge_matrix)
    normalized_matrix = normalized_matrix.abs_()
    normalized_matrix /= normalized_matrix.sum(dim=1, keepdim=True).clamp(min=1e-8)

    influences = torch.zeros(edge_matrix.shape[1], device=normalized_matrix.device)
    prod = torch.zeros(edge_matrix.shape[1], device=normalized_matrix.device)
    prod[-len(logit_p) :] = logit_p

    for _ in range(max_iter):
        prod = prod[row_to_node_index] @ normalized_matrix
        if not prod.any():
            break
        influences += prod
    else:
        raise RuntimeError("Failed to converge")

    return influences


def attribute(
    prompt: Union[str, torch.Tensor, List[int]],
    model: ReplacementModel,
    *,
    max_n_logits: int = 10,
    desired_logit_prob: float = 0.95,
    batch_size: int = 512,
    max_feature_nodes: Optional[int] = None,
    offload: Literal["cpu", "disk", None] = None,
    verbose: bool = False,
    update_interval: int = 4,
) -> Graph:
    """Compute an attribution graph for *prompt*.

    Args:
        prompt: Text, token ids, or tensor - will be tokenized if str.
        model: Frozen ``ReplacementModel``
        max_n_logits: Max number of logit nodes.
        desired_logit_prob: Keep logits until cumulative prob >= this value.
        batch_size: How many source nodes to process per backward pass.
        max_feature_nodes: Max number of feature nodes to include in the graph.
        offload: Method for offloading model parameters to save memory.
                 Options are "cpu" (move to CPU), "disk" (save to disk),
                 or None (no offloading).
        verbose: Whether to show progress information.
        update_interval: Number of batches to process before updating the feature ranking.

    Returns:
        Graph: Fully dense adjacency (unpruned).
    """

    logger = logging.getLogger("attribution")
    logger.propagate = False
    handler = None
    if verbose and not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter("%(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARNING)

    offload_handles = []
    try:
        return _run_attribution(
            model=model,
            prompt=prompt,
            max_n_logits=max_n_logits,
            desired_logit_prob=desired_logit_prob,
            batch_size=batch_size,
            max_feature_nodes=max_feature_nodes,
            offload=offload,
            verbose=verbose,
            offload_handles=offload_handles,
            update_interval=update_interval,
            logger=logger,
        )
    finally:
        for reload_handle in offload_handles:
            reload_handle()

        logger.removeHandler(handler)


def _run_attribution(
    model,
    prompt,
    max_n_logits,
    desired_logit_prob,
    batch_size,
    max_feature_nodes,
    offload,
    verbose,
    offload_handles,
    update_interval=4,
    logger=None,
):
    start_time = time.time()
    # Phase 0: precompute
    logger.info("Phase 0: Precomputing activations and vectors")
    phase_start = time.time()
    input_ids = model.ensure_tokenized(prompt)

    ctx = model.setup_attribution(input_ids)
    activation_matrix = ctx.activation_matrix

    logger.info(f"Precomputation completed in {time.time() - phase_start:.2f}s")
    logger.info(f"Found {ctx.activation_matrix._nnz()} active features")

    if offload:
        offload_handles += offload_modules(model.transcoders, offload)

    # Phase 1: forward pass
    logger.info("Phase 1: Running forward pass")
    phase_start = time.time()
    with ctx.install_hooks(model):
        residual = model.forward(input_ids.expand(batch_size, -1), stop_at_layer=model.cfg.n_layers)
        ctx._resid_activations[-1] = model.ln_final(residual)
    logger.info(f"Forward pass completed in {time.time() - phase_start:.2f}s")

    if offload:
        offload_handles += offload_modules([block.mlp for block in model.blocks], offload)

    # Phase 2: build input vector list
    logger.info("Phase 2: Building input vectors")
    phase_start = time.time()
    feat_layers, feat_pos, _ = activation_matrix.indices()
    n_layers, n_pos, _ = activation_matrix.shape
    total_active_feats = activation_matrix._nnz()

    logit_idx, logit_p, logit_vecs = compute_salient_logits(
        ctx.logits[0, -1],
        model.unembed.W_U,
        max_n_logits=max_n_logits,
        desired_logit_prob=desired_logit_prob,
    )
    logger.info(
        f"Selected {len(logit_idx)} logits with cumulative probability {logit_p.sum().item():.4f}"
    )

    if offload:
        offload_handles += offload_modules([model.unembed, model.embed], offload)

    logit_offset = len(feat_layers) + (n_layers + 1) * n_pos
    n_logits = len(logit_idx)
    total_nodes = logit_offset + n_logits

    max_feature_nodes = min(max_feature_nodes or total_active_feats, total_active_feats)
    logger.info(f"Will include {max_feature_nodes} of {total_active_feats} feature nodes")

    edge_matrix = torch.zeros(max_feature_nodes + n_logits, total_nodes)
    # Maps row indices in edge_matrix to original feature/node indices
    # First populated with logit node IDs, then feature IDs in attribution order
    row_to_node_index = torch.zeros(max_feature_nodes + n_logits, dtype=torch.int32)
    logger.info(f"Input vectors built in {time.time() - phase_start:.2f}s")

    # Phase 3: logit attribution
    logger.info("Phase 3: Computing logit attributions")
    phase_start = time.time()
    for i in range(0, len(logit_idx), batch_size):
        batch = logit_vecs[i : i + batch_size]
        rows = ctx.compute_batch(
            layers=torch.full((batch.shape[0],), n_layers),
            positions=torch.full((batch.shape[0],), n_pos - 1),
            inject_values=batch,
        )
        edge_matrix[i : i + batch.shape[0], :logit_offset] = rows.cpu()
        row_to_node_index[i : i + batch.shape[0]] = (
            torch.arange(i, i + batch.shape[0]) + logit_offset
        )
    logger.info(f"Logit attributions completed in {time.time() - phase_start:.2f}s")

    # Phase 4: feature attribution
    logger.info("Phase 4: Computing feature attributions")
    phase_start = time.time()
    st = n_logits
    visited = torch.zeros(total_active_feats, dtype=torch.bool)
    n_visited = 0

    pbar = tqdm(total=max_feature_nodes, desc="Feature influence computation", disable=not verbose)

    while n_visited < max_feature_nodes:
        if max_feature_nodes == total_active_feats:
            pending = torch.arange(total_active_feats)
        else:
            influences = compute_partial_influences(
                edge_matrix[:st], logit_p, row_to_node_index[:st]
            )
            feature_rank = torch.argsort(influences[:total_active_feats], descending=True).cpu()
            queue_size = min(update_interval * batch_size, max_feature_nodes - n_visited)
            pending = feature_rank[~visited[feature_rank]][:queue_size]

        queue = [pending[i : i + batch_size] for i in range(0, len(pending), batch_size)]

        for idx_batch in queue:
            n_visited += len(idx_batch)

            rows = ctx.compute_batch(
                layers=feat_layers[idx_batch],
                positions=feat_pos[idx_batch],
                inject_values=ctx.encoder_vecs[idx_batch],
                retain_graph=n_visited < max_feature_nodes,
            )

            end = min(st + batch_size, st + rows.shape[0])
            edge_matrix[st:end, :logit_offset] = rows.cpu()
            row_to_node_index[st:end] = idx_batch
            visited[idx_batch] = True
            st = end
            pbar.update(len(idx_batch))

    pbar.close()
    logger.info(f"Feature attributions completed in {time.time() - phase_start:.2f}s")

    # Phase 5: packaging graph
    selected_features = torch.where(visited)[0]
    if max_feature_nodes < total_active_feats:
        non_feature_nodes = torch.arange(total_active_feats, total_nodes)
        col_read = torch.cat([selected_features, non_feature_nodes])
        edge_matrix = edge_matrix[:, col_read]

    # sort rows such that features are in order
    edge_matrix = edge_matrix[row_to_node_index.argsort()]
    final_node_count = edge_matrix.shape[1]
    full_edge_matrix = torch.zeros(final_node_count, final_node_count)
    full_edge_matrix[:max_feature_nodes] = edge_matrix[:max_feature_nodes]
    full_edge_matrix[-n_logits:] = edge_matrix[max_feature_nodes:]

    graph = Graph(
        input_string=model.tokenizer.decode(input_ids),
        input_tokens=input_ids,
        logit_tokens=logit_idx,
        logit_probabilities=logit_p,
        active_features=activation_matrix.indices().T,
        activation_values=activation_matrix.values(),
        selected_features=selected_features,
        adjacency_matrix=full_edge_matrix,
        cfg=model.cfg,
        scan=model.scan,
    )

    total_time = time.time() - start_time
    logger.info(f"Attribution completed in {total_time:.2f}s")

    return graph



---
File: /circuit_tracer/attribution/context.py
---

"""
Attribution context for managing hooks during attribution computation.
"""

import contextlib
import weakref
from functools import partial
from typing import Callable, List, Tuple

import numpy as np
import torch
from einops import einsum
from transformer_lens.hook_points import HookPoint


class AttributionContext:
    """Manage hooks for computing attribution rows.

    This helper caches residual-stream activations **(forward pass)** and then
    registers backward hooks that populate a write-only buffer with
    *direct-effect rows* **(backward pass)**.

    The buffer layout concatenates rows for **feature nodes**, **error nodes**,
    **token-embedding nodes**

    Args:
        activation_matrix (torch.sparse.Tensor):
            Sparse `(n_layers, n_pos, n_features)` tensor indicating **which**
            features fired at each layer/position.
        error_vectors (torch.Tensor):
            `(n_layers, n_pos, d_model)` - *residual* the CLT / PLT failed to
            reconstruct ("error nodes").
        token_vectors (torch.Tensor):
            `(n_pos, d_model)` - embeddings of the prompt tokens.
        decoder_vectors (torch.Tensor):
            `(total_active_features, d_model)` - decoder rows **only for active
            features**, already multiplied by feature activations so they
            represent a_s * W^dec.
    """

    def __init__(
        self,
        activation_matrix: torch.sparse.Tensor,
        error_vectors: torch.Tensor,
        token_vectors: torch.Tensor,
        decoder_vecs: torch.Tensor,
        encoder_vecs: torch.Tensor,
        encoder_to_decoder_map: torch.Tensor,
        decoder_locations: torch.Tensor,
        logits: torch.Tensor,
    ) -> None:
        n_layers, n_pos, _ = activation_matrix.shape

        # Forward-pass cache
        self._resid_activations: List[torch.Tensor | None] = [None] * (n_layers + 1)
        self._batch_buffer: torch.Tensor | None = None
        self.n_layers: int = n_layers

        self.logits = logits
        self.activation_matrix = activation_matrix
        self.error_vectors = error_vectors
        self.token_vectors = token_vectors
        self.decoder_vecs = decoder_vecs
        self.encoder_vecs = encoder_vecs

        self.encoder_to_decoder_map = encoder_to_decoder_map
        self.decoder_locations = decoder_locations

        total_active_feats = activation_matrix._nnz()
        self._row_size: int = total_active_feats + (n_layers + 1) * n_pos  # + logits later

    def _caching_hooks(self, feature_input_hook: str) -> List[Tuple[str, Callable]]:
        """Return hooks that store residual activations layer-by-layer."""

        proxy = weakref.proxy(self)

        def _cache(acts: torch.Tensor, hook: HookPoint, *, layer: int) -> torch.Tensor:
            proxy._resid_activations[layer] = acts
            return acts

        hooks = [
            (f"blocks.{layer}.{feature_input_hook}", partial(_cache, layer=layer))
            for layer in range(self.n_layers)
        ]
        hooks.append(("unembed.hook_pre", partial(_cache, layer=self.n_layers)))
        return hooks

    def _compute_score_hook(
        self,
        hook_name: str,
        output_vecs: torch.Tensor,
        write_index: slice,
        read_index: slice | np.ndarray = np.s_[:],
    ) -> Tuple[str, Callable]:
        """
        Factory that contracts *gradients* with an **output vector set**.
        The hook computes A_{s->t} and writes the result into an in-place buffer row.
        """

        proxy = weakref.proxy(self)

        def _hook_fn(grads: torch.Tensor, hook: HookPoint) -> None:
            proxy._batch_buffer[write_index] += einsum(
                grads.to(output_vecs.dtype)[read_index],
                output_vecs,
                "batch position d_model, position d_model -> position batch",
            )

        return hook_name, _hook_fn

    def _make_attribution_hooks(self, feature_output_hook: str) -> List[Tuple[str, Callable]]:
        """Create the complete backward-hook for computing attribution scores."""

        n_layers, n_pos, _ = self.activation_matrix.shape
        nnz_layers, nnz_positions = self.decoder_locations

        # Feature nodes
        feature_hooks = [
            self._compute_score_hook(
                f"blocks.{layer}.{feature_output_hook}",
                self.decoder_vecs[layer_mask],
                write_index=self.encoder_to_decoder_map[layer_mask],
                read_index=np.s_[:, nnz_positions[layer_mask]],
            )
            for layer in range(n_layers)
            if (layer_mask := nnz_layers == layer).any()
        ]

        # Error nodes
        def error_offset(layer: int) -> int:  # starting row for this layer
            return self.activation_matrix._nnz() + layer * n_pos

        error_hooks = [
            self._compute_score_hook(
                f"blocks.{layer}.{feature_output_hook}",
                self.error_vectors[layer],
                write_index=np.s_[error_offset(layer) : error_offset(layer + 1)],
            )
            for layer in range(n_layers)
        ]

        # Token-embedding nodes
        tok_start = error_offset(n_layers)
        token_hook = [
            self._compute_score_hook(
                "hook_embed",
                self.token_vectors,
                write_index=np.s_[tok_start : tok_start + n_pos],
            )
        ]

        return feature_hooks + error_hooks + token_hook

    @contextlib.contextmanager
    def install_hooks(self, model: "ReplacementModel"):
        """Context manager instruments the hooks for the forward and backward passes."""
        with model.hooks(
            fwd_hooks=self._caching_hooks(model.feature_input_hook),
            bwd_hooks=self._make_attribution_hooks(model.feature_output_hook),
        ):
            yield

    def compute_batch(
        self,
        layers: torch.Tensor,
        positions: torch.Tensor,
        inject_values: torch.Tensor,
        retain_graph: bool = True,
    ) -> torch.Tensor:
        """Return attribution rows for a batch of (layer, pos) nodes.

        The routine overrides gradients at **exact** residual-stream locations
        triggers one backward pass, and copies the rows from the internal buffer.

        Args:
            layers: 1-D tensor of layer indices *l* for the source nodes.
            positions: 1-D tensor of token positions *c* for the source nodes.
            inject_values: `(batch, d_model)` tensor with outer product
                a_s * W^(enc/dec) to inject as custom gradient.

        Returns:
            torch.Tensor: ``(batch, row_size)`` matrix - one row per node.
        """

        batch_size = self._resid_activations[0].shape[0]
        self._batch_buffer = torch.zeros(
            self._row_size,
            batch_size,
            dtype=inject_values.dtype,
            device=inject_values.device,
        )

        # Custom gradient injection (per-layer registration)
        batch_idx = torch.arange(len(layers), device=layers.device)

        def _inject(grads, *, batch_indices, pos_indices, values):
            grads_out = grads.clone().to(values.dtype)
            grads_out.index_put_((batch_indices, pos_indices), values)
            return grads_out.to(grads.dtype)

        handles = []
        layers_in_batch = layers.unique().tolist()

        for layer in layers_in_batch:
            mask = layers == layer
            if not mask.any():
                continue
            fn = partial(
                _inject,
                batch_indices=batch_idx[mask],
                pos_indices=positions[mask],
                values=inject_values[mask],
            )
            handles.append(self._resid_activations[int(layer)].register_hook(fn))

        try:
            last_layer = max(layers_in_batch)
            self._resid_activations[last_layer].backward(
                gradient=torch.zeros_like(self._resid_activations[last_layer]),
                retain_graph=retain_graph,
            )
        finally:
            for h in handles:
                h.remove()

        buf, self._batch_buffer = self._batch_buffer, None
        return buf.T[: len(layers)]



---
File: /circuit_tracer/frontend/assets/README.md
---

# attribution-graphs-frontend

Snapshot of the frontend code in [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) and [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/methods.html).

To run:

```
git clone git@github.com:anthropics/attribution-graphs-frontend.git
cd attribution-graphs-frontend
npx hot-server
```



---
File: /circuit_tracer/frontend/__init__.py
---




---
File: /circuit_tracer/frontend/feature_models.py
---

from typing import List

from pydantic import BaseModel


class Example(BaseModel):
    tokens_acts_list: List[float]
    train_token_ind: int
    is_repeated_datapoint: bool
    tokens: List[str]


class ExamplesQuantile(BaseModel):
    quantile_name: str
    examples: List[Example]


class Model(BaseModel):
    transcoder_id: str
    index: int
    examples_quantiles: List[ExamplesQuantile]
    top_logits: List[str]
    bottom_logits: List[str]
    act_min: float
    act_max: float
    quantile_values: List[float]
    histogram: List[float]
    activation_frequency: float



---
File: /circuit_tracer/frontend/graph_models.py
---

from typing import List

from pydantic import BaseModel


class Metadata(BaseModel):
    slug: str
    scan: str
    transcoder_list: List[str]
    prompt_tokens: List[str]
    prompt: str
    node_threshold: float | None = None
    schema_version: int | None = 1


class QParams(BaseModel):
    pinnedIds: List[str]
    supernodes: List[List[str]]
    linkType: str
    clickedId: str
    sg_pos: str


class Node(BaseModel):
    node_id: str
    feature: int
    layer: str
    ctx_idx: int
    feature_type: str
    token_prob: float = 0.0
    is_target_logit: bool = False
    run_idx: int = 0
    reverse_ctx_idx: int = 0
    jsNodeId: str
    clerp: str = ""
    influence: float | None = None
    activation: float | None = None

    def __init__(self, **data):
        if "node_id" in data and "jsNodeId" not in data:
            data["jsNodeId"] = data["node_id"]
        super().__init__(**data)

    @classmethod
    def feature_node(cls, layer, pos, feat_idx, influence=None, activation=None):
        """Create a feature node."""

        def cantor_pairing(x, y):
            return (x + y) * (x + y + 1) // 2 + y

        reverse_ctx_idx = 0
        return cls(
            node_id=f"{layer}_{feat_idx}_{pos}",
            feature=cantor_pairing(layer, feat_idx),
            layer=str(layer),
            ctx_idx=pos,
            feature_type="cross layer transcoder",
            jsNodeId=f"{layer}_{feat_idx}-{reverse_ctx_idx}",
            influence=influence,
            activation=activation,
        )

    @classmethod
    def error_node(cls, layer, pos, influence=None):
        """Create an error node."""
        reverse_ctx_idx = 0
        return cls(
            node_id=f"0_{layer}_{pos}",
            feature=-1,
            layer=str(layer),
            ctx_idx=pos,
            feature_type="mlp reconstruction error",
            jsNodeId=f"{layer}_{pos}-{reverse_ctx_idx}",
            influence=influence,
        )

    @classmethod
    def token_node(cls, pos, vocab_idx, influence=None):
        """Create a token node."""
        return cls(
            node_id=f"E_{vocab_idx}_{pos}",
            feature=pos,
            layer="E",
            ctx_idx=pos,
            feature_type="embedding",
            jsNodeId=f"E_{vocab_idx}-{pos}",
            influence=influence,
        )

    @classmethod
    def logit_node(
        cls,
        pos,
        vocab_idx,
        token,
        num_layers,
        target_logit=False,
        token_prob=0.0,
    ):
        """Create a logit node."""
        layer = str(num_layers + 1)
        return cls(
            node_id=f"{layer}_{vocab_idx}_{pos}",
            feature=vocab_idx,
            layer=layer,
            ctx_idx=pos,
            feature_type="logit",
            token_prob=token_prob,
            is_target_logit=target_logit,
            jsNodeId=f"L_{vocab_idx}-{pos}",
            clerp=f'Output "{token}" (p={token_prob:.3f})',
        )


class Link(BaseModel):
    source: str
    target: str
    weight: float


class Model(BaseModel):
    metadata: Metadata
    qParams: QParams
    nodes: List[Node]
    links: List[dict]



---
File: /circuit_tracer/frontend/local_server.py
---

import atexit
import functools
import gzip
import http.server
import json
import logging
import os
import socketserver
import threading
from importlib.resources import files
from pathlib import Path

logger = logging.getLogger(__name__)
logger.propagate = False

DEFAULT_FRONTEND_DIR = files("circuit_tracer") / "frontend/assets"


class ListHandler(logging.Handler):
    """Handler that appends log records to a list."""

    def __init__(self, log_list):
        super().__init__()
        self.log_list = log_list

    def emit(self, record):
        msg = self.format(record)
        self.log_list.append(msg)


class ReusableTCPServer(socketserver.TCPServer):
    allow_reuse_address = True


# Create handler for serving circuit graph data
class CircuitGraphHandler(http.server.SimpleHTTPRequestHandler):
    def __init__(self, *args, frontend_dir=None, data_dir=None, **kwargs):
        self.data_dir = data_dir
        super().__init__(*args, directory=str(frontend_dir), **kwargs)

    def log_message(self, format, *args):
        message = format % args
        logger.info(
            "%s - - [%s] %s" % (self.address_string(), self.log_date_time_string(), message)
        )

    def do_GET(self):
        try:
            self._do_GET()
        except Exception as e:
            logger.exception(f"Error handling GET request: {e}")
            self.send_response(500)
            self.end_headers()

    def _do_GET(self):
        logger.info(f"Received request for {self.path}")

        # Handle data and graph_data requests from local storage
        if self.path.startswith(("/data/", "/graph_data/")):
            # Extract the file path from the URL
            if self.path.startswith("/data/"):
                rel_path = self.path[len("/data/") :].split("?")[0]
            else:  # /graph_data/
                rel_path = self.path[len("/graph_data/") :].split("?")[0]

            # Properly join paths to handle missing slashes
            local_path = os.path.join(self.data_dir, rel_path)

            logger.info(
                f"Rewritten path to {local_path}. "
                f"(self.path: {self.path}; self.data_dir: {self.data_dir})"
            )
            if not os.path.exists(local_path):
                self.send_response(404)
                self.end_headers()
                return

            self.send_response(200)
            with open(local_path, "rb") as f:
                content = f.read()

            # Compress large responses
            if len(content) > 1024**2:  # 1MB threshold
                content = gzip.compress(content, compresslevel=3)
                self.send_header("Content-Encoding", "gzip")

            self.send_header("Content-Type", "application/json")
            self.send_header("Content-Length", len(content))
            self.end_headers()
            self.wfile.write(content)
            return

        super().do_GET()

    def do_POST(self):
        if not self.path.startswith("/save_graph/"):
            self.send_response(404)
            return

        try:
            # Extract scan and slug from the URL path
            parts = self.path.split("?")[0].strip("/").split("/")
            slug = parts[-1]

            logger.info(f"Saving graph for {slug}")

            # Read the request body
            content_length = int(self.headers["Content-Length"])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode("utf-8"))

            # Generate filename with timestamp
            save_path = os.path.join(self.data_dir, f"{slug}.json")

            # Read the existing file and update it
            with open(save_path, "r") as f:
                graph = json.load(f)
                graph["qParams"] = data["qParams"]

            with open(save_path, "w") as f:
                json.dump(graph, f, indent=2)

            self.send_response(200)
            self.end_headers()
            logger.info(f"Graph saved: {save_path}")

        except Exception as e:
            logger.exception(f"Error saving graph: {e}")
            self.send_response(500)
            self.end_headers()


class Server:
    def __init__(self, httpd, server_thread):
        self.httpd = httpd
        self.server_thread = server_thread
        self.logs = []
        self._stopped = False  # Initialize the flag here

        # Add a handler to logger that records to self.logs
        self.log_handler = ListHandler(self.logs)
        self.log_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
        )
        logger.addHandler(self.log_handler)
        logger.setLevel(logging.INFO)
        # Register shutdown with atexit
        atexit.register(self.stop)

    def stop(self):
        # Check if already stopped to prevent multiple calls
        if self._stopped:
            return
        self._stopped = True

        logger.info("Stopping server...")

        try:
            # First, stop accepting new connections
            self.httpd.socket.close()
        except Exception as e:
            logger.debug(f"Error closing socket: {e}")

        # Then shutdown the server
        shutdown_thread = threading.Thread(target=self.httpd.shutdown)
        shutdown_thread.daemon = True
        shutdown_thread.start()

        # Wait with timeout for threads to complete
        shutdown_thread.join(timeout=5)
        self.server_thread.join(timeout=5)

        # Force socket close regardless of shutdown success
        try:
            self.httpd.server_close()
        except Exception as e:
            logger.debug(f"Error during server_close: {e}")

        logger.info("Server stopped")

        # Remove our handler when the server stops
        logger.removeHandler(self.log_handler)

        # Unregister from atexit to avoid duplicate calls
        atexit.unregister(self.stop)

    def get_logs(self):
        """Return the current log messages."""
        return self.logs


def serve(data_dir, frontend_dir=None, port=8032):
    """Start a local HTTP server in a separate thread.

    Args:
        data_dir: Directory for local graph data.
        frontend_dir: Directory containing frontend files. Defaults to DEFAULT_FRONTEND_DIR.
        port: Port to serve on. Defaults to 8032.

    Returns:
        Server object with a stop() method to shut down the server.
    """

    # Use provided directories or defaults
    frontend_dir = Path(frontend_dir).resolve() if frontend_dir else DEFAULT_FRONTEND_DIR

    frontend_dir_path = Path(frontend_dir)
    if not frontend_dir_path.exists() and frontend_dir_path.is_dir():
        raise ValueError(f"Got frontend dir {frontend_dir} but this is not a valid directory")

    logger.info(f"Serving files from: {frontend_dir}")

    # Create a partially applied handler class with configured directories
    handler = functools.partial(CircuitGraphHandler, frontend_dir=frontend_dir, data_dir=data_dir)

    httpd = ReusableTCPServer(("", port), handler)

    # Start the server in a thread
    server_thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    server_thread.start()

    logger.info(f"Serving at http://localhost:{port}")
    logger.info(f"Serving files from: {frontend_dir}")
    logger.info(f"Serving data from: {data_dir}")

    return Server(httpd, server_thread)



---
File: /circuit_tracer/frontend/utils.py
---

import json
import os


def add_graph_metadata(graph_metadata, path):
    assert os.path.exists(os.path.dirname(path))
    if os.path.isdir(path):
        path = os.path.join(path, "graph-metadata.json")

    if os.path.exists(path):
        with open(path, "r") as f:
            metadata = json.load(f)
    else:
        metadata = {"graphs": []}

    metadata["graphs"] = [g for g in metadata["graphs"] if g["slug"] != graph_metadata["slug"]]
    metadata["graphs"].append(graph_metadata)

    with open(path, "w") as f:
        json.dump(metadata, f, indent=2)


def process_token(token: str) -> str:
    return token.replace("\n", "⏎").replace("\t", "→").replace("\r", "↵")



---
File: /circuit_tracer/transcoder/__init__.py
---

from circuit_tracer.transcoder.single_layer_transcoder import (
    SingleLayerTranscoder,
    TranscoderSet,
    load_transcoder_set,
)

__all__ = ["SingleLayerTranscoder", "load_transcoder_set", "TranscoderSet"]



---
File: /circuit_tracer/transcoder/activation_functions.py
---

from typing import Any, Tuple

import torch
from torch import nn


def rectangle(x: torch.Tensor) -> torch.Tensor:
    return ((x > -0.5) & (x < 0.5)).to(x)


class jumprelu(torch.autograd.Function):
    @staticmethod
    def forward(x: torch.Tensor, threshold: torch.Tensor, bandwidth: float) -> torch.Tensor:
        return (x * (x > threshold)).to(x)

    @staticmethod
    def setup_context(
        ctx: Any, inputs: Tuple[torch.Tensor, torch.Tensor, float], output: torch.Tensor
    ) -> None:
        x, threshold, bandwidth = inputs
        del output
        ctx.save_for_backward(x, threshold)
        ctx.bandwidth = bandwidth

    @staticmethod
    def backward(ctx: Any, grad_output: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, None]:
        x, threshold = ctx.saved_tensors
        bandwidth = ctx.bandwidth
        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input
        threshold_grad = torch.sum(
            -(threshold / bandwidth) * rectangle((x - threshold) / bandwidth) * grad_output,
            dim=0,
        )
        return x_grad, threshold_grad, None


class JumpReLU(torch.nn.Module):
    def __init__(self, threshold: float, bandwidth: float = 2) -> None:
        super().__init__()
        self.threshold = nn.Parameter(torch.tensor(threshold))
        self.bandwidth = bandwidth

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return jumprelu.apply(x, self.threshold, self.bandwidth)

    def extra_repr(self) -> str:
        return f"threshold={self.threshold}, bandwidth={self.bandwidth}"


class TopK(nn.Module):
    def __init__(self, k: int):
        super().__init__()
        self.k = k

    def forward(self, x: torch.Tensor):
        _, indices = torch.topk(x, k=self.k, dim=-1)
        gate = torch.zeros_like(x)
        gate.scatter_(dim=-1, index=indices, value=1)
        return x * gate.to(x.dtype)



---
File: /circuit_tracer/transcoder/cross_layer_transcoder.py
---

import glob
import os
from typing import List, Optional, Union

import numpy as np
import torch
from safetensors import safe_open
from torch.nn import functional as F

from circuit_tracer.transcoder.activation_functions import JumpReLU
from circuit_tracer.utils import get_default_device


class CrossLayerTranscoder(torch.nn.Module):
    """
    A cross-layer transcoder (CLT) where features read from one layer and write to all subsequent layers.

    Cross-layer transcoders are the core architecture enabling the circuit tracing methodology.
    Unlike per-layer transcoders, CLT features can "bridge over" multiple MLP layers, allowing
    a single feature to represent computation that spans the entire forward pass. This dramatically
    shortens paths in attribution graphs by collapsing amplification chains into single features.

    Each CLT feature has:
    - One encoder that reads from the residual stream at a specific layer
    - Multiple decoders that can write to all subsequent MLP outputs
    - The ability to represent cross-layer superposition where related computation
    is distributed across multiple transformer layers

    A single CLT provides an alternative to using multiple per-layer transcoders (managed by
    TranscoderSet) for feature-based model interpretation and replacement.

    Attributes:
        n_layers: Number of transformer layers the CLT spans
        d_transcoder: Number of features per layer
        d_model: Dimension of transformer residual stream
        W_enc: Encoder weights for each layer [n_layers, d_transcoder, d_model]
        W_dec: Decoder weights (lazily loaded) for cross-layer outputs
        b_enc: Encoder biases [n_layers, d_transcoder]
        b_dec: Decoder biases [n_layers, d_model]
        activation_function: Sparsity-inducing nonlinearity (default: ReLU)
        lazy_decoder: Whether to load decoder weights on-demand to save memory
        feature_input_hook: Hook point where features read from (e.g., "hook_resid_mid")
        feature_output_hook: Hook point where features write to (e.g., "hook_mlp_out")
        scan: Optional identifier for feature visualization
    """

    def __init__(
        self,
        n_layers: int,
        d_transcoder: int,
        d_model: int,
        activation_function: str = "relu",
        lazy_decoder=True,
        lazy_encoder=False,
        feature_input_hook: str = "hook_resid_mid",
        feature_output_hook: str = "hook_mlp_out",
        scan: Optional[Union[str, List[str]]] = None,
        device: Optional[torch.device] = None,
        dtype: torch.dtype = torch.bfloat16,
        clt_path: Optional[str] = None,
    ):
        super().__init__()

        if device is None:
            device = get_default_device()

        self.n_layers = n_layers
        self.d_transcoder = d_transcoder
        self.d_model = d_model
        self.lazy_decoder = lazy_decoder
        self.lazy_encoder = lazy_encoder
        self.clt_path = clt_path

        self.feature_input_hook = feature_input_hook
        self.feature_output_hook = feature_output_hook
        self.skip_connection = False
        self.scan = scan

        if activation_function == "jump_relu":
            self.activation_function = JumpReLU(
                torch.zeros(n_layers, 1, d_transcoder, device=device, dtype=dtype)
            )
        elif activation_function == "relu":
            self.activation_function = F.relu
        else:
            raise ValueError(f"Invalid activation function: {activation_function}")

        if not lazy_encoder:
            self.W_enc = torch.nn.Parameter(
                torch.zeros(n_layers, d_transcoder, d_model, device=device, dtype=dtype)
            )

        self.b_dec = torch.nn.Parameter(torch.zeros(n_layers, d_model, device=device, dtype=dtype))
        self.b_enc = torch.nn.Parameter(
            torch.zeros(n_layers, d_transcoder, device=device, dtype=dtype)
        )

        if not lazy_decoder:
            self.W_dec = torch.nn.ParameterList(
                [
                    torch.nn.Parameter(
                        torch.zeros(d_transcoder, n_layers - i, d_model, device=device, dtype=dtype)
                    )
                    for i in range(n_layers)
                ]
            )
        else:
            self.W_dec = None

    @property
    def device(self):
        """Get the device of the module's parameters."""
        return self.b_enc.device

    @property
    def dtype(self):
        """Get the dtype of the module's parameters."""
        return self.b_enc.dtype

    def _get_encoder_weights(self, layer_id=None):
        """Get encoder weights, loading from disk if lazy."""
        if not self.lazy_encoder:
            return self.W_enc if layer_id is None else self.W_enc[layer_id]

        if layer_id is not None:
            # Load single layer encoder
            enc_file = os.path.join(self.clt_path, f"W_enc_{layer_id}.safetensors")
            with safe_open(enc_file, framework="pt", device=self.device.type) as f:
                return f.get_tensor(f"W_enc_{layer_id}").to(self.dtype)

        # Load all encoder weights
        W_enc = torch.zeros(
            self.n_layers, self.d_transcoder, self.d_model, device=self.device, dtype=self.dtype
        )
        for i in range(self.n_layers):
            enc_file = os.path.join(self.clt_path, f"W_enc_{i}.safetensors")
            with safe_open(enc_file, framework="pt", device=self.device.type) as f:
                W_enc[i] = f.get_tensor(f"W_enc_{i}").to(self.dtype)
        return W_enc

    def encode(self, x):
        W_enc = self._get_encoder_weights()
        features = torch.einsum("lbd,lfd->lbf", x, W_enc) + self.b_enc[:, None]
        return self.activation_function(features)

    def encode_layer(self, x, layer_id):
        W_enc_layer = self._get_encoder_weights(layer_id)
        features = torch.einsum("...d,fd->...f", x, W_enc_layer) + self.b_enc[layer_id]
        if isinstance(self.activation_function, JumpReLU):
            mask = features > self.activation_function.threshold[layer_id]
            features = features * mask
        else:
            features = self.activation_function(features)
        return features

    def encode_sparse(self, x, zero_first_pos=True):
        """Encode input to sparse activations, processing one layer at a time for memory efficiency.

        This method processes layers sequentially and converts to sparse format immediately
        to minimize peak memory usage, especially beneficial for large cross-layer transcoders.

        Args:
            x: Input tensor of shape (n_layers, n_pos, d_model)
            zero_first_pos: Whether to zero out position 0

        Returns:
            sparse_features: Sparse tensor of shape (n_layers, n_pos, d_transcoder)
            active_encoders: Encoder vectors for active features only
        """
        sparse_layers = []
        encoder_vectors = []

        for layer_id in range(self.n_layers):
            W_enc_layer = self._get_encoder_weights(layer_id)
            layer_features = (
                torch.einsum("bd,fd->bf", x[layer_id], W_enc_layer) + self.b_enc[layer_id]
            )

            if isinstance(self.activation_function, JumpReLU):
                mask = layer_features > self.activation_function.threshold[layer_id]
                layer_features = layer_features * mask
            else:
                layer_features = self.activation_function(layer_features)

            if zero_first_pos:
                layer_features[0] = 0

            sparse_layer = layer_features.to_sparse()
            sparse_layers.append(sparse_layer)

            _, feat_idx = sparse_layer.indices()
            encoder_vectors.append(W_enc_layer[feat_idx])

        sparse_features = torch.stack(sparse_layers).coalesce()
        active_encoders = torch.cat(encoder_vectors, dim=0)
        return sparse_features, active_encoders

    def _get_decoder_vectors(self, layer_id, feat_ids=None):
        to_read = feat_ids if feat_ids is not None else np.s_[:]

        if not self.lazy_decoder:
            return self.W_dec[layer_id][to_read].to(self.dtype)

        path = os.path.join(self.clt_path, f"W_dec_{layer_id}.safetensors")
        with safe_open(path, framework="pt", device=self.device.type) as f:
            return f.get_slice(f"W_dec_{layer_id}")[to_read].to(self.dtype)

    def select_decoder_vectors(self, features):
        if not isinstance(features, torch.sparse.Tensor):
            features = features.to_sparse()
        layer_idx, pos_idx, feat_idx = features.indices()
        activations = features.values()
        n_layers = features.shape[0]
        device = features.device

        pos_ids = []
        layer_ids = []
        feat_ids = []

        decoder_vectors = []
        encoder_mapping = []
        st = 0

        for layer_id in range(n_layers):
            current_layer = layer_idx == layer_id
            if not current_layer.any():
                continue

            current_layer_features = feat_idx[current_layer]
            unique_feats, inv = current_layer_features.unique(return_inverse=True)

            unique_decoders = self._get_decoder_vectors(layer_id, unique_feats.cpu())
            scaled_decoders = unique_decoders[inv] * activations[current_layer, None, None]
            decoder_vectors.append(scaled_decoders.reshape(-1, self.d_model))

            n_output_layers = self.n_layers - layer_id
            pos_ids.append(pos_idx[current_layer].repeat_interleave(n_output_layers))
            feat_ids.append(current_layer_features.repeat_interleave(n_output_layers))
            layer_ids.append(
                torch.arange(layer_id, self.n_layers, device=device).repeat(
                    len(current_layer_features)
                )
            )

            source_ids = torch.arange(len(current_layer_features), device=device) + st
            st += len(current_layer_features)
            encoder_mapping.append(torch.repeat_interleave(source_ids, n_output_layers))

        pos_ids = torch.cat(pos_ids, dim=0)
        layer_ids = torch.cat(layer_ids, dim=0)
        feat_ids = torch.cat(feat_ids, dim=0)
        decoder_vectors = torch.cat(decoder_vectors, dim=0)
        encoder_mapping = torch.cat(encoder_mapping, dim=0)

        return pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_mapping

    def compute_reconstruction(self, pos_ids, layer_ids, decoder_vectors):
        n_pos = pos_ids.max() + 1
        flat_idx = layer_ids * n_pos + pos_ids
        recon = torch.zeros(
            n_pos * self.n_layers,
            self.d_model,
            device=decoder_vectors.device,
            dtype=decoder_vectors.dtype,
        ).index_add_(0, flat_idx, decoder_vectors)
        return recon.reshape(self.n_layers, n_pos, self.d_model) + self.b_dec[:, None]

    def decode(self, features):
        pos_ids, layer_ids, feat_ids, decoder_vectors, _ = self.select_decoder_vectors(features)
        return self.compute_reconstruction(pos_ids, layer_ids, decoder_vectors)

    def forward(self, x):
        features = self.encode(x).to_sparse()
        return self.decode(features)

    def compute_attribution_components(self, inputs):
        """Extract active features and their encoder/decoder vectors for attribution.

        Args:
            inputs: Input tensor to encode

        Returns:
            Dict containing all components needed for AttributionContext:
                - activation_matrix: Sparse activation matrix
                - reconstruction: Reconstructed outputs
                - encoder_vecs: Concatenated encoder vectors for active features
                - decoder_vecs: Concatenated decoder vectors (scaled by activations)
                - encoder_to_decoder_map: Mapping from encoder to decoder indices
        """
        features, encoder_vectors = self.encode_sparse(inputs, zero_first_pos=True)
        pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_to_decoder_map = (
            self.select_decoder_vectors(features)
        )
        reconstruction = self.compute_reconstruction(pos_ids, layer_ids, decoder_vectors)

        return {
            "activation_matrix": features,
            "reconstruction": reconstruction,
            "encoder_vecs": encoder_vectors,
            "decoder_vecs": decoder_vectors,
            "encoder_to_decoder_map": encoder_to_decoder_map,
            "decoder_locations": torch.stack((layer_ids, pos_ids)),
        }


def load_clt(
    clt_path: str,
    feature_input_hook: str = "hook_resid_mid",
    feature_output_hook: str = "hook_mlp_out",
    scan: Optional[Union[str, List[str]]] = None,
    device: Optional[torch.device] = None,
    dtype: torch.dtype = torch.bfloat16,
    lazy_decoder: bool = True,
    lazy_encoder: bool = False,
) -> CrossLayerTranscoder:
    """Load a cross-layer transcoder from safetensors files.

    Args:
        clt_path: Path to directory containing W_enc_*.safetensors and W_dec_*.safetensors files
        dtype: Data type for loaded tensors
        lazy_decoder: Whether to load decoder weights on-demand
        lazy_encoder: Whether to load encoder weights on-demand
        feature_input_hook: Hook point where features read from
        feature_output_hook: Hook point where features write to
        scan: Optional identifier for feature visualization
        device: Device to load tensors to (defaults to auto-detected)

    Returns:
        CrossLayerTranscoder: Loaded transcoder instance
    """
    if device is None:
        device = get_default_device()

    state_dict = _load_state_dict(clt_path, lazy_decoder, lazy_encoder, device, dtype)

    # Infer dimensions from loaded tensors
    n_layers = state_dict["b_dec"].shape[0]
    d_transcoder = state_dict["b_enc"].shape[1]
    d_model = state_dict["b_dec"].shape[1]

    act_fn = "jump_relu" if "activation_function.threshold" in state_dict else "relu"

    # Create instance and load state dict
    with torch.device("meta"):
        instance = CrossLayerTranscoder(
            n_layers,
            d_transcoder,
            d_model,
            activation_function=act_fn,
            lazy_decoder=lazy_decoder,
            lazy_encoder=lazy_encoder,
            feature_input_hook=feature_input_hook,
            feature_output_hook=feature_output_hook,
            scan=scan,
            dtype=dtype,
            clt_path=clt_path,
        )

    instance.load_state_dict(state_dict, assign=True)

    return instance


def _load_state_dict(
    clt_path, lazy_decoder=True, lazy_encoder=False, device=None, dtype=torch.bfloat16
):
    if device is None:
        device = get_default_device()

    enc_files = glob.glob(os.path.join(clt_path, "W_enc_*.safetensors"))
    n_layers = len(enc_files)

    # Get dimensions from first file
    dec_file = "W_enc_0.safetensors"
    with safe_open(os.path.join(clt_path, dec_file), framework="pt", device=device.type) as f:
        d_transcoder, d_model = f.get_slice("W_enc_0").get_shape()
        has_threshold = "threshold_0" in f.keys()

    # Preallocate tensors
    b_dec = torch.zeros(n_layers, d_model, device=device, dtype=dtype)
    b_enc = torch.zeros(n_layers, d_transcoder, device=device, dtype=dtype)

    state_dict = {"b_dec": b_dec, "b_enc": b_enc}

    if has_threshold:
        state_dict["activation_function.threshold"] = torch.zeros(
            n_layers, 1, d_transcoder, device=device, dtype=dtype
        )

    # Only create W_enc if not lazy
    if not lazy_encoder:
        W_enc = torch.zeros(n_layers, d_transcoder, d_model, device=device, dtype=dtype)
        state_dict["W_enc"] = W_enc

    # Load all layers
    for i in range(n_layers):
        enc_file = f"W_enc_{i}.safetensors"
        with safe_open(os.path.join(clt_path, enc_file), framework="pt", device=device.type) as f:
            b_dec[i] = f.get_tensor(f"b_dec_{i}").to(dtype)
            b_enc[i] = f.get_tensor(f"b_enc_{i}").to(dtype)

            # Only load W_enc if not lazy
            if not lazy_encoder:
                W_enc[i] = f.get_tensor(f"W_enc_{i}").to(dtype)

            if has_threshold:
                threshold = f.get_tensor(f"threshold_{i}").to(dtype)
                state_dict["activation_function.threshold"][i] = threshold.unsqueeze(0)

        # Load W_dec for this layer if not lazy
        if not lazy_decoder:
            dec_file = os.path.join(clt_path, f"W_dec_{i}.safetensors")
            with safe_open(dec_file, framework="pt", device=device.type) as f:
                state_dict[f"W_dec.{i}"] = f.get_tensor(f"W_dec_{i}").to(dtype)

    return state_dict



---
File: /circuit_tracer/transcoder/single_layer_transcoder.py
---

import os
from typing import Dict, List, Optional, Union

import numpy as np
import torch
import torch.nn.functional as F
from huggingface_hub import hf_hub_download
from safetensors import safe_open
from torch import nn

from circuit_tracer.transcoder.activation_functions import JumpReLU
from circuit_tracer.utils import get_default_device


class SingleLayerTranscoder(nn.Module):
    """
    A per-layer transcoder (PLT) that replaces MLP computation with interpretable features.

    Per-layer transcoders decompose the output of a single MLP layer into sparsely active
    features that often correspond to interpretable concepts. Unlike cross-layer transcoders,
    each PLT operates independently on its assigned layer, which can result in longer paths
    through attribution graphs when features amplify across multiple layers.

    Attributes:
        d_model: Dimension of the transformer's residual stream
        d_transcoder: Number of learned features (typically >> d_model for superposition)
        layer_idx: Which transformer layer this transcoder replaces
        W_enc: Encoder weights mapping residual stream to feature space
        W_dec: Decoder weights mapping features back to residual stream
        b_enc: Encoder bias terms
        b_dec: Decoder bias terms (reconstruction baseline)
        W_skip: Optional skip connection weights (https://arxiv.org/abs/2501.18823)
        activation_function: Sparsity-inducing nonlinearity (e.g., ReLU, JumpReLU)
    """

    def __init__(
        self,
        d_model: int,
        d_transcoder: int,
        activation_function,
        layer_idx: int,
        skip_connection: bool = False,
        transcoder_path: Optional[str] = None,
        lazy_encoder: bool = False,
        lazy_decoder: bool = False,
        device: Optional[torch.device] = None,
        dtype: torch.dtype = torch.bfloat16,
    ):
        super().__init__()

        if device is None:
            device = get_default_device()

        self.d_model = d_model
        self.d_transcoder = d_transcoder
        self.layer_idx = layer_idx
        self.transcoder_path = transcoder_path
        self.lazy_encoder = lazy_encoder
        self.lazy_decoder = lazy_decoder

        if lazy_encoder or lazy_decoder:
            assert self.transcoder_path is not None, "Transcoder path must be set for lazy loading"

        if not lazy_encoder:
            self.W_enc = nn.Parameter(
                torch.zeros(d_transcoder, d_model, device=device, dtype=dtype)
            )

        if not lazy_decoder:
            self.W_dec = nn.Parameter(
                torch.zeros(d_transcoder, d_model, device=device, dtype=dtype)
            )

        self.b_enc = nn.Parameter(torch.zeros(d_transcoder, device=device, dtype=dtype))
        self.b_dec = nn.Parameter(torch.zeros(d_model, device=device, dtype=dtype))

        if skip_connection:
            self.W_skip = nn.Parameter(torch.zeros(d_model, d_model, device=device, dtype=dtype))
        else:
            self.W_skip = None

        self.activation_function = activation_function

    @property
    def device(self):
        """Get the device of the module's parameters."""
        return next(self.parameters()).device

    @property
    def dtype(self):
        """Get the dtype of the module's parameters."""
        return self.b_enc.dtype

    def __getattr__(self, name):
        """Dynamically load weights when accessed if lazy loading is enabled."""
        if name == "W_enc" and self.lazy_encoder and self.transcoder_path is not None:
            with safe_open(self.transcoder_path, framework="pt", device=self.device.type) as f:
                return f.get_tensor("W_enc").to(self.dtype)
        elif name == "W_dec" and self.lazy_decoder and self.transcoder_path is not None:
            with safe_open(self.transcoder_path, framework="pt", device=self.device.type) as f:
                return f.get_tensor("W_dec").to(self.dtype)

        return super().__getattr__(name)

    def _get_decoder_vectors(self, feat_ids=None):
        to_read = feat_ids if feat_ids is not None else np.s_[:]
        if not self.lazy_decoder:
            return self.W_dec[to_read].to(self.dtype)

        with safe_open(self.transcoder_path, framework="pt", device=self.device.type) as f:
            return f.get_slice("W_dec")[to_read].to(self.dtype)

    def encode(self, input_acts, apply_activation_function: bool = True):
        W_enc = self.W_enc
        pre_acts = F.linear(input_acts.to(W_enc.dtype), W_enc, self.b_enc)
        if not apply_activation_function:
            return pre_acts
        return self.activation_function(pre_acts)

    def decode(self, acts):
        W_dec = self.W_dec
        return acts @ W_dec + self.b_dec

    def compute_skip(self, input_acts):
        if self.W_skip is not None:
            return input_acts @ self.W_skip.T
        else:
            raise ValueError("Transcoder has no skip connection")

    def forward(self, input_acts):
        transcoder_acts = self.encode(input_acts)
        decoded = self.decode(transcoder_acts)
        decoded = decoded.detach()
        decoded.requires_grad = True

        if self.W_skip is not None:
            skip = self.compute_skip(input_acts)
            decoded = decoded + skip

        return decoded

    def encode_sparse(self, input_acts, zero_first_pos: bool = True):
        """Encode and return sparse activations with active encoder vectors.

        Args:
            input_acts: Input activations
            zero_first_pos: Whether to zero out position 0

        Returns:
            sparse_acts: Sparse tensor of activations
            active_encoders: Encoder vectors for active features only
        """
        W_enc = self.W_enc
        pre_acts = F.linear(input_acts.to(W_enc.dtype), W_enc, self.b_enc)
        acts = self.activation_function(pre_acts)

        if zero_first_pos:
            acts[0] = 0

        sparse_acts = acts.to_sparse()
        _, feat_idx = sparse_acts.indices()
        active_encoders = W_enc[feat_idx]

        return sparse_acts, active_encoders

    def decode_sparse(self, sparse_acts):
        """Decode sparse activations and return reconstruction with scaled decoder vectors.

        Returns:
            reconstruction: Decoded output
            scaled_decoders: Decoder vectors scaled by activation values
        """
        pos_idx, feat_idx = sparse_acts.indices()
        values = sparse_acts.values()

        # Get decoder vectors for active features only
        W_dec = self._get_decoder_vectors(feat_idx.cpu())
        scaled_decoders = W_dec * values[:, None]

        # Reconstruct using index_add
        n_pos = sparse_acts.shape[0]
        reconstruction = torch.zeros(
            n_pos, self.d_model, device=sparse_acts.device, dtype=sparse_acts.dtype
        )
        reconstruction = reconstruction.index_add_(0, pos_idx, scaled_decoders)
        reconstruction = reconstruction + self.b_dec

        return reconstruction, scaled_decoders


class TranscoderSet(nn.Module):
    """
    A collection of per-layer transcoders that enable construction of a replacement model.

    TranscoderSet manages the collection of SingleLayerTranscoders needed for this substitution,
    where each transcoder replaces the MLP computation at its corresponding layer.

    Attributes:
        transcoders: ModuleList of SingleLayerTranscoder instances, one per layer
        n_layers: Total number of layers covered
        d_transcoder: Common feature dimension across all transcoders
        feature_input_hook: Hook point where features read from (e.g., "hook_resid_mid")
        feature_output_hook: Hook point where features write to (e.g., "hook_mlp_out")
        scan: Optional identifier to identify corresponding feature visualization
        skip_connection: Whether transcoders include learned skip connections
    """

    def __init__(
        self,
        transcoders: Dict[int, SingleLayerTranscoder],
        feature_input_hook: str,
        feature_output_hook: str,
        scan: Optional[Union[str, List[str]]] = None,
    ):
        super().__init__()
        # Validate that we have continuous layers from 0 to max
        assert set(transcoders.keys()) == set(range(max(transcoders.keys()) + 1)), (
            f"Each layer should have a transcoder, but got transcoders for layers "
            f"{set(transcoders.keys())}"
        )

        self.transcoders = nn.ModuleList([transcoders[i] for i in range(len(transcoders))])
        self.n_layers = len(self.transcoders)
        self.d_transcoder = self.transcoders[0].d_transcoder

        # Verify all transcoders have the same d_transcoder
        for transcoder in self.transcoders:
            assert transcoder.d_transcoder == self.d_transcoder, (
                f"All transcoders must have the same d_transcoder, but got "
                f"{transcoder.d_transcoder} != {self.d_transcoder}"
            )

        # Store hook configuration
        self.feature_input_hook = feature_input_hook
        self.feature_output_hook = feature_output_hook
        self.scan = scan
        self.skip_connection = self.transcoders[0].W_skip is not None

    def __len__(self):
        return self.n_layers

    def __getitem__(self, idx):
        return self.transcoders[idx]

    def encode(self, input_acts):
        return torch.stack([transcoder.encode(input_acts[i]) for i, transcoder in enumerate(self.transcoders)], dim=0)

    def decode(self, acts):
        return torch.stack([transcoder.decode(acts[i]) for i, transcoder in enumerate(self.transcoders)], dim=0)

    def compute_attribution_components(
        self,
        mlp_inputs: torch.Tensor,
    ) -> Dict[str, torch.Tensor]:
        """Extract active features and their encoder/decoder vectors for attribution.

        Args:
            mlp_inputs: (n_layers, n_pos, d_model) tensor of MLP inputs

        Returns:
            Dict containing all components needed for AttributionContext:
                - activation_matrix: Sparse (n_layers, n_pos, d_transcoder) activations
                - reconstruction: (n_layers, n_pos, d_model) reconstructed outputs
                - encoder_vecs: Concatenated encoder vectors for active features
                - decoder_vecs: Concatenated decoder vectors (scaled by activations)
                - encoder_to_decoder_map: Mapping from encoder to decoder indices
        """
        device = mlp_inputs.device

        reconstruction = torch.zeros_like(mlp_inputs)
        encoder_vectors = []
        decoder_vectors = []
        sparse_acts_list = []

        for layer, transcoder in enumerate(self.transcoders):
            sparse_acts, active_encoders = transcoder.encode_sparse(
                mlp_inputs[layer], zero_first_pos=True
            )
            reconstruction[layer], active_decoders = transcoder.decode_sparse(sparse_acts)
            encoder_vectors.append(active_encoders)
            decoder_vectors.append(active_decoders)
            sparse_acts_list.append(sparse_acts)

        activation_matrix = torch.stack(sparse_acts_list).coalesce()
        encoder_to_decoder_map = torch.arange(activation_matrix._nnz(), device=device)

        return {
            "activation_matrix": activation_matrix,
            "reconstruction": reconstruction,
            "encoder_vecs": torch.cat(encoder_vectors, dim=0),
            "decoder_vecs": torch.cat(decoder_vectors, dim=0),
            "encoder_to_decoder_map": encoder_to_decoder_map,
            "decoder_locations": activation_matrix.indices()[:2],
        }

    def encode_layer(self, x, layer_id):
        return self.transcoders[layer_id].encode(x)


def load_gemma_scope_transcoder(
    path: str,
    layer: int,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = torch.float32,
    revision: Optional[str] = None,
    **kwargs,
) -> SingleLayerTranscoder:
    if device is None:
        device = get_default_device()
    if os.path.isfile(path):
        path_to_params = path
    else:
        path_to_params = hf_hub_download(
            repo_id="google/gemma-scope-2b-pt-transcoders",
            filename=path,
            revision=revision,
            force_download=False,
        )

    # load the parameters, have to rename the threshold key,
    # as ours is nested inside the activation_function module
    param_dict = np.load(path_to_params)
    param_dict = {k: torch.tensor(v, device=device, dtype=dtype) for k, v in param_dict.items()}
    param_dict["activation_function.threshold"] = param_dict["threshold"]
    param_dict["W_enc"] = param_dict["W_enc"].T.contiguous()
    del param_dict["threshold"]

    # create the transcoders
    # d_model = param_dict["W_enc"].shape[0]
    # d_transcoder = param_dict["W_enc"].shape[1]
    d_transcoder, d_model = param_dict["W_enc"].shape

    # dummy JumpReLU; will get loaded via load_state_dict
    activation_function = JumpReLU(0.0, 0.1)
    with torch.device("meta"):
        transcoder = SingleLayerTranscoder(d_model, d_transcoder, activation_function, layer)
    transcoder.load_state_dict(param_dict, assign=True)
    return transcoder


def load_relu_transcoder(
    path: str,
    layer: int,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = torch.float32,
    lazy_encoder: bool = True,
    lazy_decoder: bool = True,
):
    if device is None:
        device = get_default_device()

    param_dict = {}
    with safe_open(path, framework="pt", device=device.type) as f:
        for k in f.keys():
            if lazy_encoder and k == "W_enc":
                continue
            if lazy_decoder and k == "W_dec":
                continue
            param_dict[k] = f.get_tensor(k)

    d_sae = param_dict["b_enc"].shape[0]
    d_model = param_dict["b_dec"].shape[0]

    assert param_dict.get("log_thresholds") is None
    activation_function = F.relu
    with torch.device("meta"):
        transcoder = SingleLayerTranscoder(
            d_model,
            d_sae,
            activation_function,
            layer,
            skip_connection=param_dict.get("W_skip") is not None,
            transcoder_path=path,
            lazy_encoder=lazy_encoder,
            lazy_decoder=lazy_decoder,
        )
    transcoder.load_state_dict(param_dict, assign=True)
    return transcoder.to(dtype)


def load_transcoder_set(
    transcoder_paths: dict,
    scan: str,
    feature_input_hook: str,
    feature_output_hook: str,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = torch.float32,
    gemma_scope: bool = False,
    lazy_encoder: bool = True,
    lazy_decoder: bool = True,
) -> TranscoderSet:
    if device is None:
        device = get_default_device()
    """Loads either a preset set of transcoders, or a set specified by a file.

    Args:
        transcoder_paths: Dictionary mapping layer indices to transcoder paths
        scan: Scan identifier
        feature_input_hook: Hook point where features read from
        feature_output_hook: Hook point where features write to
        device (Optional[torch.device], optional): Device to load to
        dtype (Optional[torch.dtype], optional): Data type to use
        gemma_scope: Whether to use gemma scope loader
        lazy_encoder: Whether to use lazy loading for encoder weights
        lazy_decoder: Whether to use lazy loading for decoder weights

    Returns:
        TranscoderSet: The loaded transcoder set with all configuration
    """

    transcoders = {}
    load_fn = load_gemma_scope_transcoder if gemma_scope else load_relu_transcoder
    for layer in range(len(transcoder_paths)):
        transcoders[layer] = load_fn(
            transcoder_paths[layer],
            layer,
            device=device,
            dtype=dtype,
            lazy_encoder=lazy_encoder,
            lazy_decoder=lazy_decoder,
        )
    # we don't know how many layers the model has, but we need all layers from 0 to max covered
    assert set(transcoders.keys()) == set(range(max(transcoders.keys()) + 1)), (
        f"Each layer should have a transcoder, but got transcoders for layers "
        f"{set(transcoders.keys())}"
    )

    return TranscoderSet(
        transcoders,
        feature_input_hook=feature_input_hook,
        feature_output_hook=feature_output_hook,
        scan=scan,
    )



---
File: /circuit_tracer/utils/__init__.py
---

import torch

from circuit_tracer.utils.create_graph_files import create_graph_files


def get_default_device() -> torch.device:
    """Get the default device, preferring CUDA if available."""
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


__all__ = ["create_graph_files"]



---
File: /circuit_tracer/utils/create_graph_files.py
---

import logging
import os
import time
from typing import Union

import torch
from transformers import AutoTokenizer

from circuit_tracer.frontend.graph_models import Metadata, Model, Node, QParams
from circuit_tracer.frontend.utils import add_graph_metadata, process_token
from circuit_tracer.graph import Graph, prune_graph

logger = logging.getLogger(__name__)


def load_graph_data(file_path) -> Graph:
    """Load graph data from a PyTorch file."""
    start_time = time.time()
    graph = Graph.from_pt(file_path)
    time_ms = (time.time() - start_time) * 1000
    logger.info(f"Loading graph data: {time_ms=:.2f} ms")
    return graph


def create_nodes(graph: Graph, node_mask, tokenizer, cumulative_scores):
    """Create all nodes for the graph."""
    start_time = time.time()

    nodes = {}

    n_features = len(graph.selected_features)
    layers = graph.cfg.n_layers
    error_end_idx = n_features + graph.n_pos * layers
    token_end_idx = error_end_idx + len(graph.input_tokens)

    for node_idx in node_mask.nonzero().squeeze().tolist():
        if node_idx in range(n_features):
            layer, pos, feat_idx = graph.active_features[graph.selected_features[node_idx]].tolist()
            nodes[node_idx] = Node.feature_node(
                layer,
                pos,
                feat_idx,
                influence=cumulative_scores[node_idx],
                activation=graph.activation_values[graph.selected_features[node_idx]].item(),
            )
        elif node_idx in range(n_features, error_end_idx):
            layer, pos = divmod(node_idx - n_features, graph.n_pos)
            nodes[node_idx] = Node.error_node(layer, pos, influence=cumulative_scores[node_idx])
        elif node_idx in range(error_end_idx, token_end_idx):
            pos = node_idx - error_end_idx
            nodes[node_idx] = Node.token_node(
                pos, graph.input_tokens[pos], influence=cumulative_scores[node_idx]
            )
        elif node_idx in range(token_end_idx, len(cumulative_scores)):
            pos = node_idx - token_end_idx
            nodes[node_idx] = Node.logit_node(
                pos=graph.n_pos - 1,
                vocab_idx=graph.logit_tokens[pos],
                token=process_token(tokenizer.decode(graph.logit_tokens[pos])),
                target_logit=pos == 0,
                token_prob=graph.logit_probabilities[pos],
                num_layers=layers,
            )

    total_time = (time.time() - start_time) * 1000
    logger.info(f"Total node creation: {total_time=:.2f} ms")

    return nodes


def create_used_nodes_and_edges(graph: Graph, nodes, edge_mask):
    """Filter to only used nodes and create edges."""
    start_time = time.time()
    edges = edge_mask.numpy()
    dsts, srcs = edges.nonzero()
    weights = graph.adjacency_matrix.numpy()[dsts, srcs].tolist()

    used_edges = [
        {"source": nodes[src].node_id, "target": nodes[dst].node_id, "weight": weight}
        for src, dst, weight in zip(srcs, dsts, weights)
        if src in nodes and dst in nodes
    ]

    connected_ids = set()
    for edge in used_edges:
        connected_ids.add(edge["source"])
        connected_ids.add(edge["target"])

    nodes_before = len(nodes)
    used_nodes = [
        node
        for node in nodes.values()
        if node.node_id in connected_ids or node.feature_type in ["embedding", "logit"]
    ]
    nodes_after = len(used_nodes)
    logger.info(f"Filtered {nodes_before - nodes_after} nodes")

    time_ms = (time.time() - start_time) * 1000
    logger.info(f"Creating used nodes and edges: {time_ms=:.2f} ms")
    logger.info(f"Used nodes: {len(used_nodes)}, Used edges: {len(used_edges)}")

    return used_nodes, used_edges


def build_model(graph: Graph, used_nodes, used_edges, slug, scan, node_threshold, tokenizer):
    """Build the full model object."""
    start_time = time.time()

    if isinstance(scan, list):
        transcoder_list = scan
        transcoder_list_str = "-".join(transcoder_list)
        transcoder_list_hash = hash(transcoder_list_str)
        scan = "custom-" + str(transcoder_list_hash)
    else:
        transcoder_list = []

    meta = Metadata(
        slug=slug,
        scan=scan,
        transcoder_list=transcoder_list,
        prompt_tokens=[process_token(tokenizer.decode(t)) for t in graph.input_tokens],
        prompt=graph.input_string,
        node_threshold=node_threshold,
    )

    qparams = QParams(
        pinnedIds=[],
        supernodes=[],
        linkType="both",
        clickedId="",
        sg_pos="",
    )

    full_model = Model(
        metadata=meta,
        qParams=qparams,
        nodes=used_nodes,
        links=used_edges,
    )

    time_ms = (time.time() - start_time) * 1000
    logger.info(f"Building model: {time_ms=:.2f} ms")

    return full_model


def create_graph_files(
    graph_or_path: Union[Graph, str],
    slug: str,
    output_path,
    scan=None,
    node_threshold=0.8,
    edge_threshold=0.98,
):
    total_start_time = time.time()

    if isinstance(graph_or_path, Graph):
        graph = graph_or_path
    else:
        graph = load_graph_data(graph_or_path)

    if os.path.exists(output_path):
        assert os.path.isdir(output_path)
    else:
        os.makedirs(output_path, exist_ok=True)

    if scan is None:
        if graph.scan is None:
            raise ValueError(
                "Neither scan nor graph.scan was set. One must be set to identify "
                "which transcoders were used when creating the graph."
            )
        scan = graph.scan

    device = "cuda" if torch.cuda.is_available() else "cpu"
    graph.to(device)
    node_mask, edge_mask, cumulative_scores = (
        el.cpu() for el in prune_graph(graph, node_threshold, edge_threshold)
    )
    graph.to("cpu")

    tokenizer = AutoTokenizer.from_pretrained(graph.cfg.tokenizer_name)
    nodes = create_nodes(graph, node_mask, tokenizer, cumulative_scores)
    used_nodes, used_edges = create_used_nodes_and_edges(graph, nodes, edge_mask)
    model = build_model(graph, used_nodes, used_edges, slug, scan, node_threshold, tokenizer)

    # Write the output locally
    with open(os.path.join(output_path, f"{slug}.json"), "w") as f:
        f.write(model.model_dump_json(indent=2))
    add_graph_metadata(model.metadata.model_dump(), output_path)
    logger.info(f"Graph data written to {output_path}")

    total_time_ms = (time.time() - total_start_time) * 1000
    logger.info(f"Total execution time: {total_time_ms=:.2f} ms")



---
File: /circuit_tracer/utils/disk_offload.py
---

import atexit
import os
import tempfile
from typing import Literal

from safetensors.torch import load_file, save_file

_offload_files = set()

_TEMP_PREFIX = "safetensors-offload-YqKRr8m3-"


@atexit.register
def cleanup_offload_files():
    for f in _offload_files:
        os.remove(f)


def cleanup_all_offload_files():
    temp_dir = tempfile.gettempdir()
    n_removed = 0
    for f in os.listdir(temp_dir):
        if f.startswith(_TEMP_PREFIX):
            os.remove(os.path.join(temp_dir, f))
            n_removed += 1
    return n_removed


def disk_offload_module(module):
    org_device = next(module.parameters()).device
    with tempfile.NamedTemporaryFile(prefix=_TEMP_PREFIX, delete=False) as f:
        save_file(module.state_dict(), f.name)
        _offload_files.add(f.name)

    module.to(device="meta")

    def reload_handle(device=None):
        module.load_state_dict(load_file(f.name, device=(device or str(org_device))), assign=True)
        os.remove(f.name)
        _offload_files.remove(f.name)

    return reload_handle


def cpu_offload_module(module):
    org_device = next(module.parameters()).device
    module.to(device="cpu")

    def reload_handle():
        module.to(device=org_device)

    return reload_handle


def offload_modules(modules, offload_type: Literal["cpu", "disk"]):
    offload_fn = disk_offload_module if offload_type == "disk" else cpu_offload_module
    return [offload_fn(module) for module in modules]



---
File: /circuit_tracer/utils/hf_utils.py
---

from __future__ import annotations

import glob
import logging
import os
from typing import Dict, Iterable, NamedTuple, Optional
from urllib.parse import parse_qs, urlparse

import torch
import yaml
from huggingface_hub import get_token, hf_api, hf_hub_download, snapshot_download
from huggingface_hub.constants import HF_HUB_ENABLE_HF_TRANSFER
from huggingface_hub.utils.tqdm import tqdm as hf_tqdm
from tqdm.contrib.concurrent import thread_map

logger = logging.getLogger(__name__)


class HfUri(NamedTuple):
    """Structured representation of a HuggingFace URI."""

    repo_id: str
    file_path: Optional[str]
    revision: Optional[str]

    @classmethod
    def from_str(cls, hf_ref: str):
        if hf_ref.startswith("hf://"):
            return parse_hf_uri(hf_ref)

        parts = hf_ref.split("@", 1)
        repo_id = parts[0]
        revision = parts[1] if len(parts) > 1 else None
        return cls(repo_id, None, revision)


def load_transcoder_from_hub(
    hf_ref: str,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = torch.float32,
    lazy_encoder: bool = False,
    lazy_decoder: bool = True,
):
    """Load a transcoder from a HuggingFace URI."""

    # resolve legacy references
    if hf_ref == "gemma":
        hf_ref = "mntss/gemma-scope-transcoders"
    elif hf_ref == "llama":
        hf_ref = "mntss/transcoder-Llama-3.2-1B"

    hf_ref = HfUri.from_str(hf_ref)
    try:
        config_path = hf_hub_download(
            repo_id=hf_ref.repo_id,
            revision=hf_ref.revision,
            filename="config.yaml",
        )
    except Exception as e:
        raise FileNotFoundError(f"Could not download config.yaml from {hf_ref.repo_id}") from e

    with open(config_path, "r") as f:
        config = yaml.safe_load(f)

    config["repo_id"] = hf_ref.repo_id
    config["revision"] = hf_ref.revision
    config["scan"] = f"{hf_ref.repo_id}@{hf_ref.revision}" if hf_ref.revision else hf_ref.repo_id

    return load_transcoders(config, device, dtype, lazy_encoder, lazy_decoder), config


def load_transcoders(
    config: dict,
    device: Optional[torch.device] = None,
    dtype: Optional[torch.dtype] = torch.float32,
    lazy_encoder: bool = False,
    lazy_decoder: bool = True,
):
    """Load a transcoder from a HuggingFace URI."""

    model_kind = config["model_kind"]
    if model_kind == "transcoder_set":
        from circuit_tracer.transcoder.single_layer_transcoder import load_transcoder_set

        transcoder_paths = resolve_transcoder_paths(config)
        is_gemma_scope = "gemma-scope" in config.get("repo_id", "")

        return load_transcoder_set(
            transcoder_paths,
            scan=config["scan"],
            feature_input_hook=config["feature_input_hook"],
            feature_output_hook=config["feature_output_hook"],
            gemma_scope=is_gemma_scope,
            dtype=dtype,
            device=device,
            lazy_encoder=lazy_encoder,
            lazy_decoder=lazy_decoder,
        )
    elif model_kind == "cross_layer_transcoder":
        from circuit_tracer.transcoder.cross_layer_transcoder import load_clt

        local_path = snapshot_download(
            config["repo_id"],
            revision=config.get("revision", "main"),
            allow_patterns=["*.safetensors"],
        )

        return load_clt(
            local_path,
            scan=config["scan"],
            feature_input_hook=config["feature_input_hook"],
            feature_output_hook=config["feature_output_hook"],
            lazy_decoder=lazy_decoder,
            lazy_encoder=lazy_encoder,
            dtype=dtype,
            device=device,
        )
    else:
        raise ValueError(f"Unknown model kind: {model_kind}")


def resolve_transcoder_paths(config: dict) -> dict:
    if "transcoders" in config:
        hf_paths = [path for path in config["transcoders"] if path.startswith("hf://")]
        local_map = download_hf_uris(hf_paths)
        transcoder_paths = {
            i: local_map.get(path, path) for i, path in enumerate(config["transcoders"])
        }
    else:
        local_path = snapshot_download(
            config["repo_id"],
            revision=config.get("revision", "main"),
            allow_patterns=["layer_*.safetensors"],
        )
        layer_files = glob.glob(os.path.join(local_path, "layer_*.safetensors"))
        transcoder_paths = {
            i: os.path.join(local_path, f"layer_{i}.safetensors") for i in range(len(layer_files))
        }
    return transcoder_paths


def parse_hf_uri(uri: str) -> HfUri:
    """Parse an HF URI into repo id, file path and revision.

    Args:
        uri: String like ``hf://org/repo/file?revision=main``.

    Returns:
        ``HfUri`` with repository id, file path and optional revision.
    """
    parsed = urlparse(uri)
    if parsed.scheme != "hf":
        raise ValueError(f"Not a huggingface URI: {uri}")
    path = parsed.path.lstrip("/")
    repo_parts = path.split("/", 1)
    if len(repo_parts) != 2:
        raise ValueError(f"Invalid huggingface URI: {uri}")
    repo_id = f"{parsed.netloc}/{repo_parts[0]}"
    file_path = repo_parts[1]
    revision = parse_qs(parsed.query).get("revision", [None])[0] or None
    return HfUri(repo_id, file_path, revision)


def download_hf_uri(uri: str) -> str:
    """Download a file referenced by a HuggingFace URI and return the local path."""
    parsed = parse_hf_uri(uri)
    return hf_hub_download(
        repo_id=parsed.repo_id,
        filename=parsed.file_path,
        revision=parsed.revision,
        force_download=False,
    )


def download_hf_uris(uris: Iterable[str], max_workers: int = 8) -> Dict[str, str]:
    """Download multiple HuggingFace URIs concurrently with pre-flight auth checks.

    Args:
        uris: Iterable of HF URIs.
        max_workers: Maximum number of parallel workers.

    Returns:
        Mapping from input URI to the local file path on disk.
    """
    if not uris:
        return {}

    uri_list = list(uris)
    if not uri_list:
        return {}
    parsed_map = {uri: parse_hf_uri(uri) for uri in uri_list}

    # ---  Pre-flight Check ---
    logger.info("Performing pre-flight metadata check...")
    unique_repos = {info.repo_id for info in parsed_map.values()}
    token = get_token()

    for repo_id in unique_repos:
        if hf_api.repo_info(repo_id=repo_id, token=token).gated is not False:
            if token is None:
                raise PermissionError("Cannot access a gated repo without a hf token.")

    logger.info("Pre-flight check complete. Starting downloads...")

    def _download(uri: str) -> str:
        info = parsed_map[uri]

        return hf_hub_download(
            repo_id=info.repo_id,
            filename=info.file_path,
            revision=info.revision,
            token=token,
            force_download=False,
        )

    if HF_HUB_ENABLE_HF_TRANSFER:
        # Use a simple loop for sequential download if HF_TRANSFER is enabled
        results = [_download(uri) for uri in uri_list]
        return dict(zip(uri_list, results))

    # The thread_map will attempt all downloads in parallel. If any worker thread
    # raises an exception (like GatedRepoError from _download), thread_map
    # will propagate that first exception, failing the entire process.
    results = thread_map(
        _download,
        uri_list,
        desc=f"Fetching {len(parsed_map)} files",
        max_workers=max_workers,
        tqdm_class=hf_tqdm,
    )
    return dict(zip(uri_list, results))



---
File: /circuit_tracer/__init__.py
---

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from circuit_tracer.attribution.attribute import attribute
    from circuit_tracer.graph import Graph
    from circuit_tracer.replacement_model import ReplacementModel

__all__ = ["ReplacementModel", "Graph", "attribute"]


def __getattr__(name):
    _lazy_imports = {
        "attribute": ("circuit_tracer.attribution.attribute", "attribute"),
        "Graph": ("circuit_tracer.graph", "Graph"),
        "ReplacementModel": ("circuit_tracer.replacement_model", "ReplacementModel"),
    }

    if name in _lazy_imports:
        module_name, attr_name = _lazy_imports[name]
        module = __import__(module_name, fromlist=[attr_name])
        return getattr(module, attr_name)
    else:
        raise AttributeError(f"module '{__name__}' has no attribute '{name}'")



---
File: /circuit_tracer/__main__.py
---

import argparse
import logging
import os
import time
import warnings


def main():
    # Configure logging
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    parser = argparse.ArgumentParser(
        description="CLI for attribution, graph file creation, and server hosting.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Create subparsers
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    subparsers.required = True

    # Attribution subcommand
    attr_parser = subparsers.add_parser("attribute", help="Run attribution analysis on a prompt")

    # Arguments from attribute_batch.py
    attr_parser.add_argument(
        "-m",
        "--model",
        type=str,
        help=("Model architecture to use for attribution. Can be inferred from transcoder config."),
    )
    attr_parser.add_argument(
        "-t",
        "--transcoder_set",
        required=True,
        help=(
            "HuggingFace repository ID containing transcoders "
            "(e.g. username/repo-name, username/repo-name@revision)."
        ),
    )
    attr_parser.add_argument("-p", "--prompt", required=True, help="Input prompt text to analyze.")
    attr_parser.add_argument(
        "-o",
        "--graph_output_path",
        help=(
            "Path where to save the attribution graph (.pt file). Required if not "
            "creating graph files."
        ),
    )
    attr_parser.add_argument(
        "--dtype",
        type=str,
        choices=["float32", "bfloat16", "float16", "fp32", "bf16", "fp16"],
        default="float32",
        help="Data type for model weights (default: float32).",
    )
    attr_parser.add_argument(
        "--max_n_logits", type=int, default=10, help="Maximum number of logit nodes."
    )
    attr_parser.add_argument(
        "--desired_logit_prob",
        type=float,
        default=0.95,
        help="Cumulative probability threshold for top logits.",
    )
    attr_parser.add_argument(
        "--batch_size", type=int, default=256, help="Batch size for backward passes."
    )
    attr_parser.add_argument(
        "--offload",
        choices=["cpu", "disk", None],
        default=None,
        help="Offload model parameters to save memory.",
    )
    attr_parser.add_argument(
        "--max_feature_nodes",
        type=int,
        default=7500,
        help="Maximum number of feature nodes.",
    )
    attr_parser.add_argument("--verbose", action="store_true", help="Display progress information.")
    attr_parser.add_argument(
        "--lazy-encoder",
        action="store_true",
        help="Enable lazy loading for encoder weights to save memory.",
    )
    attr_parser.add_argument(
        "--lazy-decoder",
        action="store_true",
        default=True,
        help="Enable lazy loading for decoder weights to save memory (default: True).",
    )

    # Arguments for graph creation
    attr_parser.add_argument(
        "--slug",
        type=str,
        help=(
            "Slug for the model metadata (used for graph files). Required if creating "
            "graph files or starting server."
        ),
    )
    attr_parser.add_argument(
        "--graph_file_dir",
        type=str,
        help=(
            "Path to save the output JSON graph files, and also used as data dir for "
            "server. Required if creating graph files or starting server."
        ),
    )
    attr_parser.add_argument(
        "--node_threshold",
        type=float,
        default=0.8,
        help="Node threshold for pruning graph files.",
    )
    attr_parser.add_argument(
        "--edge_threshold",
        type=float,
        default=0.98,
        help="Edge threshold for pruning graph files.",
    )

    # Server arguments
    attr_parser.add_argument(
        "--server",
        action="store_true",
        help="Start a local server to visualize graphs after processing.",
    )
    attr_parser.add_argument("--port", type=int, default=8041, help="Port for the local server.")

    # Start-server subcommand
    server_parser = subparsers.add_parser(
        "start-server", help="Start a local server to visualize existing graphs"
    )
    server_parser.add_argument(
        "--graph_file_dir",
        type=str,
        required=True,
        help="Path to the directory containing graph JSON files.",
    )
    server_parser.add_argument("--port", type=int, default=8041, help="Port for the local server.")

    args = parser.parse_args()

    if args.command == "attribute":
        run_attribution(args, attr_parser)
    if args.command == "start-server" or args.server:
        run_server(args)


def run_attribution(args, parser):
    # Check if one of slug/graph_file_dir is provided but not the other
    if bool(args.slug) != bool(args.graph_file_dir):
        which_one = "slug" if args.slug else "graph_file_dir"
        missing_one = "graph_file_dir" if args.slug else "slug"
        warnings.warn(
            (
                f"You provided --{which_one} but not --{missing_one}. Both are required "
                "for creating graph files."
            ),
            UserWarning,
        )

    # Determine if we're creating graph files
    create_graph_files_enabled = args.slug is not None and args.graph_file_dir is not None

    # Validate arguments
    if args.server and (not args.slug or not args.graph_file_dir):
        parser.error("Both --slug and --graph_file_dir are required when using --server")

    if not create_graph_files_enabled and not args.graph_output_path:
        parser.error(
            (
                "--graph_output_path is required when not creating graph files "
                "(--slug and --graph_file_dir)"
            )
        )

    # Ensure graph output directory exists if needed
    if create_graph_files_enabled:
        os.makedirs(args.graph_file_dir, exist_ok=True)

    import torch

    dtype = args.dtype
    # Convert short dtype string to long dtype string
    dtype_mapping = {
        "fp32": "float32",
        "bf16": "bfloat16",
        "fp16": "float16",
    }
    if dtype in dtype_mapping:
        dtype = dtype_mapping[dtype]
    dtype = getattr(torch, dtype)

    # Run attribution
    logging.info(f"Generating attribution graph for model: {args.model}")
    logging.info(f"Loading model with dtype: {dtype}")
    logging.info(f'Input prompt: "{args.prompt}"')
    if args.graph_output_path:
        logging.info(f"Output will be saved to: {args.graph_output_path}")
    logging.info(
        (
            f"Including logits with cumulative probability >= {args.desired_logit_prob} "
            f"(max {args.max_n_logits})"
        )
    )
    logging.info(f"Using batch size of {args.batch_size} for backward passes")

    from circuit_tracer import ReplacementModel, attribute
    from circuit_tracer.utils.create_graph_files import create_graph_files
    from circuit_tracer.utils.hf_utils import load_transcoder_from_hub

    transcoder, config = load_transcoder_from_hub(
        args.transcoder_set,
        dtype=dtype,
        lazy_encoder=args.lazy_encoder,
        lazy_decoder=args.lazy_decoder,
    )
    args.model = args.model or config.get("model_name", None)
    if not args.model:
        parser.error("--model must be specified when not provided in transcoder config")

    model_instance = ReplacementModel.from_pretrained_and_transcoders(
        args.model, transcoder, dtype=dtype
    )

    logging.info("Running attribution...")
    graph = attribute(
        prompt=args.prompt,
        model=model_instance,
        max_n_logits=args.max_n_logits,
        desired_logit_prob=args.desired_logit_prob,
        batch_size=args.batch_size,
        verbose=args.verbose,
        offload=args.offload,
        max_feature_nodes=args.max_feature_nodes,
    )

    # Save to file if output path specified
    if args.graph_output_path:
        logging.info(f"Saving graph to {args.graph_output_path}")
        graph.to_pt(args.graph_output_path)

    # Create graph files if both slug and graph_file_dir are provided
    if create_graph_files_enabled:
        logging.info(f"Creating graph files with slug: {args.slug}")
        create_graph_files(
            graph_or_path=graph,  # Use the graph object directly
            slug=args.slug,
            scan=None,  # No scan argument needed
            output_path=args.graph_file_dir,
            node_threshold=args.node_threshold,
            edge_threshold=args.edge_threshold,
        )
        logging.info(f"Graph JSON files written to {args.graph_file_dir}")


def run_server(args):
    from circuit_tracer.frontend.local_server import serve

    logging.info(f"Starting server on port {args.port}...")
    logging.info(f"Serving data from: {os.path.abspath(args.graph_file_dir)}")
    server = serve(data_dir=args.graph_file_dir, port=args.port)
    try:
        logging.info("Press Ctrl+C to stop the server.")
        while True:
            time.sleep(1)  # Keep the main thread alive
    except KeyboardInterrupt:
        logging.info("Stopping server...")
        server.stop()


if __name__ == "__main__":
    main()



---
File: /circuit_tracer/graph.py
---

from typing import List, NamedTuple, Optional, Union

import torch
from transformer_lens import HookedTransformerConfig


class Graph:
    input_string: str
    input_tokens: torch.Tensor
    logit_tokens: torch.Tensor
    active_features: torch.Tensor
    adjacency_matrix: torch.Tensor
    selected_features: torch.Tensor
    activation_values: torch.Tensor
    logit_probabilities: torch.Tensor
    cfg: HookedTransformerConfig
    scan: Optional[Union[str, List[str]]]

    def __init__(
        self,
        input_string: str,
        input_tokens: torch.Tensor,
        active_features: torch.Tensor,
        adjacency_matrix: torch.Tensor,
        cfg: HookedTransformerConfig,
        logit_tokens: torch.Tensor,
        logit_probabilities: torch.Tensor,
        selected_features: torch.Tensor,
        activation_values: torch.Tensor,
        scan: Optional[Union[str, List[str]]] = None,
    ):
        """
        A graph object containing the adjacency matrix describing the direct effect of each
        node on each other. Nodes are either non-zero transcoder features, transcoder errors,
        tokens, or logits. They are stored in the order [active_features[0], ...,
        active_features[n-1], error[layer0][position0], error[layer0][position1], ...,
        error[layer l - 1][position t-1], tokens[0], ..., tokens[t-1], logits[top-1 logit],
        ..., logits[top-k logit]].

        Args:
            input_string (str): The input string attributed.
            input_tokens (List[str]): The input tokens attributed.
            active_features (torch.Tensor): A tensor of shape (n_active_features, 3)
                containing the indices (layer, pos, feature_idx) of the non-zero features
                of the model on the given input string.
            adjacency_matrix (torch.Tensor): The adjacency matrix. Organized as
                [active_features, error_nodes, embed_nodes, logit_nodes], where there are
                model.cfg.n_layers * len(input_tokens) error nodes, len(input_tokens) embed
                nodes, len(logit_tokens) logit nodes. The rows represent target nodes, while
                columns represent source nodes.
            cfg (HookedTransformerConfig): The cfg of the model.
            logit_tokens (List[str]): The logit tokens attributed from.
            logit_probabilities (torch.Tensor): The probabilities of each logit token, given
                the input string.
            scan (Optional[Union[str,List[str]]], optional): The identifier of the
                transcoders used in the graph. Without a scan, the graph cannot be uploaded
                (since we won't know what transcoders were used). Defaults to None
        """
        self.input_string = input_string
        self.adjacency_matrix = adjacency_matrix
        self.cfg = cfg
        self.n_pos = len(input_tokens)
        self.active_features = active_features
        self.logit_tokens = logit_tokens
        self.logit_probabilities = logit_probabilities
        self.input_tokens = input_tokens
        if scan is None:
            print("Graph loaded without scan to identify it. Uploading will not be possible.")
        self.scan = scan
        self.selected_features = selected_features
        self.activation_values = activation_values

    def to(self, device):
        """Send all relevant tensors to the device (cpu, cuda, etc.)

        Args:
            device (_type_): device to send tensors
        """
        self.adjacency_matrix = self.adjacency_matrix.to(device)
        self.active_features = self.active_features.to(device)
        self.logit_tokens = self.logit_tokens.to(device)
        self.logit_probabilities = self.logit_probabilities.to(device)

    def to_pt(self, path: str):
        """Saves the graph at the given path

        Args:
            path (str): The path where the graph will be saved. Should end in .pt
        """
        d = {
            "input_string": self.input_string,
            "adjacency_matrix": self.adjacency_matrix,
            "cfg": self.cfg,
            "active_features": self.active_features,
            "logit_tokens": self.logit_tokens,
            "logit_probabilities": self.logit_probabilities,
            "input_tokens": self.input_tokens,
            "selected_features": self.selected_features,
            "activation_values": self.activation_values,
            "scan": self.scan,
        }
        torch.save(d, path)

    @staticmethod
    def from_pt(path: str, map_location="cpu") -> "Graph":
        """Load a graph (saved using graph.to_pt) from a .pt file at the given path.

        Args:
            path (str): The path of the Graph to load
            map_location (str, optional): the device to load the graph onto.
                Defaults to 'cpu'.

        Returns:
            Graph: the Graph saved at the specified path
        """
        d = torch.load(path, weights_only=False, map_location=map_location)
        return Graph(**d)


def normalize_matrix(matrix: torch.Tensor) -> torch.Tensor:
    normalized = matrix.abs()
    return normalized / normalized.sum(dim=1, keepdim=True).clamp(min=1e-10)


def compute_influence(A: torch.Tensor, logit_weights: torch.Tensor, max_iter: int = 1000):
    # Normally we calculate total influence B using A + A^2 + ... or (I - A)^-1 - I,
    # and do logit_weights @ B
    # But it's faster / more efficient to compute logit_weights @ A + logit_weights @ A^2
    # as follows:

    current_influence = logit_weights @ A
    influence = current_influence
    iterations = 0
    while current_influence.any():
        if iterations >= max_iter:
            raise RuntimeError(
                f"Influence computation failed to converge after {iterations} iterations"
            )
        current_influence = current_influence @ A
        influence += current_influence
        iterations += 1
    return influence


def compute_node_influence(adjacency_matrix: torch.Tensor, logit_weights: torch.Tensor):
    return compute_influence(normalize_matrix(adjacency_matrix), logit_weights)


def compute_edge_influence(pruned_matrix: torch.Tensor, logit_weights: torch.Tensor):
    normalized_pruned = normalize_matrix(pruned_matrix)
    pruned_influence = compute_influence(normalized_pruned, logit_weights)
    pruned_influence += logit_weights
    edge_scores = normalized_pruned * pruned_influence[:, None]
    return edge_scores


def find_threshold(scores: torch.Tensor, threshold: float):
    # Find score threshold that keeps the desired fraction of total influence
    sorted_scores = torch.sort(scores, descending=True).values
    cumulative_score = torch.cumsum(sorted_scores, dim=0) / torch.sum(sorted_scores)
    threshold_index = torch.searchsorted(cumulative_score, threshold)
    # make sure we don't go out of bounds (only really happens at threshold=1.0)
    threshold_index = min(threshold_index, len(cumulative_score) - 1)
    return sorted_scores[threshold_index]


class PruneResult(NamedTuple):
    node_mask: torch.Tensor  # Boolean tensor indicating which nodes to keep
    edge_mask: torch.Tensor  # Boolean tensor indicating which edges to keep
    cumulative_scores: torch.Tensor  # Tensor of cumulative influence scores for each node


def prune_graph(
    graph: Graph, node_threshold: float = 0.8, edge_threshold: float = 0.98
) -> PruneResult:
    """Prunes a graph by removing nodes and edges with low influence on the output logits.

    Args:
        graph: The graph to prune
        node_threshold: Keep nodes that contribute to this fraction of total influence
        edge_threshold: Keep edges that contribute to this fraction of total influence

    Returns:
        Tuple containing:
        - node_mask: Boolean tensor indicating which nodes to keep
        - edge_mask: Boolean tensor indicating which edges to keep
        - cumulative_scores: Tensor of cumulative influence scores for each node
    """

    if node_threshold > 1.0 or node_threshold < 0.0:
        raise ValueError("node_threshold must be between 0.0 and 1.0")
    if edge_threshold > 1.0 or edge_threshold < 0.0:
        raise ValueError("edge_threshold must be between 0.0 and 1.0")

    # Extract dimensions
    n_tokens = len(graph.input_tokens)
    n_logits = len(graph.logit_tokens)
    n_features = len(graph.selected_features)

    logit_weights = torch.zeros(
        graph.adjacency_matrix.shape[0], device=graph.adjacency_matrix.device
    )
    logit_weights[-n_logits:] = graph.logit_probabilities

    # Calculate node influence and apply threshold
    node_influence = compute_node_influence(graph.adjacency_matrix, logit_weights)
    node_mask = node_influence >= find_threshold(node_influence, node_threshold)
    # Always keep tokens and logits
    node_mask[-n_logits - n_tokens :] = True

    # Create pruned matrix with selected nodes
    pruned_matrix = graph.adjacency_matrix.clone()
    pruned_matrix[~node_mask] = 0
    pruned_matrix[:, ~node_mask] = 0
    # we could also do iterative pruning here (see below)

    # Calculate edge influence and apply threshold
    edge_scores = compute_edge_influence(pruned_matrix, logit_weights)

    edge_mask = edge_scores >= find_threshold(edge_scores.flatten(), edge_threshold)

    old_node_mask = node_mask.clone()
    # Ensure feature and error nodes have outgoing edges
    node_mask[: -n_logits - n_tokens] &= edge_mask[:, : -n_logits - n_tokens].any(0)
    # Ensure feature nodes have incoming edges
    node_mask[:n_features] &= edge_mask[:n_features].any(1)

    # iteratively prune until all nodes missing incoming / outgoing edges are gone
    # (each pruning iteration potentially opens up new candidates for pruning)
    # this should not take more than n_layers + 1 iterations
    while not torch.all(node_mask == old_node_mask):
        old_node_mask[:] = node_mask
        edge_mask[~node_mask] = False
        edge_mask[:, ~node_mask] = False

        # Ensure feature and error nodes have outgoing edges
        node_mask[: -n_logits - n_tokens] &= edge_mask[:, : -n_logits - n_tokens].any(0)
        # Ensure feature nodes have incoming edges
        node_mask[:n_features] &= edge_mask[:n_features].any(1)

    # Calculate cumulative influence scores
    sorted_scores, sorted_indices = torch.sort(node_influence, descending=True)
    cumulative_scores = torch.cumsum(sorted_scores, dim=0) / torch.sum(sorted_scores)
    final_scores = torch.zeros_like(node_influence)
    final_scores[sorted_indices] = cumulative_scores

    return PruneResult(node_mask, edge_mask, final_scores)



---
File: /circuit_tracer/replacement_model.py
---

import warnings
from collections import defaultdict
from contextlib import contextmanager
from functools import partial
from typing import Callable, List, Optional, Tuple, Union

import torch
import torch.nn.functional as F
from torch import nn
from transformer_lens import HookedTransformer, HookedTransformerConfig
from transformer_lens.hook_points import HookPoint

from circuit_tracer.attribution.context import AttributionContext
from circuit_tracer.transcoder import TranscoderSet
from circuit_tracer.transcoder.cross_layer_transcoder import CrossLayerTranscoder
from circuit_tracer.utils import get_default_device
from circuit_tracer.utils.hf_utils import load_transcoder_from_hub

# Type definition for an intervention tuple (layer, position, feature_idx, value)
Intervention = Tuple[int, Union[int, slice, torch.Tensor], int, Union[int, torch.Tensor]]


class ReplacementMLP(nn.Module):
    """Wrapper for a TransformerLens MLP layer that adds in extra hooks"""

    def __init__(self, old_mlp: nn.Module):
        super().__init__()
        self.old_mlp = old_mlp
        self.hook_in = HookPoint()
        self.hook_out = HookPoint()

    def forward(self, x):
        x = self.hook_in(x)
        mlp_out = self.old_mlp(x)
        return self.hook_out(mlp_out)


class ReplacementUnembed(nn.Module):
    """Wrapper for a TransformerLens Unembed layer that adds in extra hooks"""

    def __init__(self, old_unembed: nn.Module):
        super().__init__()
        self.old_unembed = old_unembed
        self.hook_pre = HookPoint()
        self.hook_post = HookPoint()

    @property
    def W_U(self):
        return self.old_unembed.W_U

    @property
    def b_U(self):
        return self.old_unembed.b_U

    def forward(self, x):
        x = self.hook_pre(x)
        x = self.old_unembed(x)
        return self.hook_post(x)


class ReplacementModel(HookedTransformer):
    transcoders: Union[TranscoderSet, CrossLayerTranscoder]  # Support both types
    feature_input_hook: str
    feature_output_hook: str
    skip_transcoder: bool
    scan: Optional[Union[str, List[str]]]

    @classmethod
    def from_config(
        cls,
        config: HookedTransformerConfig,
        transcoders: Union[TranscoderSet, CrossLayerTranscoder],  # Accept both
        **kwargs,
    ) -> "ReplacementModel":
        """Create a ReplacementModel from a given HookedTransformerConfig and TranscoderSet

        Args:
            config (HookedTransformerConfig): the config of the HookedTransformer
            transcoders (TranscoderSet): The transcoder set with configuration

        Returns:
            ReplacementModel: The loaded ReplacementModel
        """
        model = cls(config, **kwargs)
        model._configure_replacement_model(transcoders)
        return model

    @classmethod
    def from_pretrained_and_transcoders(
        cls,
        model_name: str,
        transcoders: Union[TranscoderSet, CrossLayerTranscoder],  # Accept both
        **kwargs,
    ) -> "ReplacementModel":
        """Create a ReplacementModel from the name of HookedTransformer and TranscoderSet

        Args:
            model_name (str): the name of the pretrained HookedTransformer
            transcoders (TranscoderSet): The transcoder set with configuration

        Returns:
            ReplacementModel: The loaded ReplacementModel
        """
        model = super().from_pretrained(
            model_name,
            fold_ln=False,
            center_writing_weights=False,
            center_unembed=False,
            **kwargs,
        )

        model._configure_replacement_model(transcoders)
        return model

    @classmethod
    def from_pretrained(
        cls,
        model_name: str,
        transcoder_set: str,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = torch.float32,
        **kwargs,
    ) -> "ReplacementModel":
        """Create a ReplacementModel from model name and transcoder config

        Args:
            model_name (str): the name of the pretrained HookedTransformer
            transcoder_set (str): Either a predefined transcoder set name, or a config file

        Returns:
            ReplacementModel: The loaded ReplacementModel
        """
        if device is None:
            device = get_default_device()

        transcoders, _ = load_transcoder_from_hub(transcoder_set, device=device, dtype=dtype)

        return cls.from_pretrained_and_transcoders(
            model_name,
            transcoders,
            device=device,
            dtype=dtype,
            **kwargs,
        )

    def _configure_replacement_model(
        self, transcoder_set: Union[TranscoderSet, CrossLayerTranscoder]
    ):
        transcoder_set.to(self.cfg.device, self.cfg.dtype)

        self.transcoders = transcoder_set
        self.feature_input_hook = transcoder_set.feature_input_hook
        self.original_feature_output_hook = transcoder_set.feature_output_hook
        self.feature_output_hook = transcoder_set.feature_output_hook + ".hook_out_grad"
        self.skip_transcoder = transcoder_set.skip_connection
        self.scan = transcoder_set.scan

        for block in self.blocks:
            block.mlp = ReplacementMLP(block.mlp)

        self.unembed = ReplacementUnembed(self.unembed)

        self._configure_gradient_flow()
        self._deduplicate_attention_buffers()
        self.setup()

    def _configure_gradient_flow(self):
        if isinstance(self.transcoders, TranscoderSet):
            for layer, transcoder in enumerate(self.transcoders):
                self._configure_skip_connection(self.blocks[layer], transcoder)
        else:
            for layer in range(self.cfg.n_layers):
                self._configure_skip_connection(self.blocks[layer], self.transcoders)

        def stop_gradient(acts, hook):
            return acts.detach()

        for block in self.blocks:
            block.attn.hook_pattern.add_hook(stop_gradient, is_permanent=True)
            block.ln1.hook_scale.add_hook(stop_gradient, is_permanent=True)
            block.ln2.hook_scale.add_hook(stop_gradient, is_permanent=True)
            if hasattr(block, "ln1_post"):
                block.ln1_post.hook_scale.add_hook(stop_gradient, is_permanent=True)
            if hasattr(block, "ln2_post"):
                block.ln2_post.hook_scale.add_hook(stop_gradient, is_permanent=True)
            self.ln_final.hook_scale.add_hook(stop_gradient, is_permanent=True)

        for param in self.parameters():
            param.requires_grad = False

        def enable_gradient(acts, hook):
            acts.requires_grad = True
            return acts

        self.hook_embed.add_hook(enable_gradient, is_permanent=True)

    def _configure_skip_connection(self, block, transcoder):
        cached = {}

        def cache_activations(acts, hook):
            cached["acts"] = acts

        def add_skip_connection(acts: torch.Tensor, hook: HookPoint, grad_hook: HookPoint):
            # We add grad_hook because we need a way to hook into the gradients of the output
            # of this function. If we put the backwards hook here at hook, the grads will be 0
            # because we detached acts.
            skip_input_activation = cached.pop("acts")
            if hasattr(transcoder, "W_skip") and transcoder.W_skip is not None:
                skip = transcoder.compute_skip(skip_input_activation)
            else:
                skip = skip_input_activation * 0
            return grad_hook(skip + (acts - skip).detach())

        # add feature input hook
        output_hook_parts = self.feature_input_hook.split(".")
        subblock = block
        for part in output_hook_parts:
            subblock = getattr(subblock, part)
        subblock.add_hook(cache_activations, is_permanent=True)

        # add feature output hook and special grad hook
        output_hook_parts = self.original_feature_output_hook.split(".")
        subblock = block
        for part in output_hook_parts:
            subblock = getattr(subblock, part)
        subblock.hook_out_grad = HookPoint()
        subblock.add_hook(
            partial(add_skip_connection, grad_hook=subblock.hook_out_grad),
            is_permanent=True,
        )

    def _deduplicate_attention_buffers(self):
        """
        Share attention buffers across layers to save memory.

        TransformerLens makes separate copies of the same masks and RoPE
        embeddings for each layer - This just keeps one copy
        of each and shares it across all layers.
        """

        attn_masks = {}

        for block in self.blocks:
            attn_masks[block.attn.attn_type] = block.attn.mask
            if hasattr(block.attn, "rotary_sin"):
                attn_masks["rotary_sin"] = block.attn.rotary_sin
                attn_masks["rotary_cos"] = block.attn.rotary_cos

        for block in self.blocks:
            block.attn.mask = attn_masks[block.attn.attn_type]
            if hasattr(block.attn, "rotary_sin"):
                block.attn.rotary_sin = attn_masks["rotary_sin"]
                block.attn.rotary_cos = attn_masks["rotary_cos"]

    def _get_activation_caching_hooks(
        self,
        sparse: bool = False,
        apply_activation_function: bool = True,
        append: bool = False,
    ) -> Tuple[List[torch.Tensor], List[Tuple[str, Callable]]]:
        activation_matrix = (
            [[] for _ in range(self.cfg.n_layers)] if append else [None] * self.cfg.n_layers
        )

        def cache_activations(acts, hook, layer):
            transcoder_acts = (
                self.transcoders[layer]
                .encode(acts, apply_activation_function=apply_activation_function)
                .detach()
                .squeeze(0)
            )
            if sparse:
                transcoder_acts = transcoder_acts.to_sparse()

            if append:
                activation_matrix[layer].append(transcoder_acts)
            else:
                activation_matrix[layer] = transcoder_acts

        activation_hooks = [
            (
                f"blocks.{layer}.{self.feature_input_hook}",
                partial(cache_activations, layer=layer),
            )
            for layer in range(self.cfg.n_layers)
        ]
        return activation_matrix, activation_hooks

    def get_activations(
        self,
        inputs: Union[str, torch.Tensor],
        sparse: bool = False,
        apply_activation_function: bool = True,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get the transcoder activations for a given prompt

        Args:
            inputs (Union[str, torch.Tensor]): The inputs you want to get activations over
            sparse (bool, optional): Whether to return a sparse tensor of activations.
                Useful if d_transcoder is large. Defaults to False.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: the model logits on the inputs and the
                associated activation cache
        """

        activation_cache, activation_hooks = self._get_activation_caching_hooks(
            sparse=sparse,
            apply_activation_function=apply_activation_function,
        )
        with torch.inference_mode(), self.hooks(activation_hooks):
            logits = self(inputs)
        activation_cache = torch.stack(activation_cache)
        if sparse:
            activation_cache = activation_cache.coalesce()
        return logits, activation_cache

    @contextmanager
    def zero_softcap(self):
        current_softcap = self.cfg.output_logits_soft_cap
        try:
            self.cfg.output_logits_soft_cap = 0.0
            yield
        finally:
            self.cfg.output_logits_soft_cap = current_softcap

    def ensure_tokenized(self, prompt: Union[str, torch.Tensor, List[int]]) -> torch.Tensor:
        """Convert prompt to 1-D tensor of token ids with proper special token handling.

        This method ensures that a special token (BOS/PAD) is prepended to the input sequence.
        The first token position in transformer models typically exhibits unusually high norm
        and an excessive number of active features due to how models process the beginning of
        sequences. By prepending a special token, we ensure that actual content tokens have
        more consistent and interpretable feature activations, avoiding the artifacts present
        at position 0. This prepended token is later ignored during attribution analysis.

        Args:
            prompt: String, tensor, or list of token ids representing a single sequence

        Returns:
            1-D tensor of token ids with BOS/PAD token at the beginning

        Raises:
            TypeError: If prompt is not str, tensor, or list
            ValueError: If tensor has wrong shape (must be 1-D or 2-D with batch size 1)
        """

        if isinstance(prompt, str):
            tokens = self.tokenizer(prompt, return_tensors="pt").input_ids.squeeze(0)
        elif isinstance(prompt, torch.Tensor):
            tokens = prompt.squeeze()
        elif isinstance(prompt, list):
            tokens = torch.tensor(prompt, dtype=torch.long).squeeze()
        else:
            raise TypeError(f"Unsupported prompt type: {type(prompt)}")

        if tokens.ndim > 1:
            raise ValueError(f"Tensor must be 1-D, got shape {tokens.shape}")

        # Check if a special token is already present at the beginning
        if tokens[0] in self.tokenizer.all_special_ids:
            return tokens.to(self.cfg.device)

        # Prepend a special token to avoid artifacts at position 0
        candidate_bos_token_ids = [
            self.tokenizer.bos_token_id,
            self.tokenizer.pad_token_id,
            self.tokenizer.eos_token_id,
        ]
        candidate_bos_token_ids += self.tokenizer.all_special_ids

        dummy_bos_token_id = next(filter(None, candidate_bos_token_ids))
        if dummy_bos_token_id is None:
            warnings.warn(
                "No suitable special token found for BOS token replacement. The first token will be ignored."
            )
        else:
            tokens = torch.cat([torch.tensor([dummy_bos_token_id], device=tokens.device), tokens])

        return tokens.to(self.cfg.device)

    @torch.no_grad()
    def setup_attribution(self, inputs: Union[str, torch.Tensor]):
        """Precomputes the transcoder activations and error vectors, saving them and the
        token embeddings.

        Args:
            inputs (str): the inputs to attribute - hard coded to be a single string (no
                batching) for now
        """

        if isinstance(inputs, str):
            tokens = self.ensure_tokenized(inputs)
        else:
            tokens = inputs.squeeze()

        assert isinstance(tokens, torch.Tensor), "Tokens must be a tensor"
        assert tokens.ndim == 1, "Tokens must be a 1D tensor"

        mlp_in_cache, mlp_in_caching_hooks, _ = self.get_caching_hooks(
            lambda name: self.feature_input_hook in name
        )

        mlp_out_cache, mlp_out_caching_hooks, _ = self.get_caching_hooks(
            lambda name: self.feature_output_hook in name
        )
        logits = self.run_with_hooks(tokens, fwd_hooks=mlp_in_caching_hooks + mlp_out_caching_hooks)

        mlp_in_cache = torch.cat(list(mlp_in_cache.values()), dim=0)
        mlp_out_cache = torch.cat(list(mlp_out_cache.values()), dim=0)

        attribution_data = self.transcoders.compute_attribution_components(mlp_in_cache)

        # Compute error vectors
        error_vectors = mlp_out_cache - attribution_data["reconstruction"]

        error_vectors[:, 0] = 0
        token_vectors = self.W_E[tokens].detach()  # (n_pos, d_model)

        return AttributionContext(
            activation_matrix=attribution_data["activation_matrix"],
            logits=logits,
            error_vectors=error_vectors,
            token_vectors=token_vectors,
            decoder_vecs=attribution_data["decoder_vecs"],
            encoder_vecs=attribution_data["encoder_vecs"],
            encoder_to_decoder_map=attribution_data["encoder_to_decoder_map"],
            decoder_locations=attribution_data["decoder_locations"],
        )

    def setup_intervention_with_freeze(
        self, inputs: Union[str, torch.Tensor], direct_effects: bool = False
    ) -> List[Tuple[str, Callable]]:
        """Sets up an intervention with either frozen attention (default) or frozen
        attention, LayerNorm, and MLPs, for direct effects

        Args:
            inputs (Union[str, torch.Tensor]): The inputs to intervene on
            direct_effects (bool, optional): Whether to freeze not just attention, but also
                LayerNorm and MLPs. Defaults to False.

        Returns:
            List[Tuple[str, Callable]]: The freeze hooks needed to run the desired intervention.
        """

        if direct_effects:
            hookpoints_to_freeze = ["hook_pattern", "hook_scale", self.feature_output_hook]
            if self.skip_transcoder:
                hookpoints_to_freeze.append(self.feature_input_hook)
        else:
            hookpoints_to_freeze = ["hook_pattern"]

        freeze_cache, cache_hooks, _ = self.get_caching_hooks(
            names_filter=lambda name: any(hookpoint in name for hookpoint in hookpoints_to_freeze)
        )
        self.run_with_hooks(inputs, fwd_hooks=cache_hooks)

        def freeze_hook(activations, hook):
            cached_values = freeze_cache[hook.name]

            # if we're doing open-ended generation, the position dimensions won't match
            # so we'll just freeze the previous positions, and leave the new ones unfrozen
            if "hook_pattern" in hook.name and activations.shape[2:] != cached_values.shape[2:]:
                new_activations = activations.clone()
                new_activations[:, :, : cached_values.shape[2], : cached_values.shape[3]] = (
                    cached_values
                )
                return new_activations

            elif (
                "hook_scale" in hook.name or self.feature_output_hook in hook.name
            ) and activations.shape[1] != cached_values.shape[1]:
                new_activations = activations.clone()
                new_activations[:, : cached_values.shape[1]] = cached_values
                return new_activations

            # if other positions don't match, that's no good
            assert activations.shape == cached_values.shape, (
                f"Activations shape {activations.shape} does not match cached values"
                f" shape {cached_values.shape} at hook {hook.name}"
            )
            return cached_values

        fwd_hooks = [
            (hookpoint, freeze_hook)
            for hookpoint in freeze_cache.keys()
            if self.feature_input_hook not in hookpoint
        ]

        if not direct_effects:
            return fwd_hooks

        if self.skip_transcoder:
            skip_diffs = {}

            def diff_hook(activations, hook, layer: int):
                # The MLP hook out freeze hook sets the value of the MLP to the value it
                # had when run on the inputs normally. We subtract out the skip that
                # corresponds to such a run, and add in the skip with direct effects.
                frozen_skip = self.transcoders[layer].compute_skip(freeze_cache[hook.name])
                normal_skip = self.transcoders[layer].compute_skip(activations)

                skip_diffs[layer] = normal_skip - frozen_skip

            def add_diff_hook(activations, hook, layer: int):
                # open-ended generation case
                if activations.shape[1] != skip_diffs[layer].shape[1]:
                    new_activations = activations.clone()
                    new_activations[:, : skip_diffs[layer].shape[1]] += skip_diffs[layer]
                    return new_activations
                else:
                    return activations + skip_diffs[layer]

            fwd_hooks += [
                (f"blocks.{layer}.{self.feature_input_hook}", partial(diff_hook, layer=layer))
                for layer in range(self.cfg.n_layers)
            ]
            fwd_hooks += [
                (f"blocks.{layer}.{self.feature_output_hook}", partial(add_diff_hook, layer=layer))
                for layer in range(self.cfg.n_layers)
            ]
        return fwd_hooks

    def _get_feature_intervention_hooks(
        self,
        inputs: Union[str, torch.Tensor],
        interventions: List[Intervention],
        direct_effects: bool = False,
        freeze_attention: bool = True,
        apply_activation_function: bool = True,
        sparse: bool = False,
        append: bool = False,
    ):
        """Given the input, and a dictionary of features to intervene on, performs the
        intervention, allowing all effects to propagate (optionally allowing its effects to
        propagate through transcoders)

        Args:
            input (_type_): the input prompt to intervene on
            intervention_dict (List[Tuple[int, Union[int, slice, torch.Tensor]], int,
                Union[int, torch.Tensor]]): A list of interventions to perform, formatted as
                a list of (layer, position, feature_idx, value)
            direct_effects (bool): whether to freeze all MLPs/transcoders / attn patterns /
                layernorm denominators
            apply_activation_function (bool): whether to apply the activation function when
                recording the activations to be returned. This is useful to set to False for
                testing purposes, as attribution predicts the change in pre-activation
                feature values.
            sparse (bool): whether to sparsify the activations in the returned cache. Setting
                this to True will take up less memory, at the expense of slower interventions.
            append (bool): whether to append onto the existing logit / activation cache if the
                hooks are run multiple times. Default to False (overwrite instead)
        """

        interventions_by_layer = defaultdict(list)
        for layer, pos, feature_idx, value in interventions:
            interventions_by_layer[layer].append((pos, feature_idx, value))

        # This activation cache will fill up during our forward intervention pass
        activation_cache, activation_hooks = self._get_activation_caching_hooks(
            apply_activation_function=apply_activation_function, sparse=sparse, append=append
        )

        def intervention_hook(activations, hook, layer: int, layer_interventions):
            transcoder_activations = (
                activation_cache[layer][-1] if append else activation_cache[layer]
            )
            if transcoder_activations.is_sparse:
                transcoder_activations = transcoder_activations.to_dense()

            if not apply_activation_function:
                transcoder_activations = (
                    self.transcoders[layer]
                    .activation_function(transcoder_activations.unsqueeze(0))
                    .squeeze(0)
                )

            transcoder_activations = transcoder_activations.clone()

            transcoder_output = self.transcoders[layer].decode(transcoder_activations)
            for pos, feature_idx, value in layer_interventions:
                transcoder_activations[pos, feature_idx] = value
            new_transcoder_output = self.transcoders[layer].decode(transcoder_activations)
            steering_vector = new_transcoder_output - transcoder_output
            return activations + steering_vector

        intervention_hooks = [
            (
                f"blocks.{layer}.{self.feature_output_hook}",
                partial(intervention_hook, layer=layer, layer_interventions=layer_interventions),
            )
            for layer, layer_interventions in interventions_by_layer.items()
        ]

        all_hooks = (
            self.setup_intervention_with_freeze(inputs, direct_effects=direct_effects)
            if freeze_attention or direct_effects
            else []
        )
        all_hooks += activation_hooks + intervention_hooks

        cached_logits = [] if append else [None]

        def logit_cache_hook(activations, hook):
            # we need to manually apply the softcap (if used by the model), as it comes post-hook
            if self.cfg.output_logits_soft_cap > 0.0:
                logits = self.cfg.output_logits_soft_cap * F.tanh(
                    activations / self.cfg.output_logits_soft_cap
                )
            else:
                logits = activations.clone()
            if append:
                cached_logits.append(logits)
            else:
                cached_logits[0] = logits

        all_hooks.append(("unembed.hook_post", logit_cache_hook))

        return all_hooks, cached_logits, activation_cache

    @torch.no_grad
    def feature_intervention(
        self,
        inputs: Union[str, torch.Tensor],
        interventions: List[Intervention],
        direct_effects: bool = False,
        freeze_attention: bool = True,
        apply_activation_function: bool = True,
        sparse: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Given the input, and a dictionary of features to intervene on, performs the
        intervention, and returns the logits and feature activations. If direct_effects is
        True, attention patterns will be frozen, along with MLPs and LayerNorms. If it is
        False, the effects of the intervention will propagate through transcoders /
        LayerNorms

        Args:
            input (_type_): the input prompt to intervene on
            interventions (List[Tuple[int, Union[int, slice, torch.Tensor]], int,
                Union[int, torch.Tensor]]): A list of interventions to perform, formatted as
                a list of (layer, position, feature_idx, value)
            direct_effects (bool): whether to freeze all MLPs/transcoders / attn patterns /
                layernorm denominators
            freeze_attention (bool): whether to freeze all attention patterns.
            apply_activation_function (bool): whether to apply the activation function when
                recording the activations to be returned. This is useful to set to False for
                testing purposes, as attribution predicts the change in pre-activation
                feature values.
            sparse (bool): whether to sparsify the activations in the returned cache. Setting
                this to True will take up less memory, at the expense of slower interventions.
        """

        hooks, _, activation_cache = self._get_feature_intervention_hooks(
            inputs,
            interventions,
            direct_effects=direct_effects,
            freeze_attention=freeze_attention,
            apply_activation_function=apply_activation_function,
            sparse=sparse,
        )

        with self.hooks(hooks):
            logits = self(inputs)

        activation_cache = torch.stack(activation_cache)

        return logits, activation_cache

    def _convert_open_ended_interventions(
        self,
        interventions: List[Intervention],
    ) -> List[Intervention]:
        """Convert open-ended interventions into position-0 equivalents.

        An intervention is *open-ended* if its position component is a ``slice`` whose
        ``stop`` attribute is ``None`` (e.g. ``slice(1, None)``). Such interventions will
        also apply to tokens generated in an open-ended generation loop. In such cases,
        when use_past_kv_cache=True, the model only runs the most recent token
        (and there is thus only 1 position).
        """
        converted = []
        for layer, pos, feature_idx, value in interventions:
            if isinstance(pos, slice) and pos.stop is None:
                converted.append((layer, 0, feature_idx, value))
        return converted

    @torch.no_grad
    def feature_intervention_generate(
        self,
        inputs: Union[str, torch.Tensor],
        interventions: List[Intervention],
        direct_effects: bool = False,
        freeze_attention: bool = True,
        apply_activation_function: bool = True,
        sparse: bool = False,
        **kwargs,
    ) -> Tuple[str, torch.Tensor, torch.Tensor]:
        """Given the input, and a dictionary of features to intervene on, performs the
        intervention, and generates a continuation, along with the logits and activations at each generation position.
        This function accepts all kwargs valid for HookedTransformer.generate(). Note that direct_effects
        and freeze_attention apply only to the first token generated.

        Note that if kv_cache is True (default), generation will be faster, as the model will cache the KVs, and only
        process the one new token per step; if it is False, the model will generate by doing a full forward pass across
        all tokens. Note that due to numerical precision issues, you are only guaranteed that the logits / activations of
        model.feature_intervention_generate(s, ...) are equivalent to model.feature_intervention(s, ...) if kv_cache is False.

        Args:
            input (_type_): the input prompt to intervene on
            interventions (List[Tuple[int, Union[int, slice, torch.Tensor]], int,
                Union[int, torch.Tensor]]): A list of interventions to perform, formatted as
                a list of (layer, position, feature_idx, value)
            direct_effects (bool): whether to freeze all MLPs/transcoders / attn patterns /
                layernorm denominators. This will only apply to the very first token generated.
            freeze_attention (bool): whether to freeze all attention patterns.
            apply_activation_function (bool): whether to apply the activation function when
                recording the activations to be returned. This is useful to set to False for
                testing purposes, as attribution predicts the change in pre-activation
                feature values.
            sparse (bool): whether to sparsify the activations in the returned cache. Setting
                this to True will take up less memory, at the expense of slower interventions.
        """

        feature_intervention_hook_output = self._get_feature_intervention_hooks(
            inputs,
            interventions,
            direct_effects=direct_effects,
            freeze_attention=freeze_attention,
            apply_activation_function=apply_activation_function,
            sparse=sparse,
        )

        hooks, logit_cache, activation_cache = feature_intervention_hook_output

        if kwargs.get("use_past_kv_cache", True):
            # Next, convert any open-ended interventions so they target position `0` (the
            # only token present during the incremental forward passes performed by
            # `generate`) and build the corresponding hooks.
            open_ended_interventions = self._convert_open_ended_interventions(interventions)

            # get new hooks that will target pos 0 / append logits / activations to the cache (not overwrite)
            open_ended_hooks, open_ended_logits, open_ended_activations = (
                self._get_feature_intervention_hooks(
                    inputs,
                    open_ended_interventions,
                    direct_effects=False,
                    freeze_attention=False,
                    apply_activation_function=apply_activation_function,
                    sparse=sparse,
                    append=True,
                )
            )

            # at the end of the model, clear original hooks and add open-ended hooks
            def clear_and_add_hooks(activations, hook):
                self.reset_hooks()
                for open_ended_name, open_ended_hook in open_ended_hooks:
                    self.add_hook(open_ended_name, open_ended_hook)

            for name, hook in hooks:
                self.add_hook(name, hook)

            self.add_hook("unembed.hook_post", clear_and_add_hooks)

            generation = self.generate(inputs, **kwargs)
            self.reset_hooks()

            logits = torch.cat((logit_cache[0], *open_ended_logits), dim=1)
            open_ended_activations = torch.stack(
                [torch.cat(acts, dim=0) for acts in open_ended_activations]
            )
            activation_cache = torch.stack(activation_cache)
            activations = torch.cat((activation_cache, open_ended_activations), dim=1)
            if sparse:
                activations = activations.coalesce()

            return generation, logits, activations
        else:
            # we can do more or less normal generation
            with self.hooks(hooks):
                generation = self.generate(inputs, **kwargs)

            activation_cache = torch.stack(activation_cache)
            if sparse:
                activation_cache = activation_cache.coalesce()

            return generation, logit_cache[0], activation_cache

    def __del__(self):
        # Prevent memory leaks
        self.reset_hooks(including_permanent=True)



---
File: /demos/graph_visualization.py
---

#%%
from collections import namedtuple
from typing import List, Optional, Tuple, Dict
import random
import string
import math
import html

import torch
from IPython.display import SVG


Feature = namedtuple('Feature', ['layer', 'pos', 'feature_idx'])

class InterventionGraph:
    prompt: str
    ordered_nodes: List['Supernode']
    nodes: Dict[str, 'Supernode']

    def __init__(self, ordered_nodes: List['Supernode'], prompt: str):
        self.ordered_nodes = ordered_nodes
        self.prompt = prompt
        self.nodes = {}

    def initialize_node(self, node, activations):
        self.nodes[node.name] = node
        if node.features:
            node.default_activations = torch.tensor([activations[feature] for feature in node.features])
        else:
            node.default_activations = None

    def set_node_activation_fractions(self, current_activations):
        for node in self.nodes.values():
            if node.features:
                current_node_activation = torch.tensor([current_activations[feature] for feature in node.features]) 
                node.activation = (current_node_activation / node.default_activations).mean().item()
            else:
                node.activation = None
            node.intervention = None
            node.replacement_node = None

class Supernode:
    name: str
    activation: float|None
    default_activations: torch.Tensor|None
    children: List['Supernode']
    intervention: None
    replacement_node: Optional['Supernode']

    def __init__(self, name: str, features: List[Feature], children: List['Supernode'] = [], 
                 intervention: Optional[str] = None, replacement_node: Optional['Supernode'] = None):
        self.name = name
        self.features = features
        self.activation = None
        self.default_activations = None
        self.children = children
        self.intervention = intervention
        self.replacement_node = replacement_node

    def __repr__(self):
        return f"Node(name={self.name}, activation={self.activation}, children={self.children}, intervention={self.intervention}, replacement_node={self.replacement_node})"


def calculate_node_positions(nodes: List[List['Supernode']]):
    """Calculate positions for all nodes including replacements"""
    container_width = 600
    container_height = 250
    node_width = 100
    node_height = 35
    
    node_data = {}
    
    # First, position the base nodes from the layout
    for row_index in range(len(nodes)):
        row = nodes[row_index]
        row_y = container_height - (row_index * (container_height / (len(nodes) + 0.5)))
        
        for col_index in range(len(row)):
            node = row[col_index]
            row_width = len(row) * node_width + (len(row) - 1) * 50
            start_x = (container_width - row_width) / 2
            node_x = start_x + col_index * (node_width + 50)
            
            node_data[node.name] = {
                'x': node_x,
                'y': row_y,
                'node': node
            }
    
    # Then, position replacement nodes directly above their original nodes
    all_nodes = set()
    for layer in nodes:
        for node in layer:
            all_nodes.add(node)
            if node.replacement_node:
                all_nodes.add(node.replacement_node)
    
    for node in all_nodes:
        if node.replacement_node and node.replacement_node.name not in node_data:
            original_pos = node_data.get(node.name)
            if original_pos:
                node_data[node.replacement_node.name] = {
                    'x': original_pos['x'] + 30,
                    'y': original_pos['y'] - 35,
                    'node': node.replacement_node
                }
    
    return node_data


def get_node_center(node_data, node_name):
    """Get center coordinates of a node"""
    node = node_data.get(node_name)
    if not node:
        return {'x': 0, 'y': 0}
    return {
        'x': node['x'] + 50,  # Center of node (100px wide)
        'y': node['y'] + 17.5  # Center of node (35px tall)
    }


def create_connection_svg(node_data, connections):
    """Generate SVG elements for all connections"""
    svg_parts = []
    
    for conn in connections:
        from_center = get_node_center(node_data, conn['from'])
        to_center = get_node_center(node_data, conn['to'])
        
        if from_center['x'] == 0 or to_center['x'] == 0:
            continue  # Skip if node doesn't exist
        
        # Line color and width
        if conn.get('replacement'):
            stroke_color = "#D2691E"
            stroke_width = "4"
        else:
            stroke_color = "#8B4513"
            stroke_width = "3"
        
        # Create connection line
        svg_parts.append(f'<line x1="{from_center["x"]}" y1="{from_center["y"]}" '
                        f'x2="{to_center["x"]}" y2="{to_center["y"]}" '
                        f'stroke="{stroke_color}" stroke-width="{stroke_width}"/>')
        
        # Create arrow at the end of the line
        dx = to_center['x'] - from_center['x']
        dy = to_center['y'] - from_center['y']
        length = math.sqrt(dx * dx + dy * dy)
        
        if length > 0:
            # Normalize direction vector
            dx_norm = dx / length
            dy_norm = dy / length
            
            # Arrow points
            arrow_size = 8
            arrow_tip_x = to_center['x']
            arrow_tip_y = to_center['y']
            
            # Calculate arrow base points
            base_x = arrow_tip_x - arrow_size * dx_norm
            base_y = arrow_tip_y - arrow_size * dy_norm
            
            # Perpendicular vector for arrow width
            perp_x = -dy_norm * (arrow_size / 2)
            perp_y = dx_norm * (arrow_size / 2)
            
            left_x = base_x + perp_x
            left_y = base_y + perp_y
            right_x = base_x - perp_x
            right_y = base_y - perp_y
            
            svg_parts.append(f'<polygon points="{arrow_tip_x},{arrow_tip_y} {left_x},{left_y} {right_x},{right_y}" '
                           f'fill="{stroke_color}"/>')
    
    return '\n'.join(svg_parts)


def create_nodes_svg(node_data):
    """Generate SVG elements for all nodes"""
    svg_parts = []
    
    # Collect all replacement nodes
    replacement_nodes = set()
    for data in node_data.values():
        node = data['node']
        if node.replacement_node:
            replacement_nodes.add(node.replacement_node.name)
    
    for name, data in node_data.items():
        node = data['node']
        x = data['x']
        y = data['y']
        
        # Determine node colors and styles
        is_low_activation = node.activation is not None and node.activation <= 0.25
        has_negative_intervention = node.intervention and '-' in node.intervention
        is_replacement = name in replacement_nodes
        
        if is_low_activation or has_negative_intervention:
            fill_color = "#f0f0f0"
            text_color = "#bbb"
            stroke_color = "#ddd"
        elif is_replacement:
            fill_color = "#FFF8DC"
            text_color = "#333"
            stroke_color = "#D2691E"
        else:
            fill_color = "#e8e8e8"
            text_color = "#333"
            stroke_color = "#999"
        
        # Node rectangle
        svg_parts.append(f'<rect x="{x}" y="{y}" width="100" height="35" '
                        f'fill="{fill_color}" stroke="{stroke_color}" stroke-width="2" rx="8"/>')
        
        # Node text
        text_x = x + 50  # Center horizontally
        text_y = y + 22  # Center vertically (approximate)
        escaped_name = html.escape(name)
        svg_parts.append(f'<text x="{text_x}" y="{text_y}" text-anchor="middle" '
                        f'fill="{text_color}" font-family="Arial, sans-serif" font-size="12" font-weight="bold">{escaped_name}</text>')
        
        # Add activation label if exists
        if node.activation is not None:
            activation_pct = round(node.activation * 100)
            label_x = x - 15
            label_y = y - 5
            
            # Background for activation label
            svg_parts.append(f'<rect x="{label_x}" y="{label_y}" width="30" height="16" '
                           f'fill="white" stroke="#ccc" stroke-width="1" rx="4"/>')
            
            # Activation text
            svg_parts.append(f'<text x="{label_x + 15}" y="{label_y + 12}" text-anchor="middle" '
                           f'fill="#8B4513" font-family="Arial, sans-serif" font-size="10" font-weight="bold">{activation_pct}%</text>')
        
        # Add intervention if exists
        if node.intervention:
            intervention_x = x - 20
            intervention_y = y - 5
            
            # Estimate text width for background
            text_width = len(node.intervention) * 8 + 10
            escaped_intervention = html.escape(node.intervention)
            
            # Background for intervention
            svg_parts.append(f'<rect x="{intervention_x}" y="{intervention_y}" width="{text_width}" height="16" '
                           f'fill="#D2691E" stroke="none" rx="12"/>')
            
            # Intervention text
            svg_parts.append(f'<text x="{intervention_x + text_width/2}" y="{intervention_y + 12}" text-anchor="middle" '
                           f'fill="white" font-family="Arial, sans-serif" font-size="10" font-weight="bold">{escaped_intervention}</text>')
    
    return '\n'.join(svg_parts)


def build_connections_data(nodes: List[List['Supernode']]):
    """Build connection data from node relationships"""
    connections = []
    
    # Collect all unique nodes
    all_nodes = set()
    
    def add_node_and_related(node):
        all_nodes.add(node)
        if node.replacement_node:
            add_node_and_related(node.replacement_node)
        for child in node.children:
            add_node_and_related(child)
    
    for layer in nodes:
        for node in layer:
            add_node_and_related(node)
    
    # First, identify which nodes are replacement nodes
    replacement_nodes = set()
    for node in all_nodes:
        if node.replacement_node:
            replacement_nodes.add(node.replacement_node.name)
    
    # Add all connections from nodes to their children
    for node in all_nodes:
        for child in node.children:
            # Skip connections where the 'from' node has a replacement and this isn't a replacement connection
            if node.replacement_node:
                continue  # Skip original connections when replacement exists
            
            # A connection is a replacement if the source node IS a replacement node
            is_replacement = node.name in replacement_nodes
            
            connection = {
                'from': node.name,
                'to': child.name
            }
            if is_replacement:
                connection['replacement'] = True
            
            connections.append(connection)
    
    return connections


def wrap_text_for_svg(text, max_width=80):
    """Simple text wrapping for SVG - split into lines that fit within max_width characters"""
    if len(text) <= max_width:
        return [text]
    
    words = text.split()
    lines = []
    current_line = ""
    
    for word in words:
        if len(current_line + " " + word) <= max_width:
            current_line = current_line + " " + word if current_line else word
        else:
            if current_line:
                lines.append(current_line)
            current_line = word
    
    if current_line:
        lines.append(current_line)
    
    return lines


def create_graph_visualization(intervention_graph: InterventionGraph, top_outputs: List[Tuple[str, float]]):
    """
    Creates an SVG-based graph visualization that renders properly on GitHub and other platforms.
    """
    
    nodes = intervention_graph.ordered_nodes
    prompt = intervention_graph.prompt
    
    # Calculate all positions
    node_data = calculate_node_positions(nodes)
    
    # Build connection data
    connections = build_connections_data(nodes)
    
    # Generate SVG components
    connections_svg = create_connection_svg(node_data, connections)
    nodes_svg = create_nodes_svg(node_data)
    
    # Create output items as SVG text
    output_y_start = 350
    output_items_svg = []
    current_x = 40  # Align with header instead of 20
    
    for i, (text, percentage) in enumerate(top_outputs):
        if i >= 6:  # Limit to 6 items to fit nicely
            break
            
        display_text = text if text else "(empty)"
        escaped_display_text = html.escape(display_text)
        percentage_text = f"{round(percentage * 100)}%"
        
        # Background rectangle for output item
        item_width = len(display_text) * 8 + len(percentage_text) * 6 + 20
        output_items_svg.append(f'<rect x="{current_x}" y="{output_y_start}" width="{item_width}" height="20" '
                               f'fill="#e8e8e8" stroke="none" rx="6"/>')
        
        # Output text
        output_items_svg.append(f'<text x="{current_x + 5}" y="{output_y_start + 14}" '
                               f'fill="#333" font-family="Arial, sans-serif" font-size="11" font-weight="bold">'
                               f'{escaped_display_text} <tspan fill="#555" font-size="10">{percentage_text}</tspan></text>')
        
        current_x += item_width + 10
    
    output_items_svg_str = '\n'.join(output_items_svg)
    
    # Escape the prompt text for XML and wrap it
    escaped_prompt = html.escape(prompt)
    prompt_lines = wrap_text_for_svg(escaped_prompt, max_width=80)
    
    # Create prompt text lines as SVG
    prompt_text_svg = []
    for i, line in enumerate(prompt_lines):
        y_offset = 325 + (i * 15)  # 15px line spacing
        prompt_text_svg.append(f'<text x="40" y="{y_offset}" fill="#333" font-family="Arial, sans-serif" font-size="12">{line}</text>')
    
    prompt_text_svg_str = '\n'.join(prompt_text_svg)
    
    # Create the complete SVG
    svg_content = f'''<svg width="700" height="400" xmlns="http://www.w3.org/2000/svg">
    <!-- Background -->
    <rect width="700" height="400" fill="#f5f5f5"/>
    <rect x="20" y="20" width="660" height="360" fill="white" stroke="none" rx="12"/>
    
    <!-- Title -->
    <text x="40" y="45" fill="#666" font-family="Arial, sans-serif" font-size="14" font-weight="bold" 
          text-transform="uppercase" letter-spacing="1px">Graph &amp; Interventions</text>
    
    <!-- Graph area (moved up significantly) -->
    <g transform="translate(50, 0)">
        {connections_svg}
        {nodes_svg}
    </g>
    
    <!-- Prompt section -->
    <line x1="40" y1="290" x2="660" y2="290" stroke="#ddd" stroke-width="1"/>
    <text x="40" y="310" fill="#666" font-family="Arial, sans-serif" font-size="12" font-weight="bold" 
          text-transform="uppercase" letter-spacing="0.5px">Prompt</text>
    
    <!-- Prompt text (GitHub-compatible) -->
    {prompt_text_svg_str}
    
    <!-- Top outputs section -->
    <text x="40" y="350" fill="#666" font-family="Arial, sans-serif" font-size="10" font-weight="bold" 
          text-transform="uppercase" letter-spacing="0.5px">Top Outputs</text>
    
    <!-- Output items -->
    <g transform="translate(0, 5)">
        {output_items_svg_str}
    </g>
</svg>'''
    
    return SVG(svg_content)

#%%
# if __name__ == '__main__':
#     say_austin_node = Node('Say Austin', activation=0.18)
#     texas_node = Node('Texas', activation=0.91, children=[say_austin_node])
#     say_capital_node = Node('Say a capital', activation=None, intervention='-2x', children=[say_austin_node])
#     dallas_node = Node('Dallas', activation=1.0, children=[texas_node])
#     state_node = Node('State', activation=1.0, children=[say_capital_node, texas_node])
#     capital_node = Node('capital', activation=1.0, children=[say_capital_node])

#     old_nodes = [[capital_node, state_node, dallas_node],[say_capital_node, texas_node], [say_austin_node]]

#     prompt = "Fact: the capital of the state containing Dallas is"
#     top_outputs = [("Texas", 0.76), ("located", 0.04), ("", 0.04), ("Houston", 0.03), ("Austin", 0.01), ("a", 0.01)]

#     create_graph_visualization(old_nodes, prompt, top_outputs)

#     say_sacramento_node = Node('Say Sacramento', activation=None)
#     say_austin_node = Node('Say Austin', activation=0.0, replacement_node=say_sacramento_node)
#     california_node = Node('California', activation=None, children=[say_sacramento_node], intervention='+2x')
#     texas_node = Node('Texas', activation=None, children=[say_austin_node], intervention='-2x', replacement_node=california_node)
#     say_capital_node = Node('Say a capital', activation=0.91, children=[say_austin_node])
#     dallas_node = Node('Dallas', activation=1.0, children=[texas_node])
#     state_node = Node('State', activation=1.0, children=[say_capital_node, texas_node])
#     capital_node = Node('capital', activation=1.0, children=[say_capital_node])

#     prompt = "Fact: the capital of the state containing Dallas is"
#     top_outputs = [("Sacramento", 0.97), ("", 0.007), ("not", 0.004), ("the", 0.003), ("⏎", 0.003), ("()", 0.002)]

#     nodes = [[capital_node, state_node, dallas_node],[say_capital_node, texas_node], [say_austin_node]]

#     create_graph_visualization(nodes, prompt, top_outputs)

# # %%



---
File: /demos/utils.py
---

import html
import json
import urllib.parse
from collections import namedtuple
from typing import Dict, List, Tuple

import torch 
from IPython.display import HTML, display

Feature = namedtuple("Feature", ["layer", "pos", "feature_idx"])

def get_topk(logits:torch.Tensor, tokenizer, k:int=5):
    probs = torch.softmax(logits.squeeze()[-1], dim=-1)
    topk = torch.topk(probs, k)
    return [(tokenizer.decode([topk.indices[i]]), topk.values[i].item()) for i in range(k)]

# Now let's create a version that's more adaptive to dark/light mode
def display_topk_token_predictions(sentence, original_logits, new_logits, tokenizer, k:int=5):
    """
    Version that tries to be more adaptive to both dark and light modes
    using higher contrast elements and CSS variables where possible
    """

    original_tokens = get_topk(original_logits, tokenizer, k)
    new_tokens = get_topk(new_logits, tokenizer, k)
    
    # This version uses a technique that will work better in dark mode
    # by using a combination of background colors and border styling
    html = f"""
    <style>
    .token-viz {{
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
        margin-bottom: 10px;
        max-width: 700px;
    }}
    .token-viz .header {{
        font-weight: bold;
        font-size: 14px;
        margin-bottom: 3px;
        padding: 4px 6px;
        border-radius: 3px;
        color: white;
        display: inline-block;
    }}
    .token-viz .sentence {{
        background-color: rgba(200, 200, 200, 0.2);
        padding: 4px 6px;
        border-radius: 3px;
        border: 1px solid rgba(100, 100, 100, 0.5);
        font-family: monospace;
        margin-bottom: 8px;
        font-weight: 500;
        font-size: 14px;
    }}
    .token-viz table {{
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 8px;
        font-size: 13px;
        table-layout: fixed;
    }}
    .token-viz th {{
        text-align: left;
        padding: 4px 6px;
        font-weight: bold;
        border: 1px solid rgba(150, 150, 150, 0.5);
        background-color: rgba(200, 200, 200, 0.3);
    }}
    .token-viz td {{
        padding: 3px 6px;
        border: 1px solid rgba(150, 150, 150, 0.5);
        font-weight: 500;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
    }}
    .token-viz .token-col {{
        width: 20%;
    }}
    .token-viz .prob-col {{
        width: 15%;
    }}
    .token-viz .dist-col {{
        width: 65%;
    }}
    .token-viz .monospace {{
        font-family: monospace;
    }}
    .token-viz .bar-container {{
        display: flex;
        align-items: center;
    }}
    .token-viz .bar {{
        height: 12px;
        min-width: 2px;
    }}
    .token-viz .bar-text {{
        margin-left: 6px;
        font-weight: 500;
        font-size: 12px;
    }}
    .token-viz .even-row {{
        background-color: rgba(240, 240, 240, 0.1);
    }}
    .token-viz .odd-row {{
        background-color: rgba(255, 255, 255, 0.1);
    }}
    </style>
    
    <div class="token-viz">
        <div class="header" style="background-color: #555555;">Input Sentence:</div>
        <div class="sentence">{sentence}</div>
        
        <div>
            <div class="header" style="background-color: #2471A3;">Original Top {k} Tokens</div>
            <table>
                <thead>
                    <tr>
                        <th class="token-col">Token</th>
                        <th class="prob-col" style="text-align: right;">Probability</th>
                        <th class="dist-col">Distribution</th>
                    </tr>
                </thead>
                <tbody>
    """
    
    # Calculate max probability for scaling
    max_prob = max(
        max([prob for _, prob in original_tokens]),
        max([prob for _, prob in new_tokens])
    )
    
    # Add rows for original tokens
    for i, (token, prob) in enumerate(original_tokens):
        bar_width = int(prob / max_prob * 100)
        row_class = "even-row" if i % 2 == 0 else "odd-row"
        html += f"""
                    <tr class="{row_class}">
                        <td class="monospace token-col" title="{token}">{token}</td>
                        <td class="prob-col" style="text-align: right;">{prob:.3f}</td>
                        <td class="dist-col">
                            <div class="bar-container">
                                <div class="bar" style="background-color: #2471A3; width: {bar_width}%;"></div>
                                <span class="bar-text">{prob*100:.1f}%</span>
                            </div>
                        </td>
                    </tr>
        """
    
    # Add new tokens table
    html += f"""
                </tbody>
            </table>
            
            <div class="header" style="background-color: #27AE60;">New Top {k} Tokens</div>
            <table>
                <thead>
                    <tr>
                        <th class="token-col">Token</th>
                        <th class="prob-col" style="text-align: right;">Probability</th>
                        <th class="dist-col">Distribution</th>
                    </tr>
                </thead>
                <tbody>
    """
    
    # Add rows for new tokens
    for i, (token, prob) in enumerate(new_tokens):
        bar_width = int(prob / max_prob * 100)
        row_class = "even-row" if i % 2 == 0 else "odd-row"
        html += f"""
                    <tr class="{row_class}">
                        <td class="monospace token-col" title="{token}">{token}</td>
                        <td class="prob-col" style="text-align: right;">{prob:.3f}</td>
                        <td class="dist-col">
                            <div class="bar-container">
                                <div class="bar" style="background-color: #27AE60; width: {bar_width}%;"></div>
                                <span class="bar-text">{prob*100:.1f}%</span>
                            </div>
                        </td>
                    </tr>
        """
    
    html += """
                </tbody>
            </table>
        </div>
    </div>
    """
    
    display(HTML(html))


def display_generations_comparison(original_text, pre_intervention_gens, post_intervention_gens):
    """
    Display a comparison of pre-intervention and post-intervention generations
    with the new/continuation text highlighted.
    """
    # Ensure the original text is properly escaped
    escaped_original = html.escape(original_text)
    
    # Build the HTML with CSS for styling
    html_content = """
    <style>
    .generations-viz {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
        margin-bottom: 12px;
        font-size: 13px;
        max-width: 700px;
    }
    .generations-viz .section-header {
        font-weight: bold;
        font-size: 14px;
        margin: 10px 0 5px 0;
        padding: 4px 6px;
        border-radius: 3px;
        color: white;
        display: block;
    }
    .generations-viz .pre-intervention-header {
        background-color: #2471A3;
    }
    .generations-viz .post-intervention-header {
        background-color: #27AE60;
    }
    .generations-viz .generation-container {
        margin-bottom: 8px;
        padding: 3px;
        border-left: 3px solid rgba(100, 100, 100, 0.5);
    }
    .generations-viz .generation-text {
        background-color: rgba(200, 200, 200, 0.2);
        padding: 6px 8px;
        border-radius: 3px;
        border: 1px solid rgba(100, 100, 100, 0.5);
        font-family: monospace;
        font-weight: 500;
        white-space: pre-wrap;
        line-height: 1.2;
        font-size: 13px;
        overflow-x: auto;
    }
    .generations-viz .base-text {
        color: rgba(100, 100, 100, 0.9);
    }
    .generations-viz .new-text {
        background-color: rgba(255, 255, 0, 0.25);
        font-weight: bold;
        padding: 1px 0;
        border-radius: 2px;
    }
    .generations-viz .pre-intervention-item {
        border-left-color: #2471A3;
    }
    .generations-viz .post-intervention-item {
        border-left-color: #27AE60;
    }
    .generations-viz .generation-number {
        font-weight: bold;
        margin-bottom: 3px;
        color: rgba(70, 70, 70, 0.9);
        font-size: 12px;
    }
    </style>
    
    <div class="generations-viz">
    """
    
    # Add pre-intervention section
    html_content += """
    <div class="section-header pre-intervention-header">Pre-intervention generations:</div>
    """
    
    # Add each pre-intervention generation
    for i, gen_text in enumerate(pre_intervention_gens):
        # Split the text to highlight the continuation
        if gen_text.startswith(original_text):
            base_part = html.escape(original_text)
            new_part = html.escape(gen_text[len(original_text):])
            formatted_text = f'<span class="base-text">{base_part}</span><span class="new-text">{new_part}</span>'
        else:
            formatted_text = html.escape(gen_text)
        
        html_content += f"""
        <div class="generation-container pre-intervention-item">
            <div class="generation-number">Generation {i+1}</div>
            <div class="generation-text">{formatted_text}</div>
        </div>
        """
    
    # Add post-intervention section
    html_content += """
    <div class="section-header post-intervention-header">Post-intervention generations:</div>
    """
    
    # Add each post-intervention generation
    for i, gen_text in enumerate(post_intervention_gens):
        # Split the text to highlight the continuation
        if gen_text.startswith(original_text):
            base_part = html.escape(original_text)
            new_part = html.escape(gen_text[len(original_text):])
            formatted_text = f'<span class="base-text">{base_part}</span><span class="new-text">{new_part}</span>'
        else:
            formatted_text = html.escape(gen_text)
        
        html_content += f"""
        <div class="generation-container post-intervention-item">
            <div class="generation-number">Generation {i+1}</div>
            <div class="generation-text">{formatted_text}</div>
        </div>
        """
    
    html_content += """
    </div>
    """
    
    display(HTML(html_content))


def decode_url_features(url: str) -> Tuple[Dict[str, List[Feature]], List[Feature]]:
    """
    Extract both supernode features and individual singleton features from URL.

    Returns:
        Tuple of (supernode_features, singleton_features)
        - supernode_features: Dict mapping supernode names to lists of Features
        - singleton_features: List of individual Feature objects
    """
    decoded = urllib.parse.unquote(url)

    parsed_url = urllib.parse.urlparse(decoded)
    query_params = urllib.parse.parse_qs(parsed_url.query)

    # Extract supernodes
    supernodes_json = query_params.get("supernodes", ["[]"])[0]
    supernodes_data = json.loads(supernodes_json)

    supernode_features = {}
    name_counts = {}

    for supernode in supernodes_data:
        name = supernode[0]
        node_ids = supernode[1:]

        # Handle duplicate names by adding counter
        if name in name_counts:
            name_counts[name] += 1
            unique_name = f"{name} ({name_counts[name]})"
        else:
            name_counts[name] = 1
            unique_name = name

        nodes = []
        for node_id in node_ids:
            layer, feature_idx, pos = map(int, node_id.split("_"))
            nodes.append(Feature(layer, pos, feature_idx))

        supernode_features[unique_name] = nodes

    # Extract individual/singleton features from pinnedIds
    pinned_ids_str = query_params.get("pinnedIds", [""])[0]
    singleton_features = []

    if pinned_ids_str:
        pinned_ids = pinned_ids_str.split(",")
        for pinned_id in pinned_ids:
            # Handle both regular format (layer_feature_pos) and E_ format
            if pinned_id.startswith("E_"):
                # E_26865_9 format - embedding layer
                parts = pinned_id[2:].split("_")  # Remove 'E_' prefix
                if len(parts) == 2:
                    feature_idx, pos = map(int, parts)
                    # Use -1 to indicate embedding layer
                    singleton_features.append(Feature(-1, pos, feature_idx))
            else:
                # Regular layer_feature_pos format
                parts = pinned_id.split("_")
                if len(parts) == 3:
                    layer, feature_idx, pos = map(int, parts)
                    singleton_features.append(Feature(layer, pos, feature_idx))

    return supernode_features, singleton_features


# Keep the old function for backward compatibility
def extract_supernode_features(url: str) -> Dict[str, List[Feature]]:
    """Legacy function - only extracts supernode features"""
    supernode_features, _ = decode_url_features(url)
    return supernode_features


---
File: /tests/test_attribution_clt.py
---

import torch
import torch.nn as nn
from transformer_lens import HookedTransformerConfig

from circuit_tracer import attribute
from circuit_tracer.replacement_model import ReplacementModel
from circuit_tracer.transcoder.cross_layer_transcoder import CrossLayerTranscoder

torch.manual_seed(42)


def create_clt_model(cfg: HookedTransformerConfig):
    """Create a CLT and ReplacementModel with random weights."""
    # Create CLT with 4x expansion
    clt = CrossLayerTranscoder(
        n_layers=cfg.n_layers,
        d_transcoder=cfg.d_model * 4,
        d_model=cfg.d_model,
        dtype=cfg.dtype,
        lazy_decoder=False
    )

    # Initialize CLT weights
    with torch.no_grad():
        for param in clt.parameters():
            nn.init.uniform_(param, a=-0.1, b=0.1)

    # Create model
    model = ReplacementModel.from_config(cfg, clt)

    # Monkey patch all_special_ids if necessary
    type(model.tokenizer).all_special_ids = property(lambda self: [0])

    # Initialize model weights
    with torch.no_grad():
        for param in model.parameters():
            if param.requires_grad:
                nn.init.uniform_(param, a=-0.1, b=0.1)

    return model


def verify_feature_intervention(model, graph, feature_idx):
    """Verify that intervening on a feature produces the expected effects."""
    prompt = graph.input_tokens.unsqueeze(0)
    layer, pos, feat_id = graph.active_features[feature_idx]
    activation = graph.activation_values[feature_idx]
    influences = graph.adjacency_matrix[:, feature_idx]

    # Get decoder vectors for cross-layer effects
    decoder_vectors = model.transcoders.W_dec[layer][feat_id]

    def apply_steering(activations, hook):
        l = hook.layer() - layer
        activations[0, pos] += decoder_vectors[l] * activation
        return activations

    # Setup hooks
    cache, caching_hooks, _ = model.get_caching_hooks(
        names_filter=lambda name: model.feature_input_hook in name
    )
    freeze_hooks = model.setup_intervention_with_freeze(prompt, direct_effects=True)
    steering_hooks = [
        (f"blocks.{l}.{model.feature_output_hook}", apply_steering)
        for l in range(layer, model.cfg.n_layers)
    ]

    # Run intervention
    with model.hooks(freeze_hooks + caching_hooks + steering_hooks):
        _ = model(prompt)

    # Compute new activations
    clt_inputs = torch.cat(list(cache.values()), dim=0)
    new_activations = (
        torch.einsum("lbd,lfd->lbf", clt_inputs, model.transcoders.W_enc)
        + model.transcoders.b_enc[:, None]
    )
    new_activations = new_activations[tuple(graph.active_features.T)]

    # Calculate error
    delta = new_activations - graph.activation_values
    n_active = len(graph.active_features)
    expected_delta = influences[:n_active].cuda()

    max_error = (delta - expected_delta).abs().max().item()
    return max_error


def test_clt_attribution():
    """Test CLT attribution and intervention mechanism."""
    # Minimal config
    cfg = HookedTransformerConfig.from_dict(
        {
            "n_layers": 4,
            "d_model": 8,
            "n_ctx": 32,
            "d_head": 4,
            "n_heads": 2,
            "d_mlp": 32,
            "act_fn": "gelu",
            "d_vocab": 50,
            "model_name": "test-clt",
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "tokenizer_name": "gpt2",
        }
    )

    # Create model
    model = create_clt_model(cfg)

    # Run attribution
    prompt = torch.tensor([[0, 1, 2, 3, 4, 5]])
    graph = attribute(prompt, model, max_n_logits=5, desired_logit_prob=0.8, batch_size=32)

    # Test interventions on multiple random features
    n_active = len(graph.active_features)
    n_samples = min(100, n_active)
    sample_indices = torch.randperm(n_active)[:n_samples]

    max_errors = []

    from tqdm import tqdm
    for idx in tqdm(sample_indices):
        max_error = verify_feature_intervention(model, graph, idx)
        max_errors.append(max_error)

        assert max_error < 5e-4, f"Feature {idx}: max error {max_error:.6f} exceeds threshold"

    mean_error = sum(max_errors) / len(max_errors)
    max_error = max(max_errors)

    print(f"✓ CLT attribution test passed!")
    print(f"  Tested {n_samples} features out of {n_active} active features")
    print(f"  Mean max error: {mean_error:.6f}, Worst error: {max_error:.6f}")


if __name__ == "__main__":
    test_clt_attribution()



---
File: /tests/test_attributions_gemma.py
---

from functools import partial

import numpy as np
import torch
import torch.nn as nn
from torch import device
from tqdm import tqdm
from transformer_lens import HookedTransformerConfig

from circuit_tracer import attribute, Graph, ReplacementModel
from circuit_tracer.transcoder import SingleLayerTranscoder, TranscoderSet
from circuit_tracer.transcoder.activation_functions import JumpReLU


def verify_token_and_error_edges(
    model: ReplacementModel,
    graph: Graph,
    delete_bos: bool = False,
    act_atol=1e-3,
    act_rtol=1e-3,
    logit_atol=1e-5,
    logit_rtol=1e-3,
):
    s = graph.input_tokens
    adjacency_matrix = graph.adjacency_matrix.cuda()
    active_features = graph.active_features.cuda()
    logit_tokens = graph.logit_tokens.cuda()
    total_active_features = active_features.size(0)
    pos_start = 1 if delete_bos else 0

    ctx = model.setup_attribution(s)

    error_vectors = ctx.error_vectors
    token_vectors = ctx.token_vectors

    logits, activation_cache = model.get_activations(s, apply_activation_function=False)
    logits = logits.squeeze(0)

    relevant_activations = activation_cache[
        active_features[:, 0], active_features[:, 1], active_features[:, 2]
    ]
    relevant_logits = logits[-1, logit_tokens]
    demeaned_relevant_logits = relevant_logits - logits[-1].mean()

    freeze_hooks = model.setup_intervention_with_freeze(s, direct_effects=True)

    def verify_intervention(expected_effects, intervention):
        new_activation_cache, activation_hooks = model._get_activation_caching_hooks(
            apply_activation_function=False
        )

        fwd_hooks = [*freeze_hooks, intervention, *activation_hooks]
        new_logits = model.run_with_hooks(s, fwd_hooks=fwd_hooks)
        new_logits = new_logits.squeeze(0)

        new_activation_cache = torch.stack(new_activation_cache)
        new_relevant_activations = new_activation_cache[
            active_features[:, 0], active_features[:, 1], active_features[:, 2]
        ]
        new_relevant_logits = new_logits[-1, logit_tokens]
        new_demeaned_relevant_logits = new_relevant_logits - new_logits[-1].mean()

        expected_activation_difference = expected_effects[:total_active_features]
        expected_logit_difference = expected_effects[-len(logit_tokens) :]

        assert torch.allclose(
            new_relevant_activations,
            relevant_activations + expected_activation_difference,
            atol=act_atol,
            rtol=act_rtol,
        )
        assert torch.allclose(
            new_demeaned_relevant_logits,
            demeaned_relevant_logits + expected_logit_difference,
            atol=logit_atol,
            rtol=logit_rtol,
        )

    def hook_error_intervention(activations, hook, layer: int, pos: int):
        steering_vector = torch.zeros_like(activations)
        steering_vector[:, pos] += error_vectors[layer, pos]
        return activations + steering_vector

    for error_node_layer in range(error_vectors.size(0)):
        for error_node_pos in range(pos_start, error_vectors.size(1)):
            error_node_index = error_node_layer * error_vectors.size(1) + error_node_pos
            expected_effects = adjacency_matrix[:, total_active_features + error_node_index]
            intervention = (
                f"blocks.{error_node_layer}.{model.feature_output_hook}",
                partial(hook_error_intervention, layer=error_node_layer, pos=error_node_pos),
            )
            verify_intervention(expected_effects, intervention)

    def hook_token_intervention(activations, hook, pos: int):
        steering_vector = torch.zeros_like(activations)
        steering_vector[:, pos] += token_vectors[pos]
        return activations + steering_vector

    total_error_nodes = error_vectors.size(0) * error_vectors.size(1)
    for token_pos in range(pos_start, token_vectors.size(0)):
        expected_effects = adjacency_matrix[
            :, total_active_features + total_error_nodes + token_pos
        ]
        intervention = ("hook_embed", partial(hook_token_intervention, pos=token_pos))
        verify_intervention(expected_effects, intervention)


def verify_feature_edges(
    model: ReplacementModel,
    graph: Graph,
    n_samples: int = 100,
    act_atol=5e-4,
    act_rtol=1e-5,
    logit_atol=1e-5,
    logit_rtol=1e-3,
):
    s = graph.input_tokens
    adjacency_matrix = graph.adjacency_matrix.cuda()
    active_features = graph.active_features.cuda()
    logit_tokens = graph.logit_tokens.cuda()
    total_active_features = active_features.size(0)

    logits, activation_cache = model.get_activations(s, apply_activation_function=False)
    logits = logits.squeeze(0)

    relevant_activations = activation_cache[
        active_features[:, 0], active_features[:, 1], active_features[:, 2]
    ]
    relevant_logits = logits[-1, logit_tokens]
    demeaned_relevant_logits = relevant_logits - logits[-1].mean()

    def verify_intervention(
        expected_effects, layer: int, pos: int, feature_idx: int, new_activation
    ):
        new_logits, new_activation_cache = model.feature_intervention(
            s,
            [(layer, pos, feature_idx, new_activation)],
            direct_effects=True,
            apply_activation_function=False,
        )
        new_logits = new_logits.squeeze(0)

        new_relevant_activations = new_activation_cache[
            active_features[:, 0], active_features[:, 1], active_features[:, 2]
        ]
        new_relevant_logits = new_logits[-1, logit_tokens]
        new_demeaned_relevant_logits = new_relevant_logits - new_logits[-1].mean()

        expected_activation_difference = expected_effects[:total_active_features]
        expected_logit_difference = expected_effects[-len(logit_tokens) :]

        assert torch.allclose(
            new_relevant_activations,
            relevant_activations + expected_activation_difference,
            atol=act_atol,
            rtol=act_rtol,
        )
        assert torch.allclose(
            new_demeaned_relevant_logits,
            demeaned_relevant_logits + expected_logit_difference,
            atol=logit_atol,
            rtol=logit_rtol,
        )

    random_order = torch.randperm(active_features.size(0))
    chosen_nodes = random_order[:n_samples]
    for chosen_node in tqdm(chosen_nodes):
        layer, pos, feature_idx = active_features[chosen_node]
        old_activation = activation_cache[layer, pos, feature_idx]
        new_activation = old_activation * 2
        expected_effects = adjacency_matrix[:, chosen_node]
        verify_intervention(expected_effects, layer, pos, feature_idx, new_activation)


def load_dummy_gemma_model(cfg: HookedTransformerConfig):
    transcoders = {
        layer_idx: SingleLayerTranscoder(
            cfg.d_model, cfg.d_model * 4, JumpReLU(0.0, 0.1), layer_idx
        )
        for layer_idx in range(cfg.n_layers)
    }
    for transcoder in transcoders.values():
        for _, param in transcoder.named_parameters():
            nn.init.uniform_(param, a=-1, b=1)

    transcoder_set = TranscoderSet(transcoders, feature_input_hook="mlp.hook_in", feature_output_hook="mlp.hook_out")
    model = ReplacementModel.from_config(cfg, transcoder_set)

    type(model.tokenizer).all_special_ids = property(lambda self: [0])

    for _, param in model.named_parameters():
        nn.init.uniform_(param, a=-1, b=1)

    for transcoder in model.transcoders:
        nn.init.uniform_(transcoder.activation_function.threshold, a=0, b=1)

    return model


def verify_small_gemma_model(s: torch.Tensor):
    gemma_small_cfg = {
        "n_layers": 2,
        "d_model": 8,
        "n_ctx": 8192,
        "d_head": 4,
        "model_name": "gemma-2-2b",
        "n_heads": 2,
        "d_mlp": 16,
        "act_fn": "gelu_pytorch_tanh",
        "d_vocab": 16,
        "eps": 1e-06,
        "use_attn_result": False,
        "use_attn_scale": True,
        "attn_scale": np.float64(16.0),
        "use_split_qkv_input": False,
        "use_hook_mlp_in": False,
        "use_attn_in": False,
        "use_local_attn": True,
        "ungroup_grouped_query_attention": False,
        "original_architecture": "Gemma2ForCausalLM",
        "from_checkpoint": False,
        "checkpoint_index": None,
        "checkpoint_label_type": None,
        "checkpoint_value": None,
        "tokenizer_name": "google/gemma-2-2b",
        "window_size": 4096,
        "attn_types": ["global", "local"],
        "init_mode": "gpt2",
        "normalization_type": "RMSPre",
        "device": device(type="cuda"),
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": False,
        "seed": None,
        "initializer_range": 0.02,
        "init_weights": False,
        "scale_attn_by_inverse_layer_idx": False,
        "positional_embedding_type": "rotary",
        "final_rms": True,
        "d_vocab_out": 16,
        "parallel_attn_mlp": False,
        "rotary_dim": 4,
        "n_params": 2146959360,
        "use_hook_tokens": False,
        "gated_mlp": True,
        "default_prepend_bos": True,
        "dtype": torch.float32,
        "tokenizer_prepends_bos": True,
        "n_key_value_heads": 2,
        "post_embedding_ln": False,
        "rotary_base": 10000.0,
        "trust_remote_code": False,
        "rotary_adjacent_pairs": False,
        "load_in_4bit": False,
        "num_experts": None,
        "experts_per_token": None,
        "relative_attention_max_distance": None,
        "relative_attention_num_buckets": None,
        "decoder_start_token_id": None,
        "tie_word_embeddings": False,
        "use_normalization_before_and_after": True,
        "attn_scores_soft_cap": 50.0,
        "output_logits_soft_cap": 0.0,
        "use_NTK_by_parts_rope": False,
        "NTK_by_parts_low_freq_factor": 1.0,
        "NTK_by_parts_high_freq_factor": 4.0,
        "NTK_by_parts_factor": 8.0,
    }
    cfg = HookedTransformerConfig.from_dict(gemma_small_cfg)
    model = load_dummy_gemma_model(cfg)
    graph = attribute(s, model)

    verify_token_and_error_edges(model, graph, delete_bos=False)
    verify_feature_edges(model, graph)


def verify_large_gemma_model(s: torch.Tensor):
    gemma_large_cfg = {
        "n_layers": 16,
        "d_model": 64,
        "n_ctx": 8192,
        "d_head": 32,
        "model_name": "gemma-2-2b",
        "n_heads": 16,
        "d_mlp": 128,
        "act_fn": "gelu_pytorch_tanh",
        "d_vocab": 128,
        "eps": 1e-06,
        "use_attn_result": False,
        "use_attn_scale": True,
        "attn_scale": np.float64(16.0),
        "use_split_qkv_input": False,
        "use_hook_mlp_in": False,
        "use_attn_in": False,
        "use_local_attn": True,
        "ungroup_grouped_query_attention": False,
        "original_architecture": "Gemma2ForCausalLM",
        "from_checkpoint": False,
        "checkpoint_index": None,
        "checkpoint_label_type": None,
        "checkpoint_value": None,
        "tokenizer_name": "google/gemma-2-2b",
        "window_size": 4096,
        "attn_types": [
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
            "global",
            "local",
        ],
        "init_mode": "gpt2",
        "normalization_type": "RMSPre",
        "device": device(type="cuda"),
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": False,
        "seed": None,
        "initializer_range": 0.02,
        "init_weights": False,
        "scale_attn_by_inverse_layer_idx": False,
        "positional_embedding_type": "rotary",
        "final_rms": True,
        "d_vocab_out": 128,
        "parallel_attn_mlp": False,
        "rotary_dim": 32,
        "n_params": 2146959360,
        "use_hook_tokens": False,
        "gated_mlp": True,
        "default_prepend_bos": True,
        "dtype": torch.float32,
        "tokenizer_prepends_bos": True,
        "n_key_value_heads": 16,
        "post_embedding_ln": False,
        "rotary_base": 10000.0,
        "trust_remote_code": False,
        "rotary_adjacent_pairs": False,
        "load_in_4bit": False,
        "num_experts": None,
        "experts_per_token": None,
        "relative_attention_max_distance": None,
        "relative_attention_num_buckets": None,
        "decoder_start_token_id": None,
        "tie_word_embeddings": False,
        "use_normalization_before_and_after": True,
        "attn_scores_soft_cap": 50.0,
        "output_logits_soft_cap": 0.0,
        "use_NTK_by_parts_rope": False,
        "NTK_by_parts_low_freq_factor": 1.0,
        "NTK_by_parts_high_freq_factor": 4.0,
        "NTK_by_parts_factor": 8.0,
    }
    cfg = HookedTransformerConfig.from_dict(gemma_large_cfg)
    model = load_dummy_gemma_model(cfg)
    graph = attribute(s, model)

    verify_token_and_error_edges(model, graph, delete_bos=False)
    verify_feature_edges(model, graph)


def verify_gemma_2_2b(s: str):
    model = ReplacementModel.from_pretrained("google/gemma-2-2b", "gemma")
    graph = attribute(s, model)

    print("Changing logit softcap to 0, as the logits will otherwise be off.")
    with model.zero_softcap():
        verify_token_and_error_edges(model, graph, delete_bos=True)
        verify_feature_edges(model, graph)


def test_small_gemma_model():
    s = torch.tensor([0, 3, 4, 3, 2, 5, 3, 8])
    verify_small_gemma_model(s)


def test_large_gemma_model():
    s = torch.tensor([0, 113, 24, 53, 27])
    verify_large_gemma_model(s)


def test_gemma_2_2b():
    s = "The National Digital Analytics Group (ND"
    verify_gemma_2_2b(s)


if __name__ == "__main__":
    torch.manual_seed(42)
    test_small_gemma_model()
    test_large_gemma_model()
    test_gemma_2_2b()



---
File: /tests/test_attributions_llama.py
---

import os
import sys

import numpy as np
import torch
import torch.nn as nn
from torch import device
from transformer_lens import HookedTransformerConfig

from circuit_tracer import attribute, ReplacementModel
from circuit_tracer.transcoder import SingleLayerTranscoder, TranscoderSet
from circuit_tracer.transcoder.activation_functions import TopK

sys.path.append(os.path.dirname(__file__))
from test_attributions_gemma import verify_feature_edges, verify_token_and_error_edges


def load_dummy_llama_model(cfg: HookedTransformerConfig, k: int):
    transcoders = {
        layer_idx: SingleLayerTranscoder(
            cfg.d_model, cfg.d_model * 4, TopK(k), layer_idx, skip_connection=True
        )
        for layer_idx in range(cfg.n_layers)
    }
    for transcoder in transcoders.values():
        for _, param in transcoder.named_parameters():
            nn.init.uniform_(param, a=-1, b=1)

    transcoder_set = TranscoderSet(transcoders, feature_input_hook="mlp.hook_in", feature_output_hook="mlp.hook_out")
    model = ReplacementModel.from_config(cfg, transcoder_set)

    ids = model.tokenizer.all_special_ids
    type(model.tokenizer).all_special_ids = property(lambda self: [0] + ids)
    for _, param in model.named_parameters():
        nn.init.uniform_(param, a=-1, b=1)

    return model


def verify_small_llama_model(s: torch.Tensor):
    llama_small_cfg = {
        "n_layers": 2,
        "d_model": 8,
        "n_ctx": 2048,
        "d_head": 4,
        "model_name": "Llama-3.2-1B",
        "n_heads": 2,
        "d_mlp": 16,
        "act_fn": "silu",
        "d_vocab": 16,
        "eps": 1e-05,
        "use_attn_result": False,
        "use_attn_scale": True,
        "attn_scale": np.float64(8.0),
        "use_split_qkv_input": False,
        "use_hook_mlp_in": False,
        "use_attn_in": False,
        "use_local_attn": False,
        "ungroup_grouped_query_attention": False,
        "original_architecture": "LlamaForCausalLM",
        "from_checkpoint": False,
        "checkpoint_index": None,
        "checkpoint_label_type": None,
        "checkpoint_value": None,
        "tokenizer_name": "meta-llama/Llama-3.2-1B",
        "window_size": None,
        "attn_types": None,
        "init_mode": "gpt2",
        "normalization_type": "RMSPre",
        "device": device(type="cuda"),
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": False,
        "seed": None,
        "initializer_range": np.float64(0.017677669529663688),
        "init_weights": False,
        "scale_attn_by_inverse_layer_idx": False,
        "positional_embedding_type": "rotary",
        "final_rms": True,
        "d_vocab_out": 16,
        "parallel_attn_mlp": False,
        "rotary_dim": 4,
        "n_params": 1073741824,
        "use_hook_tokens": False,
        "gated_mlp": True,
        "default_prepend_bos": True,
        "dtype": torch.float32,
        "tokenizer_prepends_bos": True,
        "n_key_value_heads": 2,
        "post_embedding_ln": False,
        "rotary_base": 500000.0,
        "trust_remote_code": False,
        "rotary_adjacent_pairs": False,
        "load_in_4bit": False,
        "num_experts": None,
        "experts_per_token": None,
        "relative_attention_max_distance": None,
        "relative_attention_num_buckets": None,
        "decoder_start_token_id": None,
        "tie_word_embeddings": False,
        "use_normalization_before_and_after": False,
        "attn_scores_soft_cap": -1.0,
        "output_logits_soft_cap": -1.0,
        "use_NTK_by_parts_rope": True,
        "NTK_by_parts_low_freq_factor": 1.0,
        "NTK_by_parts_high_freq_factor": 4.0,
        "NTK_by_parts_factor": 32.0,
    }

    cfg = HookedTransformerConfig.from_dict(llama_small_cfg)
    k = 4
    model = load_dummy_llama_model(cfg, k)
    graph = attribute(s, model)

    verify_token_and_error_edges(model, graph, delete_bos=False)
    verify_feature_edges(model, graph)


def verify_large_llama_model(s: torch.Tensor):
    llama_large_cfg = {
        "n_layers": 8,
        "d_model": 128,
        "n_ctx": 2048,
        "d_head": 32,
        "model_name": "Llama-3.2-1B",
        "n_heads": 4,
        "d_mlp": 512,
        "act_fn": "silu",
        "d_vocab": 128,
        "eps": 1e-05,
        "use_attn_result": False,
        "use_attn_scale": True,
        "attn_scale": np.float64(8.0),
        "use_split_qkv_input": False,
        "use_hook_mlp_in": False,
        "use_attn_in": False,
        "use_local_attn": False,
        "ungroup_grouped_query_attention": False,
        "original_architecture": "LlamaForCausalLM",
        "from_checkpoint": False,
        "checkpoint_index": None,
        "checkpoint_label_type": None,
        "checkpoint_value": None,
        "tokenizer_name": "meta-llama/Llama-3.2-1B",
        "window_size": None,
        "attn_types": None,
        "init_mode": "gpt2",
        "normalization_type": "RMSPre",
        "device": device(type="cuda"),
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": False,
        "seed": None,
        "initializer_range": np.float64(0.017677669529663688),
        "init_weights": False,
        "scale_attn_by_inverse_layer_idx": False,
        "positional_embedding_type": "rotary",
        "final_rms": True,
        "d_vocab_out": 128,
        "parallel_attn_mlp": False,
        "rotary_dim": 32,
        "n_params": 1073741824,
        "use_hook_tokens": False,
        "gated_mlp": True,
        "default_prepend_bos": True,
        "dtype": torch.float32,
        "tokenizer_prepends_bos": True,
        "n_key_value_heads": 4,
        "post_embedding_ln": False,
        "rotary_base": 500000.0,
        "trust_remote_code": False,
        "rotary_adjacent_pairs": False,
        "load_in_4bit": False,
        "num_experts": None,
        "experts_per_token": None,
        "relative_attention_max_distance": None,
        "relative_attention_num_buckets": None,
        "decoder_start_token_id": None,
        "tie_word_embeddings": False,
        "use_normalization_before_and_after": False,
        "attn_scores_soft_cap": -1.0,
        "output_logits_soft_cap": -1.0,
        "use_NTK_by_parts_rope": True,
        "NTK_by_parts_low_freq_factor": 1.0,
        "NTK_by_parts_high_freq_factor": 4.0,
        "NTK_by_parts_factor": 32.0,
    }
    cfg = HookedTransformerConfig.from_dict(llama_large_cfg)
    k = 16
    model = load_dummy_llama_model(cfg, k)
    graph = attribute(s, model)

    verify_token_and_error_edges(model, graph, delete_bos=False)
    verify_feature_edges(model, graph)


def verify_llama_3_2_1b(s: str):
    model = ReplacementModel.from_pretrained("meta-llama/Llama-3.2-1B", "llama")
    graph = attribute(s, model, batch_size=128)

    verify_token_and_error_edges(model, graph, delete_bos=True)
    verify_feature_edges(model, graph)


def test_small_llama_model():
    s = torch.tensor([0, 3, 4, 3, 2, 5, 3, 8])
    verify_small_llama_model(s)


def test_large_llama_model():
    s = torch.tensor([0, 113, 24, 53, 27])
    verify_large_llama_model(s)


def test_llama_3_2_1b():
    s = "The National Digital Analytics Group (ND"
    verify_llama_3_2_1b(s)


if __name__ == "__main__":
    torch.manual_seed(42)
    test_small_llama_model()
    test_large_llama_model()
    test_llama_3_2_1b()



---
File: /tests/test_cross_layer_transcoder.py
---

import os
import tempfile
import pytest
import torch
import numpy as np
from safetensors.torch import save_file

from circuit_tracer.transcoder.cross_layer_transcoder import (
    CrossLayerTranscoder,
    load_clt,
)


@pytest.fixture
def create_test_clt_files():
    """Create temporary CLT safetensors files for testing."""
    def _create_files(n_layers=4, d_model=128, d_transcoder=512):
        tmpdir = tempfile.mkdtemp()
        
        # Create encoder and decoder files for each layer
        for i in range(n_layers):
            # Encoder weights and biases
            enc_dict = {
                f"W_enc_{i}": torch.randn(d_transcoder, d_model),
                f"b_enc_{i}": torch.randn(d_transcoder),
                f"b_dec_{i}": torch.randn(d_model),
            }
            enc_path = os.path.join(tmpdir, f"W_enc_{i}.safetensors")
            save_file(enc_dict, enc_path)
            
            # Decoder weights - shape depends on source layer
            dec_dict = {
                f"W_dec_{i}": torch.randn(d_transcoder, n_layers - i, d_model)
            }
            dec_path = os.path.join(tmpdir, f"W_dec_{i}.safetensors")
            save_file(dec_dict, dec_path)
        
        return tmpdir
    
    return _create_files


# === Attribution Tests ===

def test_compute_attribution_components(create_test_clt_files):
    """Test the main attribution functionality of CLT."""
    clt_path = create_test_clt_files()
    clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=True,
        lazy_decoder=True,
    )
    
    # Create test input
    n_pos = 10
    inputs = torch.randn(clt.n_layers, n_pos, clt.d_model, dtype=clt.b_enc.dtype)
    
    # Compute attribution components
    components = clt.compute_attribution_components(inputs)
    
    # Verify all required components are present
    assert "activation_matrix" in components
    assert "reconstruction" in components
    assert "encoder_vecs" in components
    assert "decoder_vecs" in components
    assert "encoder_to_decoder_map" in components
    assert "decoder_locations" in components
    
    # Check activation matrix
    act_matrix = components["activation_matrix"]
    assert act_matrix.is_sparse
    assert act_matrix.shape == (clt.n_layers, n_pos, clt.d_transcoder)
    
    # Check reconstruction
    reconstruction = components["reconstruction"]
    assert reconstruction.shape == (clt.n_layers, n_pos, clt.d_model)
    
    # Check encoder/decoder vectors have consistent counts
    n_active_encoders = act_matrix._nnz()
    assert components["encoder_vecs"].shape[0] == n_active_encoders
    
    # Decoder count should be >= encoder count due to cross-layer writing
    assert components["decoder_vecs"].shape[0] >= n_active_encoders
    assert components["decoder_vecs"].shape[1] == clt.d_model
    
    # Check decoder locations
    decoder_locs = components["decoder_locations"]
    assert decoder_locs.shape[0] == 2  # layer and position indices


def test_encode_sparse_with_lazy_encoder(create_test_clt_files):
    """Test sparse encoding with lazy encoder loading."""
    clt_path = create_test_clt_files()
    
    # Test with lazy encoder
    lazy_clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=True,
        lazy_decoder=False,
    )
    
    # Test with eager encoder for comparison
    eager_clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=False,
        lazy_decoder=False,
    )
    
    # Create test input
    n_pos = 10
    inputs = torch.randn(lazy_clt.n_layers, n_pos, lazy_clt.d_model, dtype=eager_clt.b_enc.dtype)
    
    # Encode sparse with both
    lazy_sparse, lazy_encoders = lazy_clt.encode_sparse(inputs, zero_first_pos=True)
    eager_sparse, eager_encoders = eager_clt.encode_sparse(inputs, zero_first_pos=True)
    
    # Results should be identical
    assert torch.allclose(lazy_sparse.to_dense(), eager_sparse.to_dense())
    assert torch.allclose(lazy_encoders, eager_encoders)
    
    # Check that first position is zeroed
    assert lazy_sparse.to_dense()[:, 0].sum() == 0
    
    # Check shapes
    assert lazy_sparse.shape == (lazy_clt.n_layers, n_pos, lazy_clt.d_transcoder)
    assert lazy_encoders.shape[0] == lazy_sparse._nnz()
    assert lazy_encoders.shape[1] == lazy_clt.d_model


def test_decoder_slice_access(create_test_clt_files):
    """Test _get_decoder_vectors with lazy and eager decoder."""
    clt_path = create_test_clt_files()
    
    # Test with different decoder loading modes
    lazy_clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=False,
        lazy_decoder=True,
    )
    
    eager_clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=False,
        lazy_decoder=False,
    )
    
    # Test decoder access for layer 0 (writes to all layers)
    layer_id = 0
    
    # Full access
    lazy_full = lazy_clt._get_decoder_vectors(layer_id)
    eager_full = eager_clt._get_decoder_vectors(layer_id)
    assert torch.allclose(lazy_full, eager_full)
    assert lazy_full.shape == (lazy_clt.d_transcoder, lazy_clt.n_layers, lazy_clt.d_model)
    
    # Slice access
    feat_ids = torch.tensor([10, 50, 100, 200])
    lazy_slice = lazy_clt._get_decoder_vectors(layer_id, feat_ids.numpy())
    eager_slice = eager_clt._get_decoder_vectors(layer_id, feat_ids.numpy())
    assert torch.allclose(lazy_slice, eager_slice)
    assert lazy_slice.shape == (len(feat_ids), lazy_clt.n_layers, lazy_clt.d_model)
    
    # Test decoder for last layer (writes to only itself)
    layer_id = lazy_clt.n_layers - 1
    lazy_last = lazy_clt._get_decoder_vectors(layer_id)
    eager_last = eager_clt._get_decoder_vectors(layer_id)
    assert torch.allclose(lazy_last, eager_last)
    assert lazy_last.shape == (lazy_clt.d_transcoder, 1, lazy_clt.d_model)


def test_encode_operations_with_lazy_loading(create_test_clt_files):
    """Test encode operations with different lazy loading configurations."""
    clt_path = create_test_clt_files()
    
    # Create test input based on first CLT's dimensions
    test_clt = load_clt(clt_path, device=torch.device('cpu'))
    n_pos = 10
    inputs = torch.randn(test_clt.n_layers, n_pos, test_clt.d_model, dtype=test_clt.b_enc.dtype)
    
    # Test all lazy/eager combinations
    configs = [
        (False, False),  # Both eager
        (True, False),   # Lazy encoder
        (False, True),   # Lazy decoder
        (True, True),    # Both lazy
    ]
    
    outputs = []
    for lazy_enc, lazy_dec in configs:
        clt = load_clt(
            clt_path,
            device=torch.device('cpu'),
            lazy_encoder=lazy_enc,
            lazy_decoder=lazy_dec,
        )
        
        # Test regular encode
        encoded = clt.encode(inputs)
        assert encoded.shape == (clt.n_layers, n_pos, clt.d_transcoder)
        
        # Test single layer encode
        layer_encoded = clt.encode_layer(inputs[0], 0)
        assert layer_encoded.shape == (n_pos, clt.d_transcoder)
        
        outputs.append((encoded, layer_encoded))
    
    # All outputs should be identical regardless of lazy loading
    for i in range(1, len(outputs)):
        assert torch.allclose(outputs[0][0], outputs[i][0])
        assert torch.allclose(outputs[0][1], outputs[i][1])


def test_cross_layer_decoder_structure(create_test_clt_files):
    """Test CLT-specific cross-layer decoder structure."""
    clt_path = create_test_clt_files()
    clt = load_clt(
        clt_path,
        device=torch.device('cpu'),
        lazy_encoder=True,
        lazy_decoder=True,
    )
    
    # Create test input with known sparsity pattern
    n_pos = 5
    inputs = torch.randn(clt.n_layers, n_pos, clt.d_model, dtype=clt.b_enc.dtype)
    
    # Get sparse features
    features, _ = clt.encode_sparse(inputs)
    
    # Test decoder vector selection
    pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_mapping = clt.select_decoder_vectors(features)
    
    # Verify cross-layer structure
    # Features from layer i should write to layers i through n_layers-1
    enc_layers, _, _ = features.indices()
    unique_enc_layers = enc_layers.unique()
    
    for source_layer in unique_enc_layers:
        # Find decoder entries from this source layer
        source_mask = enc_layers == source_layer
        n_source_features = source_mask.sum()
        
        # Each source feature should write to (n_layers - source_layer) output layers
        expected_decoder_count = n_source_features * (clt.n_layers - source_layer)
        
        # Count actual decoder entries
        source_positions = torch.where(source_mask)[0]
        decoder_from_source = sum(encoder_mapping == pos for pos in source_positions).sum()
        
        assert decoder_from_source == expected_decoder_count


def test_forward_pass_consistency(create_test_clt_files):
    """Test that forward pass produces consistent results across lazy modes."""
    clt_path = create_test_clt_files(n_layers=3, d_model=64, d_transcoder=256)
    
    # Create CLTs with different configurations
    eager_clt = load_clt(clt_path, device=torch.device('cpu'), lazy_encoder=False, lazy_decoder=False)
    lazy_clt = load_clt(clt_path, device=torch.device('cpu'), lazy_encoder=True, lazy_decoder=True)
    
    # Test input
    n_pos = 8
    inputs = torch.randn(eager_clt.n_layers, n_pos, eager_clt.d_model, dtype=eager_clt.b_enc.dtype)
    
    # Forward pass
    eager_output = eager_clt(inputs)
    lazy_output = lazy_clt(inputs)
    
    # Outputs should be identical
    assert torch.allclose(eager_output, lazy_output, rtol=1e-5)
    assert eager_output.shape == (eager_clt.n_layers, n_pos, eager_clt.d_model) 


---
File: /tests/test_graph.py
---

import numpy as np
import torch
from torch import device
from transformer_lens import HookedTransformerConfig

from circuit_tracer.graph import Graph, compute_edge_influence, compute_node_influence


def test_small_graph():
    value = 10
    edge_matrix = torch.zeros([12, 12])
    for node in [1, 3, 6, 8]:
        edge_matrix[11, node] = 1 / 4

    for node in [0, 1, 6]:
        edge_matrix[3, node] = 1 / 12

    for node in [9, 10]:
        edge_matrix[1, node] = 1 / 6

    edge_matrix[0, 9] = 1 / 12

    adjacency_matrix = (edge_matrix > 0).float() * value

    # These get pruned during node pruning with a 0.8 threshold
    pruned_adjacency_matrix = adjacency_matrix.clone()
    pruned_adjacency_matrix[0, 9] = 0
    pruned_adjacency_matrix[3, 0] = 0

    post_pruning_edge_matrix = torch.zeros([12, 12])
    for node in [1, 3, 6, 8]:
        post_pruning_edge_matrix[11, node] = 1 / 4

    for node in [1, 6]:
        post_pruning_edge_matrix[3, node] = 1 / 8

    for node in [9, 10]:
        post_pruning_edge_matrix[1, node] = 3 / 16

    # This is our dummy model config; it doesn't really matter besides n_layers
    gemma_small_cfg = {
        "n_layers": 2,
        "d_model": 8,
        "n_ctx": 8192,
        "d_head": 4,
        "model_name": "gemma-2-2b",
        "n_heads": 2,
        "d_mlp": 16,
        "act_fn": "gelu_pytorch_tanh",
        "d_vocab": 16,
        "eps": 1e-06,
        "use_attn_result": False,
        "use_attn_scale": True,
        "attn_scale": np.float64(16.0),
        "use_split_qkv_input": False,
        "use_hook_mlp_in": False,
        "use_attn_in": False,
        "use_local_attn": True,
        "ungroup_grouped_query_attention": False,
        "original_architecture": "Gemma2ForCausalLM",
        "from_checkpoint": False,
        "checkpoint_index": None,
        "checkpoint_label_type": None,
        "checkpoint_value": None,
        "tokenizer_name": "google/gemma-2-2b",
        "window_size": 4096,
        "attn_types": ["global", "local"],
        "init_mode": "gpt2",
        "normalization_type": "RMSPre",
        "device": device(type="cuda"),
        "n_devices": 1,
        "attention_dir": "causal",
        "attn_only": False,
        "seed": None,
        "initializer_range": 0.02,
        "init_weights": False,
        "scale_attn_by_inverse_layer_idx": False,
        "positional_embedding_type": "rotary",
        "final_rms": True,
        "d_vocab_out": 16,
        "parallel_attn_mlp": False,
        "rotary_dim": 4,
        "n_params": 2146959360,
        "use_hook_tokens": False,
        "gated_mlp": True,
        "default_prepend_bos": True,
        "dtype": torch.float32,
        "tokenizer_prepends_bos": True,
        "n_key_value_heads": 2,
        "post_embedding_ln": False,
        "rotary_base": 10000.0,
        "trust_remote_code": False,
        "rotary_adjacent_pairs": False,
        "load_in_4bit": False,
        "num_experts": None,
        "experts_per_token": None,
        "relative_attention_max_distance": None,
        "relative_attention_num_buckets": None,
        "decoder_start_token_id": None,
        "tie_word_embeddings": False,
        "use_normalization_before_and_after": True,
        "attn_scores_soft_cap": 50.0,
        "output_logits_soft_cap": 0.0,
        "use_NTK_by_parts_rope": False,
        "NTK_by_parts_low_freq_factor": 1.0,
        "NTK_by_parts_high_freq_factor": 4.0,
        "NTK_by_parts_factor": 8.0,
    }
    cfg = HookedTransformerConfig.from_dict(gemma_small_cfg)
    test_graph = Graph(
        input_string="ab",
        input_tokens=torch.tensor([0, 1]),
        active_features=torch.tensor([1, 2, 3, 4, 5]),
        adjacency_matrix=adjacency_matrix,
        cfg=cfg,
        logit_tokens=torch.tensor([0]),
        logit_probabilities=torch.tensor([1.0]),
        selected_features=torch.tensor([1, 2, 3, 4, 5]),
        activation_values=torch.tensor([1, 2, 3, 4, 5]) * 2,
    )
    test_graph.cfg.n_layers = 2

    logit_weights = torch.zeros(adjacency_matrix.size(0))
    logit_weights[-1] = 1.0

    node_influence_on_logits = compute_node_influence(test_graph.adjacency_matrix, logit_weights)
    influence_tensor = torch.tensor(
        [1 / 12, 1 / 3, 0, 1 / 4, 0, 0, 1 / 3, 0, 1 / 4, 1 / 4, 1 / 6, 0]
    )
    assert torch.allclose(node_influence_on_logits, influence_tensor)

    edge_influence_on_logits = compute_edge_influence(pruned_adjacency_matrix, logit_weights)
    assert torch.allclose(edge_influence_on_logits, post_pruning_edge_matrix)


if __name__ == "__main__":
    test_small_graph()



---
File: /tests/test_hf_utils.py
---

# file: circuit-tracer/utils/test_hf_utils.py

import unittest
from unittest import mock

# Import the function and exceptions you need to test or mock
from circuit_tracer.utils.hf_utils import download_hf_uris
from huggingface_hub.utils import GatedRepoError, RepositoryNotFoundError, HfHubHTTPError

# A dummy URI for all tests
TEST_URI = "hf://test-org/test-repo/model.bin"


class TestHfUtilsDownload(unittest.TestCase):
    """Test suite for the download_hf_uris function."""

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value=None)
    def test_public_no_token(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a public repo download with no token."""
        mock_repo_info.return_value = mock.MagicMock(private=False, gated=False)
        mock_download.return_value = "/fake/path/model.bin"
        result = download_hf_uris([TEST_URI])
        self.assertEqual(result, {TEST_URI: "/fake/path/model.bin"})
        mock_download.assert_called_once()

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value="fake_token")
    def test_public_with_token(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a public repo download with a token present."""
        mock_repo_info.return_value = mock.MagicMock(private=False, gated=False)
        mock_download.return_value = "/fake/path/model.bin"
        result = download_hf_uris([TEST_URI])
        self.assertEqual(result, {TEST_URI: "/fake/path/model.bin"})
        mock_download.assert_called_once()

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value="fake_token_with_access")
    def test_gated_with_access(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a gated repo where the user has access."""
        mock_repo_info.return_value = mock.MagicMock(private=False, gated=True)
        mock_download.return_value = "/fake/path/model.bin"
        result = download_hf_uris([TEST_URI])
        self.assertEqual(result, {TEST_URI: "/fake/path/model.bin"})
        mock_download.assert_called_once()

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value="fake_token_no_access")
    def test_gated_without_access(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a gated repo where the user lacks access.
           The download is attempted and raises GatedRepoError.
        """
        # Setup: Pre-flight check passes, as repo_info just returns metadata.
        mock_repo_info.return_value = mock.MagicMock(private=False, gated=True)
        # Setup: The download itself will fail.
        mock_download.side_effect = GatedRepoError("User has not accepted terms.")

        # Execute & Assert: Check that the GatedRepoError is raised by the function.
        with self.assertRaises(GatedRepoError):
            download_hf_uris([TEST_URI])
        
        # Assert that the download was actually attempted.
        mock_download.assert_called_once()

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value=None)
    def test_gated_no_token(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a gated repo when no token is available.
        """
        # Setup: Pre-flight check passes, as repo_info just returns metadata.
        mock_repo_info.return_value = mock.MagicMock(private=False, gated=True)

        with self.assertRaises(PermissionError):
            download_hf_uris([TEST_URI])

        # no need to attempt download as no token means no access to a gated repo
        mock_download.assert_not_called()

    @mock.patch("circuit_tracer.utils.hf_utils.hf_hub_download")
    @mock.patch("circuit_tracer.utils.hf_utils.hf_api.repo_info")
    @mock.patch("circuit_tracer.utils.hf_utils.get_token", return_value="fake_token")
    def test_private_no_access_or_non_existent(self, mock_get_token, mock_repo_info, mock_download):
        """Tests a private repo the user can't see, or a repo that doesn't exist.
           Pre-flight check fails and error is propagated
        """
        mock_repo_info.side_effect = RepositoryNotFoundError("Repo not found.")

        with self.assertRaises(RepositoryNotFoundError):
            download_hf_uris([TEST_URI])
            
        # download is not called, as the repo is not found
        mock_download.assert_not_called()


if __name__ == '__main__':
    unittest.main()


---
File: /tests/test_single_layer_transcoder.py
---

import os
import tempfile
import pytest
import torch
import numpy as np
from safetensors.torch import save_file

from circuit_tracer.transcoder.single_layer_transcoder import (
    SingleLayerTranscoder,
    load_relu_transcoder,
    load_transcoder_set,
    TranscoderSet,
)


@pytest.fixture
def create_test_transcoder_file():
    """Create a temporary transcoder safetensors file for testing."""
    def _create_file(d_model=128, d_sae=512):
        with tempfile.NamedTemporaryFile(suffix=".safetensors", delete=False) as f:
            W_enc = torch.randn(d_sae, d_model)
            W_dec = torch.randn(d_sae, d_model)
            b_enc = torch.randn(d_sae)
            b_dec = torch.randn(d_model)
            
            state_dict = {
                "W_enc": W_enc,
                "W_dec": W_dec,
                "b_enc": b_enc,
                "b_dec": b_dec,
            }
            
            save_file(state_dict, f.name)
            return f.name, state_dict
    
    files_to_cleanup = []
    
    def _create_and_track(*args, **kwargs):
        path, state_dict = _create_file(*args, **kwargs)
        files_to_cleanup.append(path)
        return path, state_dict
    
    yield _create_and_track
    
    # Cleanup
    for path in files_to_cleanup:
        if os.path.exists(path):
            os.unlink(path)


# === Attribution Tests ===

def test_transcoder_set_attribution_components(create_test_transcoder_file):
    """Test compute_attribution_components functionality."""
    # Create test files for multiple layers
    n_layers = 3
    paths = {}
    for layer in range(n_layers):
        path, _ = create_test_transcoder_file(d_model=128, d_sae=512)
        paths[layer] = path
    
    transcoder_set = load_transcoder_set(
        transcoder_paths=paths,
        scan="test_scan",
        feature_input_hook="hook_resid_mid",
        feature_output_hook="hook_mlp_out",
        device=torch.device('cpu'),
        lazy_encoder=False,
        lazy_decoder=True,  # Test with lazy decoder
    )
    
    # Create test MLP inputs
    n_pos = 10
    d_model = 128
    mlp_inputs = torch.randn(n_layers, n_pos, d_model)
    
    # Compute attribution components
    components = transcoder_set.compute_attribution_components(mlp_inputs)
    
    # Verify all required components are present
    assert "activation_matrix" in components
    assert "reconstruction" in components
    assert "encoder_vecs" in components
    assert "decoder_vecs" in components
    assert "encoder_to_decoder_map" in components
    assert "decoder_locations" in components
    
    # Check activation matrix
    act_matrix = components["activation_matrix"]
    assert act_matrix.is_sparse
    assert act_matrix.shape == (n_layers, n_pos, 512)
    
    # Check reconstruction
    reconstruction = components["reconstruction"]
    assert reconstruction.shape == (n_layers, n_pos, d_model)
    
    # Check encoder/decoder vectors have matching counts
    n_active = act_matrix._nnz()
    assert components["encoder_vecs"].shape[0] == n_active
    assert components["decoder_vecs"].shape[0] == n_active
    assert components["encoder_to_decoder_map"].shape[0] == n_active
    
    # Check decoder locations
    decoder_locs = components["decoder_locations"]
    assert decoder_locs.shape == (2, n_active)  # layer and position indices


def test_sparse_encode_decode(create_test_transcoder_file):
    """Test sparse encoding and decoding functionality."""
    path, _ = create_test_transcoder_file(d_model=128, d_sae=512)
    
    transcoder = load_relu_transcoder(
        path, layer=0, device=torch.device('cpu'),
        lazy_encoder=False, lazy_decoder=False
    )
    
    # Create test input
    n_pos = 10
    d_model = 128
    test_input = torch.randn(n_pos, d_model)
    
    # Test sparse encoding
    sparse_acts, active_encoders = transcoder.encode_sparse(test_input, zero_first_pos=True)
    
    # Check that first position is zeroed
    assert sparse_acts[0].sum() == 0
    
    # Check sparse format
    assert sparse_acts.is_sparse
    assert sparse_acts.shape == (n_pos, 512)
    
    # Test sparse decoding
    reconstruction, scaled_decoders = transcoder.decode_sparse(sparse_acts)
    
    assert reconstruction.shape == (n_pos, d_model)
    assert len(scaled_decoders) == sparse_acts._nnz()


def test_decoder_slice_access(create_test_transcoder_file):
    """Test _get_decoder_vectors with both lazy and eager decoder."""
    path, expected_state = create_test_transcoder_file(d_model=128, d_sae=512)
    
    # Test eager decoder
    eager_transcoder = load_relu_transcoder(
        path, layer=0, lazy_encoder=True, lazy_decoder=False,
        device=torch.device('cpu')
    )
    
    # Test lazy decoder
    lazy_transcoder = load_relu_transcoder(
        path, layer=0, lazy_encoder=True, lazy_decoder=True,
        device=torch.device('cpu')
    )
    
    # Test full access
    eager_full = eager_transcoder._get_decoder_vectors()
    lazy_full = lazy_transcoder._get_decoder_vectors()
    assert torch.allclose(eager_full, lazy_full)
    assert torch.allclose(eager_full, expected_state["W_dec"])
    
    # Test slice access
    indices = torch.tensor([10, 50, 100, 200])
    eager_slice = eager_transcoder._get_decoder_vectors(indices)
    lazy_slice = lazy_transcoder._get_decoder_vectors(indices)
    assert torch.allclose(eager_slice, lazy_slice)
    assert torch.allclose(eager_slice, expected_state["W_dec"][indices])


def test_encode_decode_operations(create_test_transcoder_file):
    """Test that encode/decode operations work with lazy loading."""
    path, expected_state = create_test_transcoder_file(d_model=128, d_sae=512)
    
    # Create test input
    batch_size = 2
    seq_len = 10
    d_model = 128
    test_input = torch.randn(batch_size * seq_len, d_model)
    
    # Test all combinations
    configs = [
        (False, False),  # Both eager
        (True, False),   # Lazy encoder
        (False, True),   # Lazy decoder  
        (True, True),    # Both lazy
    ]
    
    outputs = []
    for lazy_enc, lazy_dec in configs:
        transcoder = load_relu_transcoder(
            path, layer=0, lazy_encoder=lazy_enc, lazy_decoder=lazy_dec,
            device=torch.device('cpu')
        )
        
        # Test encode
        encoded = transcoder.encode(test_input)
        assert encoded.shape == (batch_size * seq_len, 512)
        
        # Test decode
        decoded = transcoder.decode(encoded)
        assert decoded.shape == (batch_size * seq_len, d_model)
        
        outputs.append((encoded, decoded))
    
    # All outputs should be identical regardless of lazy loading
    for i in range(1, len(outputs)):
        assert torch.allclose(outputs[0][0], outputs[i][0])  # encoded
        assert torch.allclose(outputs[0][1], outputs[i][1])  # decoded


# === Lazy Loading Tests ===

def test_lazy_encoder_only(create_test_transcoder_file):
    """Test lazy loading of encoder while decoder is eager."""
    path, expected_state = create_test_transcoder_file()
    
    transcoder = load_relu_transcoder(
        path, 
        layer=0, 
        lazy_encoder=True, 
        lazy_decoder=False,
        device=torch.device('cpu')
    )
    
    # W_dec should be a parameter, W_enc should not exist as attribute
    assert hasattr(transcoder, "W_dec")
    assert isinstance(transcoder.W_dec, torch.nn.Parameter)
    assert "W_enc" not in transcoder._parameters
    
    # Accessing W_enc should trigger lazy loading
    W_enc = transcoder.W_enc
    assert W_enc.shape == expected_state["W_enc"].shape
    assert torch.allclose(W_enc, expected_state["W_enc"])
    
    # W_enc should NOT become a parameter after access
    assert "W_enc" not in transcoder._parameters


def test_lazy_decoder_only(create_test_transcoder_file):
    """Test lazy loading of decoder while encoder is eager."""
    path, expected_state = create_test_transcoder_file()
    
    transcoder = load_relu_transcoder(
        path, 
        layer=0, 
        lazy_encoder=False, 
        lazy_decoder=True,
        device=torch.device('cpu')
    )
    
    # W_enc should be a parameter, W_dec should not exist as attribute
    assert hasattr(transcoder, "W_enc")
    assert isinstance(transcoder.W_enc, torch.nn.Parameter)
    assert "W_dec" not in transcoder._parameters
    
    # Accessing W_dec should trigger lazy loading
    W_dec = transcoder.W_dec
    assert W_dec.shape == expected_state["W_dec"].shape
    assert torch.allclose(W_dec, expected_state["W_dec"])
    
    # W_dec should NOT become a parameter after access
    assert "W_dec" not in transcoder._parameters


def test_lazy_loading_both(create_test_transcoder_file):
    """Test lazy loading of both encoder and decoder."""
    path, expected_state = create_test_transcoder_file()
    
    transcoder = load_relu_transcoder(
        path, 
        layer=0, 
        lazy_encoder=True, 
        lazy_decoder=True,
        device=torch.device('cpu')
    )
    
    # Neither weight should exist as parameters
    assert "W_enc" not in transcoder._parameters
    assert "W_dec" not in transcoder._parameters
    
    # Accessing weights should trigger lazy loading
    W_enc = transcoder.W_enc
    W_dec = transcoder.W_dec
    
    assert W_enc.shape == expected_state["W_enc"].shape
    assert W_dec.shape == expected_state["W_dec"].shape
    assert torch.allclose(W_enc, expected_state["W_enc"])
    assert torch.allclose(W_dec, expected_state["W_dec"])


---
File: /CONTRIBUTING.md
---

# Contributing to circuit-tracer
Thank you for your interest in contributing to circuit-tracer! We appreciate the community involvement we've seen so far and welcome contributions.

## Important Notes

### Maintenance Bandwidth
We maintain this project on a **best-effort basis**.
- PR reviews may take time and we cannot guarantee timely responses or merges
- Issues may not receive immediate attention

### API Stability
⚠️ **Warning**: This library is under active development and **breaking changes are possible**. The API is not stable and breaking changes may occur in any release.

## How to Contribute
We encourage contributions! Here's how you can help:

1. **Make your changes** with clear, descriptive commits
2. **Test your changes** - see Testing section below
3. **Submit a Pull Request** with a clear description of your changes

## Testing
We use `ruff check` and `ruff format` for code quality. When contributing:
- Run `ruff check` and `ruff format` on your changes
- Run existing tests to ensure nothing breaks
- Check that relevant demo notebooks still execute correctly, particularly:
  - `demos/circuit_tracing_tutorial.ipynb`
  - `demos/attribute_demo.ipynb`
  - `demos/intervention_demo.ipynb`
- Add tests for new functionality where possible

## What We're Looking For
- Bug fixes
- Performance enhancements
- New features that align with the project's goals
- In future: updates to support new models or transcoders (currently blocked on our pipeline for generating feature activation examples)

## Before Contributing
- Check existing issues and PRs to avoid duplicate work
- For major changes, consider opening an issue first to discuss the approach
- Understand that we cannot commit to reviewing all changes

## Code of Conduct
Please be respectful and constructive in all interactions.



---
File: /README.md
---

# circuit-tracer

This library implements tools for finding circuits using features from (cross-layer) MLP transcoders, as originally introduced by [Ameisen et al. (2025)](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) and [Lindsey et al. (2025)](https://transformer-circuits.pub/2025/attribution-graphs/biology.html).

Our library performs three main tasks. 
1. Given a model with pre-trained transcoders, it finds the circuit / attribution graph; i.e., it computes the direct effect that each non-zero transcoder feature, transcoder error node, and input token has on each other non-zero transcoder feature and output logit.
2. Given an attribution graph, it visualizes this graph and allows you to annotate these features.
3. Enables interventions on a model's transcoder features using the insights gained from the attribution graph; i.e. you can set features to arbitrary values, and observe how model output changes.

## Usage
One quick way to start is to try our [tutorial notebook](https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb)! 

You can also find circuits and visualize them in one of three ways:
1. Use `circuit-tracer` on [Neuronpedia](https://www.neuronpedia.org/gemma-2-2b/graph?slug=gemma-fact-dallas-austin&pinnedIds=27_22605_10%2C20_15589_10%2CE_26865_9%2C21_5943_10%2C23_12237_10%2C20_15589_9%2C16_25_9%2C14_2268_9%2C18_8959_10%2C4_13154_9%2C7_6861_9%2C19_1445_10%2CE_2329_7%2CE_6037_4%2C0_13727_7%2C6_4012_7%2C17_7178_10%2C15_4494_4%2C6_4662_4%2C4_7671_4%2C3_13984_4%2C1_1000_4%2C19_7477_9%2C18_6101_10%2C16_4298_10%2C7_691_10&supernodes=%5B%5B%22state%22%2C%226_4012_7%22%2C%220_13727_7%22%5D%2C%5B%22preposition+followed+by+place+name%22%2C%2219_1445_10%22%2C%2218_6101_10%22%5D%2C%5B%22Texas%22%2C%2220_15589_10%22%2C%2220_15589_9%22%2C%2219_7477_9%22%2C%2216_25_9%22%2C%224_13154_9%22%2C%2214_2268_9%22%2C%227_6861_9%22%5D%2C%5B%22capital+%2F+capital+cities%22%2C%2215_4494_4%22%2C%226_4662_4%22%2C%224_7671_4%22%2C%223_13984_4%22%2C%221_1000_4%22%2C%2221_5943_10%22%2C%2217_7178_10%22%2C%227_691_10%22%2C%2216_4298_10%22%5D%5D&pruningThreshold=0.6&clickedId=21_5943_10&densityThreshold=0.99) - no installation required! Just click on `+ New Graph` to create your own, or use the drop-down menu to select an existing graph.
2. Run `circuit-tracer` via a Python script or Jupyter notebook. Start with our [tutorial notebook](https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb). This will work on Colab with the GPU resources provided for free by default - just click on the Colab badge! Check out the **Demos** section below for more tutorials. You can also run these demo notebooks locally, with your own compute.
3. Run `circuit-tracer` via the command-line interface. This can only be done with your own compute. For more on how to do that, see **Command-Line Interface**. 

Working with Gemma-2 (2B) is possible with relatively limited GPU resources; Colab GPUs have 15GB of RAM. More GPU RAM will allow you to do less offloading, and to use a larger batch size. 

Currently, intervening on models with respect to the transcoder features you discover in your graphs is possible both when using `circuit-tracer` in a script or notebook, or on Neuronpedia for Gemma-2 (2B). To perform interventions on Neuronpedia, ensure at least one node is pinned, then click "Steer" in the subgraph.

## Installation
To install this library, clone it and run the command  `pip install .` in its directory.

## Demos
We include some demos showing how to use our library in the `demos` folder. The main demo is [`demos/circuit_tracing_tutorial.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/circuit_tracing_tutorial.ipynb), which replicates two of the findings from [this paper](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) using Gemma 2 (2B). All demos except for the Llama demo can be run on Colab.

We also make two simple demos of attribution and intervention available, for those who want to learn more about how to use the library:
- [`demos/attribute_demo.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/attribute_demo.ipynb): Demonstrates how to find circuits and visualize them. 
- [`demos/intervention_demo.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/intervention_demo.ipynb): Demonstrates how to perform interventions on models. 

We finally provide demos that dig deeper into specific, pre-computed and pre-annotated attribution graphs, performing interventions to demonstrate the correctness of the annotated graph:
- [`demos/gemma_demo.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/gemma_demo.ipynb): Explores graphs from Gemma 2 (2B).
- [`demos/gemma_it_demo.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/gemma_it_demo.ipynb): Explores graphs from instruction-tuned Gemma 2 (2B), using transcoders from the base model.
- [`demos/llama_demo.ipynb`](https://github.com/safety-research/circuit-tracer/blob/main/demos/llama_demo.ipynb): Explores graphs from Llama 3.2 (1B). Not supported on Colab.

We also provide a number of annotated attribution graphs for both models, which can be found at the top of their two demo notebooks.

## Command-Line Interface

The unified CLI performs the complete 3-step process for finding and visualizing circuits:

### 3-Step Process
1. **Attribution**: Runs the attribution algorithm to find the circuit/attribution graph, computing direct effects between transcoder features, error nodes, tokens, and output logits.
2. **Graph File Creation**: Prunes the attribution graph to remove low-effect nodes and edges, then converts it to JSON format suitable for visualization.
3. **Local Server**: Starts a local web server to visualize and interact with the graph in your browser.

### Basic Usage
To find a circuit, create the graph files, and start up a local server, use the command:

```
circuit-tracer attribute --prompt [prompt] --transcoder_set [transcoder_set] --slug [slug] --graph_file_dir [directory] --slug [slug] --graph_file_dir [graph_file_dir] --server
```

It will tell you where the server is serving (something like `localhost:[port]`). If you run this command on a remote machine, make sure to enable port forwarding, so you can see the graphs locally!

### Mandatory Arguments
**Attribution**
- `--prompt` (`-p`): The input prompt to analyze
- `--transcoder_set` (`-t`): The set of transcoders to use for attribution. Options:
  - HuggingFace repository ID (e.g., `mntss/gemma-scope-transcoders`, `username/repo-name@revision`)
  - Convenience shortcuts: `gemma` (GemmaScope transcoders) or `llama` (ReLU transcoders)

**Graph File Creation**

These are required if you want to run a local web server for visualization:
- `--slug`: A name/identifier for your analysis run
- `--graph_file_dir`: Directory path where JSON graph files will be saved

You can also save the raw attribution graph (to be loaded and used in Python later):
- `--graph_output_path` (`-o`): Path to save the raw attribution graph (`.pt` file)

You must set `--slug` and `--graph_file_dir`, or `--graph_output_path`, or both! Otherwise the CLI will output nothing.

**Local Server**
- `--server`: Start a local web server for graph visualization

### Optional Arguments

**Attribution Parameters:**
- `--model` (`-m`): Model architecture (auto-inferred for `gemma` and `llama` presets)
- `--max_n_logits` (default: 10): Maximum number of logit nodes to attribute from
- `--desired_logit_prob` (default: 0.95): Cumulative probability threshold for top logits
- `--batch_size` (default: 256): Batch size for backward passes
- `--max_feature_nodes`: Maximum number of feature nodes (defaults to 7500)
- `--dtype`: Datatype in which to load the model / transcoders (allowed: `float32/fp32`, `float16/fp16`, `bfloat16/bf16`)
- `--offload`: Memory optimization option (`cpu`, `disk`, or `None`)
- `--verbose`: Display detailed progress information

**Graph Pruning Parameters:**
- `--node_threshold` (default: 0.8): Keeps minimum nodes with cumulative influence ≥ threshold
- `--edge_threshold` (default: 0.98): Keeps minimum edges with cumulative influence ≥ threshold

**Server Parameters:**
- `--port` (default: 8041): Port for the local server

### Examples

**Complete workflow with visualization:**
```
circuit-tracer attribute \
  --prompt "The International Advanced Security Group (IAS" \
  --transcoder_set gemma \
  --slug gemma-demo \
  --graph_file_dir ./graph_files \
  --server
```

**Attribution only (save raw graph):**
```
circuit-tracer attribute \
  --prompt "The capital of France is" \
  --transcoder_set llama \
  --graph_output_path france_capital.pt
```

### Graph Annotation
When using the `--server` option, your browser will open to a local visualization interface. The interface is the same as in [the original papers](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) (frontend available [here](https://github.com/anthropics/attribution-graphs-frontend)).
- **Select a node**: Click on a node.
- **Pin / unpin a node to subgraph pane**: Ctrl+click/Commmand+click the node.
- **Annotate a node**: Click on the "Edit" button on the right side of the window while a node is selected to edit its annotation.
- **Group nodes**: Hold G and click on nodes to group them together into a supernode. Hold G and click on the x next to a supernode to ungroup all of them.
- **Annotate supernode / node group**: click on the label below the supernode to edit the supernode annotation.

## Cite
You can cite this library as follows:
```
@misc{circuit-tracer,
  author = {Hanna, Michael and Piotrowski, Mateusz and Lindsey, Jack and Ameisen, Emmanuel},
  title = {circuit-tracer},
  howpublished = {\url{https://github.com/safety-research/circuit-tracer}},
  note = {The first two authors contributed equally and are listed alphabetically.},
  year = {2025}
}
```

