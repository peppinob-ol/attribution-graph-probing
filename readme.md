# Attribution Graph Probing

**Automated Attribution Graph Analysis through Probe Prompting**

Streamlit application for automated analysis of attribution graphs with automatic classification and naming of supernodes through probe prompting.

---

## Overview

This project implements a **3-stage pipeline** for automatically analyzing and interpreting attribution graphs from models with Sparse Autoencoders (SAE) or Cross-Layer Transcoders (CLT):

### Pipeline v2.0

```
Stage 1: Graph Generation → Stage 2: Probe Prompts → Stage 3: Node Grouping
```

1. **Graph Generation**: Generate attribution graphs on Neuronpedia and extract static metrics
2. **Probe Prompts**: Analyze feature activations on LLM-generated semantic concepts
3. **Node Grouping**: Automatically classify and name supernodes for interpretability

### Results

- Automatic classification into 4 categories: **Semantic (Dictionary/Concept)**, **Say "X"**, **Relationship**
- Automatic naming based on activation patterns and peak tokens
- Export to Neuronpedia for interactive visualization
- Automatic checkpoint and resume for long analyses

---

## Quick Start

### 1. Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Or use automatic script (Windows)
.\setup_venv.ps1
```

### 2. API Keys Configuration

Create a `.env` file in the root:

```env
NEURONPEDIA_API_KEY='your-neuronpedia-key-here'
OPENAI_API_KEY='your-openai-key-here'
```

### 3. Launch the application

```bash
streamlit run eda/app.py
```

The app will open at `http://localhost:8501`

---

## Project Structure

```
attribution-graph-probing/
├── eda/                                    # Streamlit application
│   ├── app.py                              # Main app
│   ├── pages/
│   │   ├── 00_Graph_Generation.py          # Stage 1
│   │   ├── 01_Probe_Prompts.py             # Stage 2
│   │   └── 02_Node_Grouping.py             # Stage 3
│   └── README.md                           # Complete documentation
│
├── scripts/                                # Standalone Python scripts
│   ├── 00_neuronpedia_graph_generation.py  # Graph generation
│   ├── 01_probe_prompts.py                 # Probe prompting
│   ├── 02_node_grouping.py                 # Node classification and naming
│   └── causal_utils.py                     # Utilities
│
├── tests/                                  # Test suite
│   ├── test_node_naming.py
│   ├── test_probe_prompts_api.py
│   └── ...
│
├── output/                                 # Files generated by pipeline
│   ├── graph_data/                         # Attribution graphs JSON
│   ├── checkpoints/                        # Probe prompts checkpoints
│   └── *.csv                               # Exports and results
│
├── docs/                                   # Documentation
│   ├── Anthropic_circuit_tracing.md.txt    # Paper reference
│   └── archive_old_pipeline/               # Old pipeline (archived)
│
├── .env                                    # API keys (do not commit)
├── requirements.txt                        # Python dependencies
└── readme.md                               # This file
```

---

## Detailed Pipeline

### Stage 1: Graph Generation

**Script**: `scripts/00_neuronpedia_graph_generation.py`  
**UI**: `eda/pages/00_Graph_Generation.py`

**What it does:**
- Generates attribution graphs on Neuronpedia via API
- Extracts static metrics (node_influence, cumulative_influence, frac_external)
- Visualizes feature distribution (layer × context position)
- Selects relevant features for Stage 2

**Output:**
- `output/graph_data/*.json` - Complete attribution graph
- `output/*_static_metrics.csv` - Metrics per feature
- `output/*_selected_features.json` - Selected features

**Key parameters:**
- Model ID (gemma-2-2b, gpt2-small, etc.)
- Node/Edge thresholds
- Max feature nodes

---

### Stage 2: Probe Prompts

**Script**: `scripts/01_probe_prompts.py`  
**UI**: `eda/pages/01_Probe_Prompts.py`

**What it does:**
- Generates semantically related concepts via OpenAI
- Gets activations for each feature on each concept via Neuronpedia API
- Calculates activation pattern metrics (peak tokens, consistency, sparsity)
- Automatic checkpoints every N features (resume from interruptions)

**Output:**
- `output/*_export.csv` - Complete dataset with metrics
- `output/*_export_ENRICHED.csv` - With aggregated metrics
- `output/checkpoints/*.json` - Checkpoints for resume

**Calculated metrics:**
- Peak tokens (functional vs semantic)
- Sparsity (median activations)
- Consistency (peak token stability)
- Confidence scores

**Features:**
- Automatic retry with exponential backoff
- Intelligent rate limiting (2 req/sec)
- Real-time progress tracking
- Automatic resume from interruptions

---

### Stage 3: Node Grouping

**Script**: `scripts/02_node_grouping.py`  
**UI**: `eda/pages/02_Node_Grouping.py`

**What it does:**
- **Step 1**: Classifies peak tokens (functional vs semantic), finds target tokens
- **Step 2**: Classifies features into supernodes using decision tree
- **Step 3**: Assigns automatic names based on activation patterns
- **Optional**: Upload to Neuronpedia for interactive visualization

**Output:**
- `output/*_GROUPED.csv` - Complete CSV with classification and naming
- `output/*_SUMMARY.json` - Statistics and parameters used

**Supernode Categories:**

1. **Semantic (Dictionary)**
   - Always activates on the same specific token
   - High peak consistency (≥0.8), few distinct peaks (≤1)
   - Examples: "capital", "of", "the"

2. **Semantic (Concept)**
   - Activates on semantically related tokens
   - Low layers (≤3) or high semantic confidence
   - Examples: "Capital", "Texas", "Dallas"

3. **Say "X"**
   - Predicts a specific output token
   - High layers (≥7), functional dominance (≥50%), high confidence
   - Examples: 'Say "Austin"', 'Say "Capital"'

4. **Relationship**
   - Encodes relationships between entities
   - Low sparsity (<0.45), diffuse activation
   - Naming based on semantic target pairs

**Decision Tree (V4 Final):**

```python
IF peak_consistency >= 0.8 AND n_distinct_peaks <= 1:
    → Semantic (Dictionary)
ELIF func_vs_sem_pct >= 50 AND conf_F >= 0.90 AND layer >= 7:
    → Say "X"
ELIF sparsity_median < 0.45:
    → Relationship
ELIF layer <= 3 OR conf_S >= 0.50 OR func_vs_sem_pct < 50:
    → Semantic (Concept)
ELSE:
    → Review
```

---

## Command Line Usage

### Graph Generation

```bash
python scripts/00_neuronpedia_graph_generation.py \
  --model gemma-2-2b \
  --source gemmascope-transcoder-16k \
  --prompt "The capital of Texas is" \
  --target " Austin"
```

### Probe Prompts

```bash
python scripts/01_probe_prompts.py \
  --graph output/graph_data/my_graph.json \
  --api-key $NEURONPEDIA_API_KEY \
  --concepts "Dallas,Texas,Capital,city,state" \
  --checkpoint-every 10
```

### Node Grouping

```bash
python scripts/02_node_grouping.py \
  --input output/my_export_ENRICHED.csv \
  --output output/my_GROUPED.csv \
  --json output/activations.json \
  --graph output/graph_data/my_graph.json
```

---

## Documentation

- **Complete Streamlit Guide**: `eda/README.md` (600+ lines)
- **Node Grouping Guide**: `eda/pages/README_NODE_GROUPING.md`
- **Environment Setup**: `SETUP.md`
- **Tests**: `tests/` (10+ test units)

### External References

- **Circuit Tracer** (Anthropic): https://github.com/safety-research/circuit-tracer
- **Paper**: https://transformer-circuits.pub/2025/attribution-graphs/
- **Neuronpedia**: https://www.neuronpedia.org

---

## FAQ

**Q: How long does a complete analysis take?**

A: Depends on the number of features and concepts:
- Graph Generation: 1-5 minutes
- Probe Prompts (100 features × 5 concepts): ~10 minutes
- Node Grouping: < 1 minute

**Q: What happens if Probe Prompting is interrupted?**

A: Thanks to automatic checkpoints, you can resume exactly where you left off. Select the checkpoint from the dropdown and click "Resume".

**Q: Can I modify classification thresholds?**

A: Yes! In the Node Grouping sidebar you can modify all thresholds and re-run Step 2 without redoing Step 1.

**Q: How do I visualize results on Neuronpedia?**

A: In the Node Grouping page, after Step 3, use the "Upload Neuronpedia" section to upload the subgraph with named supernodes.

**Q: Is the old "anthropological" pipeline (cicciotti, influence-first, etc.) still available?**

A: No, it has been deprecated. Documentation is archived in `docs/archive_old_pipeline/` for historical reference.

---

## Troubleshooting

**Error: "API key not found"**

Solution: Create `.env` with API keys (see Setup)

**Error: "No checkpoint found"**

Solution: Normal for first run. Checkpoints are created automatically during analysis.

**Error: UnicodeEncodeError (Windows)**

Solution:
```powershell
$env:PYTHONIOENCODING='utf-8'
streamlit run eda/app.py
```

**Error: "Rate limit exceeded"**

Solution: Automatic retry handles rate limits. If it persists, increase wait times in `scripts/01_probe_prompts.py`.

---

## Contributing

This project is part of a research application for MATS (AI Safety).

For questions or contributions:
- Consult documentation in `eda/README.md`
- Run tests in `tests/`

---

## Changelog

### v2.0.0-clean (October 2025)
- Completely renewed pipeline (3 stages)
- Migration to Neuronpedia API
- Automated probe prompting
- Automatic supernode classification and naming
- Checkpoints and resume for long analyses
- Interactive Streamlit UI

### v1.x (Archived)
- Old "anthropological" pipeline (cicciotti, influence-first)
- Documentation in `docs/archive_old_pipeline/`

---

**Version**: 2.0.0-clean  
**License**: GPL-3.0  
**Last Updated**: October 2025
